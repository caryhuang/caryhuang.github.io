<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cary&#39;s Blog</title>
  
  
  <link href="http://caryhuang.github.io/atom.xml" rel="self"/>
  
  <link href="http://caryhuang.github.io/"/>
  <updated>2021-03-03T23:03:10.945Z</updated>
  <id>http://caryhuang.github.io/</id>
  
  <author>
    <name>Cary Huang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How PostgreSQL Handles Sub Transaction Visibility In Streaming Replication Setup?</title>
    <link href="http://caryhuang.github.io/2021/03/03/How-PostgreSQL-Handles-Sub-Transaction-Visibility-In-Streaming-Replication-Setup/"/>
    <id>http://caryhuang.github.io/2021/03/03/How-PostgreSQL-Handles-Sub-Transaction-Visibility-In-Streaming-Replication-Setup/</id>
    <published>2021-03-03T20:17:11.000Z</published>
    <updated>2021-03-03T23:03:10.945Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>As an experienced PostgreSQL user, you may have a lot of experience in setting up streaming replication in your database clusters to make multiple backups of your data. But have you wondered how the standby is able to correctly determine if a tuple sent from the primary should be visible to the user or not. In the case where a transaction contains multiple subtransactions, how does the standby determine the visibility in this case? You might say… well it is PostgreSQL so it will just work… This is true. If you are someone who is curious and interested in knowing how PostgreSQL does certain things internally, then this blog may be interesting for you as we will talk about normally transaction and subtransactions and how both affect the standby’s ability to determine tuple visibility.</p><h3 id="2-How-is-Tuple-Visibility-Determined"><a href="#2-How-is-Tuple-Visibility-Determined" class="headerlink" title="2. How is Tuple Visibility Determined?"></a>2. How is Tuple Visibility Determined?</h3><p>PostgreSQL determines a tuple’s visibility based on the concept of transaction ID (a.k.a <code>txid</code>) and it is normally stored in a tuple’s header as either <code>xmin</code> or <code>xmax</code> where <code>xmin</code> holds the <code>txid</code> of the transaction that inserted this tuple and <code>t_xmax</code> holds the <code>txid</code> of the transaction that deleted or updated this tuple. If a tuple has not been deleted, its <code>t_xmax</code> is 0. There is also another mechanism called commit log (a.k.a <code>clog</code>) where it has information of currently active transaction IDs. When an inserted tuple has a valid <code>t_xmin</code> and invalid <code>t_xmax</code>, PostgreSQL will have to consult the <code>clog</code> first to determine if the <code>txid</code> stored in <code>t_xmin</code> is visible or not, then the result will be stored in the tuple’s <code>hint_bit</code> field about the visibility information such that it does not always have to consult the <code>clog</code> which could be resource intensive. If a tuple has a valid <code>t_xmax</code>, then there is no need to consult the <code>clog</code> as the tuple must be invisible. </p><p>This is just the basic ideas of how visibility is determined but it is enough for me to continue with this blog. For more information about visibility, you could refer to this <a href="http://www.interdb.jp/pg/pgsql05.html#_5.7.">resource</a></p><h3 id="3-What-is-a-Subtransaction"><a href="#3-What-is-a-Subtransaction" class="headerlink" title="3. What is a Subtransaction?"></a>3. What is a Subtransaction?</h3><p>As the name suggests, it is a smaller transaction that exists within a regular transaction and it can be created using the <code>SAVEPOINT</code> keyword after your <code>BEGIN</code> statement. Normally, when you have issued a <code>BEGIN</code> statement, all the tuples created from the subsequent DML statements such as <code>INSERT</code> or <code>UPDATE</code> will have a <code>txid</code> associated with these tuples and they will all be the same. </p><p>When you issue a <code>SAVEPOINT</code> statement within this transaction, all the tuples created from the subsequent DML statements will have a different <code>txid</code> that is normally larger than the main transaction’s <code>txid</code>. When you issue the <code>SAVEPOINT</code> statement again, the <code>txid</code> changes again. </p><p>The advantage of Subtransaction is that in a very large transaction, the entire transaction will not be aborted due to an error. It allows you to rollback to the previously created <code>SAVEPOINT</code> and continue to execute the main transaction. </p><p>The biggest difference between a subtransaction and a regular transaction is that there is not a concept of <code>commit</code> associated with subtransactions and it will not be recorded in the WAL file. However, if subtransaction is aborted, it will be recorded in the WAL file and subsequently be sent to the standby.</p><h3 id="4-How-Does-a-Standby-Determines-Visibility-Information-In-a-Regular-Transaction"><a href="#4-How-Does-a-Standby-Determines-Visibility-Information-In-a-Regular-Transaction" class="headerlink" title="4. How Does a Standby Determines Visibility Information In a Regular Transaction?"></a>4. How Does a Standby Determines Visibility Information In a Regular Transaction?</h3><p>Here’s an example of a regular transaction without any subtransactions:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BEGIN;</span><br><span class="line">INSERT INTO test VALUES(111,111111);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,222222);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,333333);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">COMMIT;   &#x3D;&gt; a XLOG_XACT_COMMIT WAL record having t_xmin &#x3D; 500 is sent to standby</span><br></pre></td></tr></table></figure><p>In a general case, the above queries will send at least 4 WAL records to the standby during streaming replication. First 3 contain the actual tuple information with <code>t_xmin</code> set to the current transaction ID which is 500 in the example. In other words, before the <code>COMMIT</code> is issued, the standby should have already received 3 WAL records from primary and completed the redo, meaning that the 3 tuples already exist in standby’s memory buffer. However, they are not visible yet because transaction ID 500 does not exist in the standby. It is until the <code>COMMIT</code> is issued in the primary that will subsequently generate a <code>XLOG_XACT_COMMIT</code> WAL record to be sent to standby to notify that transaction ID 500 is now committed and valid. When standby receives the <code>XLOG_XACT_COMMIT</code> WAL record, it will eventually update to its <code>clog</code> so the subsequent <code>SELECT</code> queries can do visibility check against. </p><h3 id="5-How-Does-a-Standby-Process-Visibility-Information-In-a-Regular-Transaction-Containing-Subtransactions"><a href="#5-How-Does-a-Standby-Process-Visibility-Information-In-a-Regular-Transaction-Containing-Subtransactions" class="headerlink" title="5. How Does a Standby Process Visibility Information In a Regular Transaction Containing Subtransactions?"></a>5. How Does a Standby Process Visibility Information In a Regular Transaction Containing Subtransactions?</h3><p>Now, let’s use the same example but add some <code>SAVEPOINT</code> to create subtransactions</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BEGIN;</span><br><span class="line">INSERT INTO test VALUES(111,111111);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,222222);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,333333);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">SAVEPOINT A;</span><br><span class="line">INSERT INTO test VALUES(111,444444);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,555555);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,666666);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">SAVEPOINT B;</span><br><span class="line">INSERT INTO test VALUES(111,777777);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,888888);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,999999);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">COMMIT;   &#x3D;&gt; a XLOG_XACT_COMMIT WAL record having t_xmin &#x3D; 500 is sent to standby</span><br></pre></td></tr></table></figure><p>As you can see, after the creation of <code>SAVEPOINT</code>, the tuple’s <code>t_xmin</code> value will be changed and is no longer equal to the current transaction ID, which is 500 in the example. Before the <code>COMMIT</code> is issued, the standby should have received 9 WAL records, 3 having <code>t_xmin</code> = 500, 3 having <code>t_xmin</code> = 501 and 3 having <code>t_xmin</code> = 502. None of them is visible yet in the standby because these transaction IDs do not exist yet. When the primary issues the <code>COMMIT</code> statement, it will generate a <code>XLOG_XACT_COMMIT</code> WAL record and send to standby containing only the current transaction ID 500 without the 501 and 502. </p><p>After the commit, the standby is able to see all 9 tuples that are replicated from the primary. Now, you may be asking…. why? How come the standby knows that <code>txid</code> = 501 and 502 are also visible even though the primary never send a <code>XLOG_XACT_COMMIT</code> WAL record to tell the standby that 501 and 502 are also committed. Keep reading to find out.</p><p>Using the similar example, if we rollback to one of the SAVEPOINTs, the primary will send a XLOG_XACT_ABORT WAL record to notify the standby a transaction ID has become invalid.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BEGIN;</span><br><span class="line">INSERT INTO test VALUES(111,111111);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,222222);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,333333);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby</span><br><span class="line">SAVEPOINT A;</span><br><span class="line">INSERT INTO test VALUES(111,444444);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,555555);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,666666);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby</span><br><span class="line">SAVEPOINT B;</span><br><span class="line">INSERT INTO test VALUES(111,777777);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,888888);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">INSERT INTO test VALUES(111,999999);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">ROLLBACK TO SAVEPOINT B;  &#x3D;&gt; a XLOG_XACT_ABORT WAL record having t_xmin &#x3D; 502 is sent to standby</span><br><span class="line">COMMIT;   &#x3D;&gt; a XLOG_XACT_COMMIT WAL record having t_xmin &#x3D; 500 is sent to standby</span><br></pre></td></tr></table></figure><p>In this example, when doing a <code>ROLLBACK</code> to <code>SAVEPOINT</code> B, a XLOG_XACT_ABORT WAL record will be sent to the standby to invalidate that the transaction ID 502 is invalid. So when primary commits, records having <code>txid</code>=502 will not be visible as primary has notified.</p><h3 id="6-The-Secret-of-Standby’s-KnownAssignedTransactionIds"><a href="#6-The-Secret-of-Standby’s-KnownAssignedTransactionIds" class="headerlink" title="6. The Secret of Standby’s KnownAssignedTransactionIds"></a>6. The Secret of Standby’s KnownAssignedTransactionIds</h3><p>In addition to handling the XLOG_XACT_COMMIT and XLOG_XACT_ABORT WAL records to determine if a <code>txid</code> is valid or not, it actually manages a global <code>txid</code> list called <code>KnownAssignedTransactionIds</code>. Whenever a standby receives a heap_redo WAL record, it will save its <code>xmin</code> to <code>KnownAssignedTransactionIds</code>. Using the same example as above again here:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BEGIN;</span><br><span class="line">INSERT INTO test VALUES(111,111111);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,222222);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,333333);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIds</span><br><span class="line">SAVEPOINT A;</span><br><span class="line">INSERT INTO test VALUES(111,444444);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,555555);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,666666);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIds</span><br><span class="line">SAVEPOINT B;</span><br><span class="line">INSERT INTO test VALUES(111,777777);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,888888);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIds</span><br><span class="line">INSERT INTO test VALUES(111,999999);    &#x3D;&gt; a heap_redo WAL record having t_xmin &#x3D; 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIds</span><br><span class="line">COMMIT;   &#x3D;&gt; a XLOG_XACT_COMMIT WAL record having t_xmin &#x3D; 500 is sent to standby and standby submits 500, 501 and 502 to clog as valid.</span><br></pre></td></tr></table></figure><p>Before the <code>COMMIT</code> is issued on the primary, the standby already has <code>txid</code> 500, 501, and 502 present in the <code>KnownAssignedTransactionIds</code>. When the standby receives <code>XLOG_XACT_COMMIT WAL</code>, it will actually submit <code>txid</code> 500 from the WAL plus the 501 and 502 from <code>KnownAssignedTransactionIds</code> to the <code>clog</code>. So that the subsequent <code>SELECT</code> query consult the <code>clog</code> it will say <code>txid</code> 500, 501, and 502 are visible. If the primary sends <code>XLOG_XACT_ABORT</code> as a result of rolling back to a previous <code>SAVEPOINT</code>, that invalid <code>txid</code> will be removed from the <code>KnownAssignedTransactionIds</code>, so when the transaction commits, the invalid <code>txid</code> will not be submit to <code>clog</code>.</p><h3 id="7-Summary"><a href="#7-Summary" class="headerlink" title="7. Summary"></a>7. Summary</h3><p>So this is how PostgreSQL determines a tuple’s visibility within subtransactions in a streaming replication setup. Of course, there is more details to what is discussed here in this blog, but I hope today’s discussion can help you get a general understanding of how PostgreSQL determines the visibility and maybe it will help you in your current work on PostgreSQL.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;As an experienced Po</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
    <category term="visibility" scheme="http://caryhuang.github.io/tags/visibility/"/>
    
    <category term="streaming replication" scheme="http://caryhuang.github.io/tags/streaming-replication/"/>
    
    <category term="savepoint" scheme="http://caryhuang.github.io/tags/savepoint/"/>
    
    <category term="subtransaction" scheme="http://caryhuang.github.io/tags/subtransaction/"/>
    
  </entry>
  
  <entry>
    <title>How PostgreSQL Inserts A New Record With The help of Table Access Method API and Buffer Manager</title>
    <link href="http://caryhuang.github.io/2021/02/02/How-PostgreSQL-Inserts-A-New-Record-With-The-help-of-Table-Access-Method-API-and-Buffer-Manager/"/>
    <id>http://caryhuang.github.io/2021/02/02/How-PostgreSQL-Inserts-A-New-Record-With-The-help-of-Table-Access-Method-API-and-Buffer-Manager/</id>
    <published>2021-02-02T19:35:31.000Z</published>
    <updated>2021-02-03T20:31:38.277Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>This blog talks about a high level description of the mechanism behind PostgreSQL to execute an <code>INSERT</code> query. This process involves many steps of processing before the data is put in the right place. These process normally involves several catalog cache lookup to determine if the destination table exists or several checking on the constraint violations..etc. This blog will mainly focus on the part where the processing handle is passed to the PostgreSQL’s table access method API and its interaction with buffer manager and WAL routines. This is also the core area where an <code>INSERT</code> query is actually executed. If you are a developer looking to understand how PostgreSQL works internally, this blog may be helpful to you…</p><h3 id="2-Table-Access-Method-APIs-in-PostgreSQL"><a href="#2-Table-Access-Method-APIs-in-PostgreSQL" class="headerlink" title="2. Table Access Method APIs in PostgreSQL"></a>2. Table Access Method APIs in PostgreSQL</h3><p>Pluggable table access method API has been made available since PostgreSQL 12, which allows a developer to redefine how PostgreSQL stores / retrieves table data. This API contains a total of 42 routines that need to be implemented in order to complete the implementation and honestly it is no easy task to understand all of them and to implement them. This API structure is defined in <code>tableam.h</code> under the name <code>typedef struct TableAmRoutine</code></p><p>Today I will describe the routines related to <code>INSERT</code>.</p><h3 id="3-INSERT-Query-Overall-Call-Flow"><a href="#3-INSERT-Query-Overall-Call-Flow" class="headerlink" title="3. INSERT Query Overall Call Flow"></a>3. INSERT Query Overall Call Flow</h3><p>A few of the 42 routines will be called by executor just to complete an <code>INSERT</code> query. This section will describe these routines in the order they are called.</p><h4 id="3-1-slot-callbacks"><a href="#3-1-slot-callbacks" class="headerlink" title="3.1 slot_callbacks"></a>3.1 slot_callbacks</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">const TupleTableSlotOps *(*slot_callbacks) (Relation rel);</span><br></pre></td></tr></table></figure><p>The executor needs to find out which set of tuple table slot (TTS) callback operation this table access method is compatible with. TTS is a set of routines that ensures the tuple storage is compatible between the executor and your access method. The executor will execute the TTS callback to <code>translate</code> your tuple strucuture to <code>TupleTableSlot</code> format in which the executor will understand. The default <code>heap</code> access method uses <code>TTSOpsBufferHeapTuple</code> defined in <code>execTuples.c</code> to handle this operation</p><h4 id="3-2-heap-insert"><a href="#3-2-heap-insert" class="headerlink" title="3.2 heap_insert"></a>3.2 heap_insert</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void</span><br><span class="line">heap_insert(Relation relation, HeapTuple tup, CommandId cid,</span><br><span class="line">int options, BulkInsertState bistate)</span><br></pre></td></tr></table></figure><p><code>heap_insert</code> is the entry point to perform the actual data insertion and it will undergo several other routines provided by <code>buffer manager</code> and <code>WAL module</code> in order to complete the insertion.</p><h5 id="heap-prepare-insert"><a href="#heap-prepare-insert" class="headerlink" title="heap_prepare_insert"></a>heap_prepare_insert</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static HeapTuple heap_prepare_insert(Relation relation, HeapTuple tup,</span><br><span class="line"> TransactionId xid, CommandId cid, int options);</span><br></pre></td></tr></table></figure><p>This is a subroutine for <code>heap_insert</code> where it will initialize the tuple header contents such as relation OID, infomasks, xmin, xmax values. It will also determine if the tuple is too big that <code>TOAST</code> is required to complete the insertion. These terms and parameters are very technical in PostgreSQL. If you are not sure what exactly they are, you could refer to resources <a href="https://www.postgresql.org/docs/current/storage-toast.html">here</a> and <a href="https://www.postgresql.org/docs/current/ddl-system-columns.html">here</a>.</p><h5 id="RelationGetBufferForTuple"><a href="#RelationGetBufferForTuple" class="headerlink" title="RelationGetBufferForTuple"></a>RelationGetBufferForTuple</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">extern Buffer RelationGetBufferForTuple(Relation relation, Size len,</span><br><span class="line">Buffer otherBuffer, int options,</span><br><span class="line">BulkInsertStateData *bistate,</span><br><span class="line">Buffer *vmbuffer, Buffer *vmbuffer_other);</span><br></pre></td></tr></table></figure><p>This is an entry function to access <code>buffer manager</code> resources and all it is doing is ask the <code>buffer manager</code> to return a <code>buffer ID</code> that can be used to store the target tuple. This may sound very straightforward, but there is quite a lot of processing on the buffer manager side to properly determine a desired buffer location. </p><p>First, it will do a quick size check. If the input tuple is larger than the size of each buffer block, it will return immediately with error as <code>TOAST</code> has to be used in this case. Then it will try to put the tuple on the same page the system last inserted the tuple on to see if it will fit there. If not, it will utilize the <code>free space map</code> to find another page that could fit tuple. If that does not work out, then buffer manage will allocate a new data page (also referred to as <code>extend</code>) to be used to hold this new tuple. As soon as we have a desired buffer page determined, buffer manager will cache this page in the <code>relation</code> structure such that next time the same relation visits the buffer manager, it knows immediately about the reference to the last inserted block. </p><h5 id="RelationPutHeapTuple"><a href="#RelationPutHeapTuple" class="headerlink" title="RelationPutHeapTuple"></a>RelationPutHeapTuple</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">extern void RelationPutHeapTuple(Relation relation, Buffer buffer,</span><br><span class="line"> HeapTuple tuple, bool token);</span><br></pre></td></tr></table></figure><p>Once we have identified the location of the buffer to store the tuple, the insert routine will then call <code>RelationPutHeapTuple</code> to actually put the tuple in the specified buffer location. This routine will again ask the buffer manager to get a pointer reference to the data page using the buffer ID we obtained from <code>RelationGetBufferForTuple</code>, then add the tuple data using <code>PageAddItem()</code> routine. Internally in buffer manager, it manages the relationship between buffer ID, buffer descriptor and the actual pointer to the data page to help us correctly identify and write to a data page. After a successful write, the routine will save a <code>CTID</code> of the inserted tuple. This ID is the location of this tuple and it consists of the data page number and the offset. For more information about how buffer manager works, you can refer to the resource <a href="http://www.interdb.jp/pg/pgsql08.html">here</a></p><h5 id="Mark-buffer-dirty"><a href="#Mark-buffer-dirty" class="headerlink" title="Mark buffer dirty"></a>Mark buffer dirty</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">extern void MarkBufferDirty(Buffer buffer);</span><br></pre></td></tr></table></figure><p>At this point, the tuple data is already stored in the buffer manager referenced by a particular data page plus an offset, but it is not yet flushed to disk yet. In this case, we almost always will have to call <code>MarkBufferDirty</code> function to signal buffer manager that there are some tuples on the page that have not been flushed to disk and therefore in the next <code>checkpoint</code>, it will ensure the new tuples are flushed to disk.</p><h5 id="Insert-WAL-Record"><a href="#Insert-WAL-Record" class="headerlink" title="[Insert WAL Record]"></a>[Insert WAL Record]</h5><p>Last but not least, after doing all the hassle of finding a buffer location to put our tuple in and mark it as dirty, it is time for the <code>heap_insert</code> routine to populate a WAL record. This part is not the focus of this blog so I will skip the high level details of WAL writing.</p><h4 id="3-3-End-of-the-insertion"><a href="#3-3-End-of-the-insertion" class="headerlink" title="3.3 End of the insertion"></a>3.3 End of the insertion</h4><p>At this point the insertion of a new tuple data has finished and proper WAL record has been written. The routine will once again save the <code>CTID</code> value that we derived during the data insertion and save this value to the <code>TTS</code> structure so the executor also gets a copy of the location of the tuple. Then it will clean up the local resources before returning.</p><h3 id="4-Summary"><a href="#4-Summary" class="headerlink" title="4. Summary"></a>4. Summary</h3><p>What we have discussed here is the basic call flow of a simple sequential scan. If we were to visualize the process, it should look something like this:<br><img src="/images/tuple-insert.png" alt="tuple insert"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;This blog talks abou</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
    <category term="pluggable storage" scheme="http://caryhuang.github.io/tags/pluggable-storage/"/>
    
    <category term="table access method" scheme="http://caryhuang.github.io/tags/table-access-method/"/>
    
    <category term="buffer manager" scheme="http://caryhuang.github.io/tags/buffer-manager/"/>
    
  </entry>
  
  <entry>
    <title>How PostgreSQL Executes Sequential Scans with the Help of Table Access Methods APIs</title>
    <link href="http://caryhuang.github.io/2021/01/14/How-PostgreSQL-Executes-Sequential-Scans-with-the-Help-of-Table-Access-Methods-APIs/"/>
    <id>http://caryhuang.github.io/2021/01/14/How-PostgreSQL-Executes-Sequential-Scans-with-the-Help-of-Table-Access-Methods-APIs/</id>
    <published>2021-01-14T22:27:59.000Z</published>
    <updated>2021-01-15T20:02:11.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>There are many approaches for PostgreSQL to retrieve the data back to the user. Depending on the user’s input query, the planner module is responsible for selecting the most optimum approach to retrieve the requested data. Sequential scan is one of these approaches that is mostly selected when the user requests a large volume of data (for example, “SELECT * from tablename;”) or when a table has no index declared. Sequential scan is mostly handled by the table access method API within PostgreSQL and <code>heap</code> access method is the default one PostgreSQL uses today. In this short post, I will show you how sequential scan is done in the table access method API. </p><h3 id="2-Table-Access-Method-APIs-in-PostgreSQL"><a href="#2-Table-Access-Method-APIs-in-PostgreSQL" class="headerlink" title="2. Table Access Method APIs in PostgreSQL"></a>2. Table Access Method APIs in PostgreSQL</h3><p>Pluggable table access method API has been made available since PostgreSQL 12, which allows a developer to redefine how PostgreSQL stores / retrieves table data. This API contains a total of 42 routines that need to be implemented in order to complete the implementation and honestly it is no easy task to understand all of them and to implement them. This API structure is defined in <code>tableam.h</code> under the name <code>typedef struct TableAmRoutine</code></p><p>Today I will describe the routines related to sequential scan and I hope it could help you if you are someone looking to create your own table access method.</p><h3 id="3-Sequential-Scan-Overall-Call-Flow"><a href="#3-Sequential-Scan-Overall-Call-Flow" class="headerlink" title="3. Sequential Scan Overall Call Flow"></a>3. Sequential Scan Overall Call Flow</h3><p>Few of the 42 routines will be called by executor just to complete a sequential scan request. This section will describe these routines in the order they are called.</p><h4 id="3-1-relation-size"><a href="#3-1-relation-size" class="headerlink" title="3.1 relation_size"></a>3.1 relation_size</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uint64(*relation_size) (Relation rel, ForkNumber forkNumber);</span><br></pre></td></tr></table></figure><p><code>relation_size</code> is the first routine to be called and it is relatively simple. The caller will expect the routine to return the total size of the relation described by <code>rel</code> and <code>forkNumber</code>. The default <code>heap</code> access method will simply invoke the storage manager <code>smgr</code> to find the number of data blocks this particular relation physically occupies on disk and multiplies that number with the size of each block <code>BLCKSZ</code> (default is 8k). If you are not sure about the relationship between relation and its fork number, you could refer to this <a href="https://www.highgo.ca/2020/10/23/free-space-mapping-file-in-details/">blog</a> to get more information. </p><p>The size returned by this routine basically sets the boundary of our sequential scan.</p><h4 id="3-2-slot-callbacks"><a href="#3-2-slot-callbacks" class="headerlink" title="3.2 slot_callbacks"></a>3.2 slot_callbacks</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">const TupleTableSlotOps *(*slot_callbacks) (Relation rel);</span><br></pre></td></tr></table></figure><p>Next, the executor needs to find out which set of tuple table slot (TTS) callback operation this table access method is compatible with. TTS is a set of routines that ensures the tuple storage is compatible between the executor and your access method. The executor will execute the TTS callback to <code>translate</code> your tuple strucuture to <code>TupleTableSlot</code> format in which the executor will understand. The default <code>heap</code> access method uses <code>TTSOpsBufferHeapTuple</code> defined in <code>execTuples.c</code> to handle this operation</p><h4 id="3-3-scan-begin"><a href="#3-3-scan-begin" class="headerlink" title="3.3 scan_begin"></a>3.3 scan_begin</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TableScanDesc (*scan_begin) (Relation rel,</span><br><span class="line"> Snapshot snapshot,</span><br><span class="line"> int nkeys, struct ScanKeyData *key,</span><br><span class="line"> ParallelTableScanDesc pscan,</span><br><span class="line"> uint32 flags);</span><br></pre></td></tr></table></figure><p>Now the scan can officially begin. This is sequential scan’s initialization routine in which it will allocate a new scan descriptor using the parameters passed in by the executor. The purpose of scan descriptor structure is to keep track of the sequential scan while it is being executed. For example, to track where the scan should begin,; when was the block number of the last scan; which block should we resume scanning and how many blocks have been scanned…etc. scan descriptor will be destroyed once the sequential scan has completed.</p><p>The executor expects the routine to return a fully allocated and initialize pointer to <code>TableScanDesc</code> struct</p><h4 id="3-4-scan-getnextslot"><a href="#3-4-scan-getnextslot" class="headerlink" title="3.4 scan_getnextslot"></a>3.4 scan_getnextslot</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bool(*scan_getnextslot) (TableScanDesc scan,</span><br><span class="line"> ScanDirection direction,</span><br><span class="line"> TupleTableSlot *slot);</span><br></pre></td></tr></table></figure><p>This is the main routine for sequential scan where the caller expects the routine to fetch one tuple from the buffer manager, converts it to the TTS format in which executor understands and save it in the input pointer called <code>slot</code>. Each call to this routine will results in one tuple to be returned. If a table contains 1000 tuples, this function will be called 1000 times. The boolean return code is the indication to the caller if the routine has more tuples to return, as soon as <code>false</code> is returned, it signals the executor that we have exhausted all the tuples and it should stop calling this function.</p><p>In normal sequential scan case, this routine works in per-page mode. This means it will read one full block from buffer manager and scan it to get all the tuple addresses and their offsets in the scan descriptor, so in the subsequent calls to the same function, it will not load the full page again from buffer manage all the time; it will only start to load the next block when all the tuples on the current block have been scanned and returned. </p><p>As you can see, the scan descriptor plays an important role here as most of the control information is saved there and is regularly updated whenever a call the <code>scan_getnextslot</code> is made.</p><h4 id="3-5-scan-end"><a href="#3-5-scan-end" class="headerlink" title="3.5 scan_end"></a>3.5 scan_end</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void(*scan_end) (TableScanDesc scan);</span><br></pre></td></tr></table></figure><p>This is the last routine to be called to basically clean up the table scan descriptor, which was used heavily during the sequential scan. At this point, the executor should already have all the tuple information from the sequential scan methods. </p><h3 id="4-Prepare-the-Data-to-Return"><a href="#4-Prepare-the-Data-to-Return" class="headerlink" title="4. Prepare the Data to Return"></a>4. Prepare the Data to Return</h3><p>Now, the executor is finished with the table access method and has already had access to all the tuples for a particular relation. It then needs to go through another round of filtering to determine which of these tuples satisfy the condition set by the user, (for example, when user gives the WHERE clause to limit the scan results). This is done in another infinite <code>for</code> loop in <code>execScan.c</code> to perform <code>ExecQual</code> on each TTS. Finally, the end results will be sent to the end user.</p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p>What we have discussed here is the basic call flow of a simple sequential scan. If we were to visualize the process, it should look something like this:<br><img src="/images/sequential-scan.png" alt="Sequential Scan"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;There are many appro</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
    <category term="pluggable storage" scheme="http://caryhuang.github.io/tags/pluggable-storage/"/>
    
    <category term="table access method" scheme="http://caryhuang.github.io/tags/table-access-method/"/>
    
  </entry>
  
  <entry>
    <title>2020 PG Asia Conference Ended Successfully at an Unprecedented Scale!</title>
    <link href="http://caryhuang.github.io/2020/11/27/2020%20PG%20Asia%20Conference%20Ended%20Successfully%20at%20an%20Unprecedented%20Scale!/"/>
    <id>http://caryhuang.github.io/2020/11/27/2020%20PG%20Asia%20Conference%20Ended%20Successfully%20at%20an%20Unprecedented%20Scale!/</id>
    <published>2020-11-27T19:21:47.000Z</published>
    <updated>2020-12-02T21:16:39.319Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>On November 17-20, 2020, PostgresConf.CN &amp; PGconf.Asia2020 (referred to as 2020 PG Asia Conference) was held online for the very first time! This conference was jointly organized by the PG China Open Source Software Promotion Alliance, PostgresConf International Conference Organization, and PGConf.Asia Asian Community. This conference was broadcast exclusively via the Modb Technology Community platform in China with a record-high number of viewers streaming the conference events. With the great support from these PostgreSQL communities, the conference was held with great success, which brought together the Chinese PG power, major Asian PG contributors and many PostgreSQL experts worldwide to build the largest PG ecosystem in Asia.</p><h2 id="About-the-Conference"><a href="#About-the-Conference" class="headerlink" title="About the Conference"></a>About the Conference</h2><p>Also known as the Asia’s largest open source relational database ecology conference</p><p>PostgresConf.CN and PGConf.Asia, for the very first time, were hosted together as one conference online accompanied by additional offline sessions hosted at several reputable university campuses in China. </p><p>PostgresConf.CN is an annual conference held by the China PostgreSQL Association for PostgreSQL users and developers. It is also one of the conference series held by PostgresConf Organization. PostgreConf.CN 2019 took place in Beijing, it was very well attended by PostgreSQL users and community members across the globe.</p><p>PGCONF.Asia is also an annual PostgreSQL event that took place in Bali Indonesia in 2019, it was a continuation of the PGCONF.Asia event that took place in Tokyo, Japan in 2018. The first PGCONG.Asia conference took place in 2016 in Tokyo, this conference acts as a hub of PostgreSQL related development and technical discussion among PostgreSQL users and developers in the region as well as experts from around the globe.</p><p>Learn more about these conferences and the organizers from these resources:</p><ul><li><a href="https://2020.postgresconf.cn/en">2020.postgresconf.cn</a></li><li><a href="https://2020.pgconf.asia">2020.pgconf.asia</a></li><li><a href="https://www.postgresconf.org">PostgresConf</a></li></ul><h2 id="Sponsors"><a href="#Sponsors" class="headerlink" title="Sponsors"></a>Sponsors</h2><p>This conference was sponsored by:</p><p><strong>Platinum</strong>:</p><ul><li><a href="https://www.aliyun.com/">Alibaba Cloud</a></li><li><a href="https://cloud.tencent.com/">Tencent Cloud</a></li><li><a href="https://www.enterprisedb.com/">EDB</a></li><li><a href="https://www.highgo.com/">HighGo Software</a></li></ul><p><strong>Golden</strong>:</p><ul><li><a href="http://www.inspurpower.com/">Inspur Power Systems</a></li></ul><p><strong>Silver</strong>:</p><ul><li><a href="https://equnix.asia/">Equnix Business Solutions</a></li></ul><h2 id="14-Conference-Channels-over-4-Days"><a href="#14-Conference-Channels-over-4-Days" class="headerlink" title="14 Conference Channels over 4 Days!"></a>14 Conference Channels over 4 Days!</h2><p>The conference lasted 4 days at an unprecedented scale. A total of 14 channels, including 5 technical training + 9 main/sub-forum training channels. </p><ul><li>Alibaba Cloud Database Training Session </li><li>Tencent Cloud Database Training Session</li><li>HighGo Software Training Session</li><li>PG Training Institution Charity Session </li><li>PostgresConf Orgnization English Training Session (1 Day)</li><li>the main forum (2 days)</li><li>Chinese sub-forum (2 days)</li><li>English sub-forum A (2 days)</li><li>English sub-forum B (2 days) </li><li>CCF advanced academic forum (1 day) </li></ul><h2 id="Over-100-Participating-Experts-and-Scholars-Around-the-World"><a href="#Over-100-Participating-Experts-and-Scholars-Around-the-World" class="headerlink" title="Over 100 Participating Experts and Scholars Around the World"></a>Over 100 Participating Experts and Scholars Around the World</h2><p> The number of invited speakers for this conference has set a new record. With the theme of “Born into the World”, the conference gathered 112 technical presentations and more than 100 well-known experts and scholars around the world to provide a grand technical feast for all the participating PGers.</p><ul><li>Guangnam Ni, Fellow of the Chinese Academy of Engineering</li><li>Peng Liu, Vice chairman of China Open Source Software Promotion Alliance and researcher of the Chinese Academy of Sciences</li><li>Zhiyong Peng, deputy director of the database committee of the Chinese Computer Society</li><li>Bruce Momjian, co-founder of PostgreSQL international community and vice president of EDB</li><li>Peter Zaitsev, Founder and CEO of Percona</li><li>Tatsuo Ishii, the original author of Pgpool-ll and founders of PGconf.asian and Japanese PG user association</li><li>Experts from from Alibaba, Tencent, Amazon, JD.com, Inspur, Ping An, Suning, ZTE, HornetLab, Equnix, VMware Greenplum, yMatrix, HighGo Software, Yunhe Enmo, Percona, EDB, NTT, Postgres Professional, Fujitsu, AsiaInfo, Giant Sequoia, Mechuang, Wenwu, Guoxin Sinan, Hytera, Airwallex, Ottertune and many others.</li><li>Professors from Wuhan University, East China Normal University, Harbin Institute of Technology, Shandong University, CCF (China Computer Society) database committee members of Tianjin University</li><li>Professional lecturers from 10 well-known authorized PG training service providers</li><li>And many, many more!</li></ul><h2 id="Record-High-LIVE-Streaming-Viewers"><a href="#Record-High-LIVE-Streaming-Viewers" class="headerlink" title="Record High LIVE Streaming Viewers"></a>Record High LIVE Streaming Viewers</h2><p>The number of LIVE streaming viewers at this conference has also hit a new record. Each channel accumulated over 30,000 active LIVE streams, the official conference blog views accumulated over 50,000+ views, and the news reports and articles from media exceeded over 3,000 entries. </p><h2 id="Conference-Highlights"><a href="#Conference-Highlights" class="headerlink" title="Conference Highlights"></a>Conference Highlights</h2><p>PostgresConf is an annual event for PostgreSQL developers and users worldwide. This conference attracted core members from the global PostgreSQL community, as well as corporate and individual users who use PostgreSQL.</p><p>The PGConf.Asia conference went smoothly for 4 days and it attracted many domestic and foreign audiences worldwide to join the LIVE streaming channels. The conference has 100+ subject sharing sessions, and each session on average accumulated 30,000+ active streams.</p><p>The first two days of the conference, several leading internet service vendors such as Alibaba Cloud, Tencent Cloud and database vendor HighGo Software brought a series of rich technical contents related to PostgreSQL, providing tremendous amount of values to the audiences who are willing to learn PostgreSQL database technology</p><p>The third and fourth days of the conference consist of numerous technical presentations covering wide range of area of technical interests.</p><p>Let’s take a look at some of the highlights of the conference!</p><h3 id="Opening-Speech-by-Fellow-Guangnam-Ni"><a href="#Opening-Speech-by-Fellow-Guangnam-Ni" class="headerlink" title="Opening Speech by Fellow Guangnam Ni"></a>Opening Speech by Fellow Guangnam Ni</h3><p><img src="/images/guangnan-li.webp" alt="Guangnam Ni"><br>The conference was kick-started by an opening speech by Guangnan Ni, fellow of the Chinese Academy of Engineering and the leader of China’s basic software industry. Fellow Ni first expressed his warm congratulations on the holding of the conference and sincere greetings to the representatives of the participating countries and open source experts. He pointed out that open source is an important trend in today’s world development, and it is profoundly changing the global digital economy and information industry. </p><p>This conference brings great value to the development of the open source industry, promotes the open source movement and popularization of open source culture in Chin and also strengthens international exchanges and cooperation. Fellow Ni hopes to use the PGConf Asia Conference platform to realize intellectual exchanges between Asia and the world, achieve higher and higher levels of global cooperation, and contribute to the development of global open source technology. Finally, Fellow Ni wished the conference a complete success, and wished the PG open source ecosystem more prosperity! *”I wish the Chinese PG branch and the Asian PG community prosper and get better and better!”*</p><h3 id="Peng-Liu"><a href="#Peng-Liu" class="headerlink" title="Peng Liu"></a>Peng Liu</h3><p>Peng Liu, executive vice chairman of China Open Source Software Promotion Alliance and researcher at the Institute of Software and Chinese Academy of Sciences, delivered a speech on behalf of COPU. He pointed out that based on the open BSD license + decentralized ecological architecture, PostgreSQL has a permanent security guarantee in the past, present and future. Following the spirit of free software &amp; complying with the BSD open source agreement, PostgreSQL meets the most stringent security compliance audits, and is not subject to US technology export control jurisdiction restrictions (Long Arm Jurisdiction Limitations/US Export Controls). PostgreSQL is a recognized global technology with public property rights and countless global PG contributors are committed to promoting the free and democratic software movement to protect the rights of anyone who obtain and use such software.<br><img src="/images/peng-liu.webp" alt="Peng Liu"></p><h3 id="Bruce-Momjian"><a href="#Bruce-Momjian" class="headerlink" title="Bruce Momjian"></a>Bruce Momjian</h3><p>Bruce Momjian, co-founder of the PostgreSQL international community and vice president of EDB, shared “Will Postgres Live Forever” speech. He said that any business has its life cycle, and open source PG is no exception, but compared to closed source commercial software, the life cycle of open source software will have more vitality. As long as the source code is valuable, it can always get a new life. In 1996, Postgres got a new life due to the introduction of the SQL standard and the improvement of its functions. The development trend continues to rise today.</p><p><img src="/images/bruce-momjian.webp" alt="Bruce Momjian"><br><img src="/images/bruce-momjian-2.webp" alt="Bruce Momjian"></p><h3 id="Tatuso-Ishii"><a href="#Tatuso-Ishii" class="headerlink" title="Tatuso Ishii"></a>Tatuso Ishii</h3><p>PGconf.Asia and Tatsuo Ishii, the founder of the Japanese PG User Association and the original author of Pgpool-ll, shared “Wonderful PostgreSQL!”. Tatsuo Ishii wrote the WAL system by himself based on Gray’s business thesis, and his creativity is admirable.<br><img src="/images/tatsuo-ishii.webp" alt="Tatsuo Ishii"></p><h3 id="Peter-Zaitsev"><a href="#Peter-Zaitsev" class="headerlink" title="Peter Zaitsev"></a>Peter Zaitsev</h3><p>Percona CEO Peter Zaitsev made a sharing of “The Changing Landscape of Open Source Databases”. He summarized several key points such as distributed, cloud native, storage and computing separation, and hardware acceleration, which basically covered the main focuses of the current database technology development.</p><p><img src="/images/peter-zaitsev.webp" alt="Peter Zaitsev"><br><img src="/images/peter-zaitsev-2.webp" alt="Peter Zaitsev"><br><img src="/images/peterz2.png" alt="Peter Zaitsev"></p><h3 id="Zhiyong-Peng"><a href="#Zhiyong-Peng" class="headerlink" title="Zhiyong Peng"></a>Zhiyong Peng</h3><p>Professor Peng Zhiyong, deputy director of the database committee of CCF China Computer Society, deputy dean of the School of Computer Science of Wuhan University, made a sharing of “My Way from PG to TOTEM”. Professor Peng talked about his 30-years of database research, from leading students to in-depth PG source code research database models, to compiling PG kernel analysis textbooks, to the development of the totem (TOTEM) database.<br><img src="/images/zhiyong-peng.webp" alt="Zhiyong Peng"></p><p>As we all know, PG originated from the University of California, Berkeley. This conference was also fortunate enough to have invited Professor Peng from Wuhan University. Professor Peng has been deeply involved in the PG database for more than 30 years and has led students to in-depth PG source code research database model, to write PG kernel and analysis book. Eventually created the totem (TOTEM) database based on PG. Peng has made outstanding contributions to PostgreSQL talent training and research!</p><p><img src="/images/zhiyong-peng-2.webp" alt="Zhiyong Peng"></p><h3 id="Guoqiang-Gai"><a href="#Guoqiang-Gai" class="headerlink" title="Guoqiang Gai"></a>Guoqiang Gai</h3><p>Guoqiang Gai, the founder of Enmotech, gave a speech on “Observing the elephant: the skills migration from Oracle to PostgreSQL DBA”. Gai gave a principle analysis from a source code perspective through a typical PG rollback problem, encouraging everyone to play with open source databases to learn the source code deeply, emphasizing that DBA is essential to enterprise data management, and every DBA must pass self-employment training to reflect personal value.<br><img src="/images/guoqiang-gai.webp" alt="Guoqiang Gai"></p><p>Whether it is PG or PG-related database products, its value must always be reflected by helping companies manage core data assets. Here, Gai said that the role of DBA is crucial for the last mile from database products to users. They are the closest partners of databases, whether in client companies, database cloud vendors or software suppliers or integrators. Gai encouraged DBAs to take advantage of open source and analyze in-depth source code to solve problems.</p><p><img src="/images/guoqiang-gai-2.webp" alt="Guoqiang Gai"></p><h3 id="Julyanto-SUTANDANG"><a href="#Julyanto-SUTANDANG" class="headerlink" title="Julyanto SUTANDANG"></a>Julyanto SUTANDANG</h3><p>Julyanto SUTANDANG, CEO of Equnix Business Solutions, gave a very detailed presentation about the professional PostgreSQL certifications and how these certifications can help an individual advance his or her career in PostgreSQL related fields. Certification is one of the best ways to tell your client, or your boss that you are the right person for the job. Whether you are a regular user, a government regulator, a professor or a subcontractor, there will always be a suitable level of PostgreSQL certification for your needs!</p><p><img src="/images/julyanto.png" alt="Julyanto SUTANDANG"></p><h3 id="Lei-Feng"><a href="#Lei-Feng" class="headerlink" title="Lei Feng"></a>Lei Feng</h3><p>Lei Feng, founder and general manager of Greenplum China founder shared “Greenplum’s open source journey: ecology and community leadership” speech.<br><img src="/images/lei-feng.webp" alt="Lei Feng"></p><p>the AI-enabled database is also leading the future development direction of the database. Lei shared the trilogy of their core team’s digital advancement in the cloud era and emphasized that the core competitiveness of database companies in the future will be the exploration and practice of AI models.</p><p><img src="/images/lei-feng-2.webp" alt="Lei Feng"></p><h3 id="Shuxin-Fang"><a href="#Shuxin-Fang" class="headerlink" title="Shuxin Fang"></a>Shuxin Fang</h3><p>Shuxin Fang, general manager of the technical support department of Inspur Business Machines, shared his presentation on “K1 power and PostgreSQL help enterprises to build new normal and new core”.<br><img src="/images/shuxin-fang.webp" alt="Shuxin Fang"></p><h3 id="Zhengsheng-Ye"><a href="#Zhengsheng-Ye" class="headerlink" title="Zhengsheng Ye"></a>Zhengsheng Ye</h3><p>Zhengsheng Ye, general manager of Alibaba Cloud Database Products and Head of Database Products and Operations, shared a speech of “Database Development Trend”.<br><img src="/images/zhengsheng-ye.webp" alt="Zhengsheng Ye"></p><h3 id="Yicheng-Wang"><a href="#Yicheng-Wang" class="headerlink" title="Yicheng Wang"></a>Yicheng Wang</h3><p>Yicheng Wang, Deputy General Manager of Tencent Cloud Database, shared the “Database Behind Every WeChat Payment”.<br><img src="/images/yicheng-wang.webp" alt="Yicheng Wang"></p><p>Tencent Cloud uses Tbase’s distributed solution to carry the WeChat payment business, and at the same time, it continues to enhance the core value of the product through cluster scalability, enhanced security, and efficient data compression, providing DBaaS for more enterprise users.</p><p><img src="/images/yicheng-wang-2.webp" alt="Yicheng Wang"></p><h3 id="Xiaojun-Zheng"><a href="#Xiaojun-Zheng" class="headerlink" title="Xiaojun Zheng"></a>Xiaojun Zheng</h3><p>Xiaojun Zheng, Chief Scientist of HighGo Software, shared the “Review of Commercial Database Technology Innovation”. With 30 years of senior management experience in several well-known database companies, he elaborated on major innovations in commercial databases in the past 30 years, which has important guiding significance for future database development.</p><p><img src="/images/xiaojun-zheng.webp" alt="Xiaojun Zheng"></p><p>HighGo database (HGDB) values data security with great importance when fulfilling the demands of enterprise users, and it has enhanced the security functions through a variety of technical means, including separation of powers, FDE full disk encryption, security auditing, security marking, etc. More features of the security version can be referred to The image below figure. Not only that, the HighGo enterprise cluster database also has flexible scalability, high availability, and effective load balancing.</p><p><img src="/images/xiaojun-zheng-2.webp" alt="Xiaojun Zheng"></p><h3 id="Chaoqun-Zhan"><a href="#Chaoqun-Zhan" class="headerlink" title="Chaoqun Zhan"></a>Chaoqun Zhan</h3><p>Chaoqun Zhan, a researcher of Alibaba Group and head of the OLAP product department of the database product division, shared “Opportunities and Challenges of Cloud Native Data Warehouse”.</p><p><img src="/images/chaoqun-zhan.webp" alt="Chaoqun Zhan"></p><p>The development of database technology is mainly affected by business scenario requirements and development factors of hardware technology. From the perspective of business scenario requirements, Alibaba Cloud, as a domestic leading cloud vendor, mainly integrates PolarDB+ADB full-line database products and integrates PG to respond to users’ various business scenarios.</p><p>Zhan shared a cloud-native integrated solution within Alibaba Group, which provides extreme performance and extremely fast and flexible expansion of cloud-native DBaaS.</p><p><img src="/images/chaoqun-zhan-2.webp" alt="Chaoqun Zhan"></p><h3 id="Bohan-Zhang"><a href="#Bohan-Zhang" class="headerlink" title="Bohan Zhang"></a>Bohan Zhang</h3><p>Bohan Zhang, the co-founder of Ottertune, shared effective solutions from the Carnegie Mellon University laboratory for the automatic tuning of PG parameters, and provided DBA recruitment information. If you are interested, please contact Zhang directly<br><img src="/images/bohan-zhang.webp" alt="Bohan Zhang"></p><h2 id="Closing-the-Conference"><a href="#Closing-the-Conference" class="headerlink" title="Closing the Conference"></a>Closing the Conference</h2><p>On the last day of the conference, Peter Zaitsev, CEO of Percona, shared the topic “17 Things Developers Need to Know About Databases” to help the database developers out there (including PG and non-PG developers) to increase their productivity, their quality of work, maintain a good relationship with DevOps and most importantly, avoid deadly and expensive mistakes!</p><p><img src="/images/peterz.jpg" alt="Peter Zaitsev"></p><h2 id="About-China-PostgreSQL-Assosication"><a href="#About-China-PostgreSQL-Assosication" class="headerlink" title="About China PostgreSQL Assosication"></a>About China PostgreSQL Assosication</h2><p>The China PostgreSQL Association is a legitimate and non-profit organization under the China Open Source Software Promotion Alliance.</p><p>The association’s main focus is to conduct activities around PostgreSQL, organize operations, promote PostgreSQL, host trainings, contribute to technological innovations and implementations. In addition, the association aims to promote the localization of the database development by bridging the PostgreSQL Chinese community with the International community.</p><p>Official Website <a href="http://www.postgresqlchina.com">http://www.postgresqlchina.com</a><br><img src="/images/chinapg.jpg" alt="China PostgreSQL Association"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;On November 17-20, 2020, Postg</summary>
      
    
    
    
    
    <category term="postgres" scheme="http://caryhuang.github.io/tags/postgres/"/>
    
  </entry>
  
  <entry>
    <title>How to Analyze a PostgreSQL Crash Dump File</title>
    <link href="http://caryhuang.github.io/2020/11/06/How-to-Analyze-a-PostgreSQL-Crash-Dump-File/"/>
    <id>http://caryhuang.github.io/2020/11/06/How-to-Analyze-a-PostgreSQL-Crash-Dump-File/</id>
    <published>2020-11-06T18:03:42.000Z</published>
    <updated>2020-11-06T23:31:38.114Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>In this blog post, I will talk about how to enable the generation of crash dump file (also known as core dump) and some common GDB commands to help a developer troubleshoot a crash-related issues within PostgreSQL and also other applications. Proper analysis of the issue normally will take time and certain degree of knowledge about the application source code. From experience, sometimes it may be better to look at the bigger environment instead of looking at the point of crash.</p><h3 id="2-What-is-a-Crash-Dump-File"><a href="#2-What-is-a-Crash-Dump-File" class="headerlink" title="2. What is a Crash Dump File?"></a>2. What is a Crash Dump File?</h3><p>A crash dump file is a file that consists of the recorded state of the working memory of an application when it crashes. This state is represented by stacks of memory addresses and CPU registers and normally it is extremely difficult to debug with only memory addresses and CPU registers because they tell you no information about the application logic. Considering the core dump contents below, which shows the back trace of memory addresses to the point of crash. </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#1  0x00687a3d in ?? ()</span><br><span class="line">#2  0x00d37f06 in ?? ()</span><br><span class="line">#3  0x00bf0ba4 in ?? ()</span><br><span class="line">#4  0x00d3333b in ?? ()</span><br><span class="line">#5  0x00d3f682 in ?? ()</span><br><span class="line">#6  0x00d3407b in ?? ()</span><br><span class="line">#7  0x00d3f2f7 in ?? ()</span><br></pre></td></tr></table></figure><p>Not very useful is it? So, when we see a crash dump file that looks like this, it means the application is not built with debugging symbols, making this crash dump file useless. If this is the case, you will need to install the debug version of the application or re-build the application with debugging enabled.  </p><h3 id="3-How-to-Generate-a-Useful-Crash-Dump-File"><a href="#3-How-to-Generate-a-Useful-Crash-Dump-File" class="headerlink" title="3. How to Generate a Useful Crash Dump File"></a>3. How to Generate a Useful Crash Dump File</h3><p>Before the generation of crash dump file, we need to ensure the application is built with debugging symbols. This can be done by executing the <code>./configure</code> script like this:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;configure enable-debug</span><br></pre></td></tr></table></figure><p>This adds the <code>-g</code> argument to CFLAGS in <code>src/Makefile.global</code> with optimization level set to 2 (<code>-O2</code>). My preference is to also change the optimization to 0 (<code>-O0</code>) so when we are navigating the stack using GDB, the navigation will make much more sense rather than jumping around and we will be able to print out most variables values in memory instead of getting <code>optimized out</code> error in GDB.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CFLAGS &#x3D; -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror&#x3D;vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough&#x3D;3 -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision&#x3D;standard -Wno-format-truncation -g -O0</span><br></pre></td></tr></table></figure><p>Now, we can enable the crash dump generation. This can be done by the user limit command. </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -c unlimited</span><br></pre></td></tr></table></figure><p>to disable:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -c 0</span><br></pre></td></tr></table></figure><p>Make sure there is enough disk space because crash dump file is normally very large as it records all the memory execution states from start to crash, and make sure the <code>ulimit</code> is set up in the shell before starting PostgreSQL. When PostgreSQL crashes, a core dump file named <code>core</code> will be generated in <code>$PGDATA</code></p><h3 id="4-Analyzing-the-Dump-File-using-GDB"><a href="#4-Analyzing-the-Dump-File-using-GDB" class="headerlink" title="4. Analyzing the Dump File using GDB"></a>4. Analyzing the Dump File using GDB</h3><p>GDB (GNU Debugger) is a portable debugger that runs on many Unix-like systems and can work with many programming languages and is my favorite tool to analyze a crash dump file. To demonstrate this, I will intentionally add a line in PostgreSQL source code that will result in <code>segmentation fault</code> crash type when a <code>CREATE TABLE</code> command is run. </p><p>Assuming the PostgreSQL has already crashed and generated a core dump file <code>core</code> in this location <code>~/highgo/git/postgres/postgresdb/core</code>. I would first use the <code>file</code> utility to understand more about the core file. Information such as the kernel info, and the program that generated it.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">caryh@HGPC01:~$ file &#x2F;home&#x2F;caryh&#x2F;highgo&#x2F;git&#x2F;postgres&#x2F;postgresdb&#x2F;core</span><br><span class="line">postgresdb&#x2F;core: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from &#39;postgres: cary cary [local] CREATE TABLE&#39;, real uid: 1000, effective uid: 1000, real gid: 1000, effective gid: 1000, execfn: &#39;&#x2F;home&#x2F;caryh&#x2F;highgo&#x2F;git&#x2F;postgres&#x2F;highgo&#x2F;bin&#x2F;postgres&#39;, platform: &#39;x86_64&#39;</span><br><span class="line">caryh@HGPC01:~$</span><br></pre></td></tr></table></figure><p>The <code>file</code> utility tells me that the core file is generated by this application <code>/home/caryh/highgo/git/postgres/highgo/bin/postgres</code>, so I would execute <code>gdb</code> like this:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gdb &#x2F;home&#x2F;caryh&#x2F;highgo&#x2F;git&#x2F;postgres&#x2F;highgo&#x2F;bin&#x2F;postgres -c  &#x2F;home&#x2F;caryh&#x2F;highgo&#x2F;git&#x2F;postgres&#x2F;postgresdb&#x2F;core</span><br><span class="line"></span><br><span class="line">GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-git</span><br><span class="line">Copyright (C) 2018 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http:&#x2F;&#x2F;gnu.org&#x2F;licenses&#x2F;gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;</span><br><span class="line">and &quot;show warranty&quot; for details.</span><br><span class="line">This GDB was configured as &quot;x86_64-linux-gnu&quot;.</span><br><span class="line">Type &quot;show configuration&quot; for configuration details.</span><br><span class="line">For bug reporting instructions, please see:</span><br><span class="line">&lt;http:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;gdb&#x2F;bugs&#x2F;&gt;.</span><br><span class="line">Find the GDB manual and other documentation resources online at:</span><br><span class="line">&lt;http:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;gdb&#x2F;documentation&#x2F;&gt;.</span><br><span class="line">For help, type &quot;help&quot;.</span><br><span class="line">Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...</span><br><span class="line">Reading symbols from &#x2F;home&#x2F;caryh&#x2F;highgo&#x2F;git&#x2F;postgres&#x2F;highgo&#x2F;bin&#x2F;postgres...done.</span><br><span class="line">[New LWP 27417]</span><br><span class="line">[Thread debugging using libthread_db enabled]</span><br><span class="line">Using host libthread_db library &quot;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libthread_db.so.1&quot;.</span><br><span class="line">Core was generated by &#96;postgres: cary cary [local] CREATE TABLE                                 &#39;.</span><br><span class="line">Program terminated with signal SIGSEGV, Segmentation fault.</span><br><span class="line">#0  heap_insert (relation&#x3D;relation@entry&#x3D;0x7f872f532228, tup&#x3D;tup@entry&#x3D;0x55ba8290f778, cid&#x3D;0, options&#x3D;options@entry&#x3D;0,</span><br><span class="line">    bistate&#x3D;bistate@entry&#x3D;0x0) at heapam.c:1840</span><br><span class="line">1840            ereport(LOG,(errmsg(&quot;heap tuple len &#x3D; %d&quot;, heaptup-&gt;t_len)));</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Immediately after running <code>gdb</code> on the <code>core</code> file, it shows the location of the crash at <code>heapam.c:1840</code> and that is exactly the line I have intentionally added to cause a crash.</p><h3 id="5-Useful-GDB-Commands"><a href="#5-Useful-GDB-Commands" class="headerlink" title="5. Useful GDB Commands"></a>5. Useful GDB Commands</h3><p>With <code>gdb</code>, it is very easy to identify the location of a crash, because it tells you immediately after running <code>gdb</code> on the <code>core</code> file. Unfortunately, 95% of the time, the location of the crash is not the real cause of the problem. This is why I mentioned earlier that sometimes it may be better to look at the bigger environment instead of looking at the point of crash. The crash is likely caused by a mistake in the application logic some where else in the application before it hits the point of crash. Even if you fix the crash, the mistake in application logic still exists and most likely, the application will crash somewhere else later or yield unsatisfactory results. Therefore, it is worth awhile to understand some of the powerful GDB commands that could help us understand the call stacks better to identify the real root cause.</p><h4 id="5-1-The-bt-Back-Trace-command"><a href="#5-1-The-bt-Back-Trace-command" class="headerlink" title="5.1 The bt (Back Trace) command"></a>5.1 The <code>bt</code> (Back Trace) command</h4><p>The <code>bt</code> command shows a series of call stacks since the beginning of the application all the way to the point of crash. With full debugging enabled, you will be able to see the function arguments and values being passed in to each function calls as well as the source file and line numbers where they were called. This allows developer to travel backwards to check for any potential application logic mistake in the earlier processing. </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">(gdb) bt</span><br><span class="line">#0  heap_insert (relation&#x3D;relation@entry&#x3D;0x7f872f532228, tup&#x3D;tup@entry&#x3D;0x55ba8290f778, cid&#x3D;0, options&#x3D;options@entry&#x3D;0,</span><br><span class="line">    bistate&#x3D;bistate@entry&#x3D;0x0) at heapam.c:1840</span><br><span class="line">#1  0x000055ba81ccde3e in simple_heap_insert (relation&#x3D;relation@entry&#x3D;0x7f872f532228, tup&#x3D;tup@entry&#x3D;0x55ba8290f778)</span><br><span class="line">    at heapam.c:2356</span><br><span class="line">#2  0x000055ba81d7826d in CatalogTupleInsert (heapRel&#x3D;0x7f872f532228, tup&#x3D;0x55ba8290f778) at indexing.c:228</span><br><span class="line">#3  0x000055ba81d946ea in TypeCreate (newTypeOid&#x3D;newTypeOid@entry&#x3D;0, typeName&#x3D;typeName@entry&#x3D;0x7ffcf56ef820 &quot;test&quot;,</span><br><span class="line">    typeNamespace&#x3D;typeNamespace@entry&#x3D;2200, relationOid&#x3D;relationOid@entry&#x3D;16392, relationKind&#x3D;relationKind@entry&#x3D;114 &#39;r&#39;,</span><br><span class="line">    ownerId&#x3D;ownerId@entry&#x3D;16385, internalSize&#x3D;-1, typeType&#x3D;99 &#39;c&#39;, typeCategory&#x3D;67 &#39;C&#39;, typePreferred&#x3D;false,</span><br><span class="line">    typDelim&#x3D;44 &#39;,&#39;, inputProcedure&#x3D;2290, outputProcedure&#x3D;2291, receiveProcedure&#x3D;2402, sendProcedure&#x3D;2403,</span><br><span class="line">    typmodinProcedure&#x3D;0, typmodoutProcedure&#x3D;0, analyzeProcedure&#x3D;0, elementType&#x3D;0, isImplicitArray&#x3D;false, arrayType&#x3D;16393,</span><br><span class="line">    baseType&#x3D;0, defaultTypeValue&#x3D;0x0, defaultTypeBin&#x3D;0x0, passedByValue&#x3D;false, alignment&#x3D;100 &#39;d&#39;, storage&#x3D;120 &#39;x&#39;,</span><br><span class="line">    typeMod&#x3D;-1, typNDims&#x3D;0, typeNotNull&#x3D;false, typeCollation&#x3D;0) at pg_type.c:484</span><br><span class="line">#4  0x000055ba81d710bc in AddNewRelationType (new_array_type&#x3D;16393, new_row_type&#x3D;&lt;optimized out&gt;, ownerid&#x3D;&lt;optimized out&gt;,</span><br><span class="line">    new_rel_kind&#x3D;&lt;optimized out&gt;, new_rel_oid&#x3D;&lt;optimized out&gt;, typeNamespace&#x3D;2200, typeName&#x3D;0x7ffcf56ef820 &quot;test&quot;)</span><br><span class="line">    at heap.c:1033</span><br><span class="line">#5  heap_create_with_catalog (relname&#x3D;relname@entry&#x3D;0x7ffcf56ef820 &quot;test&quot;, relnamespace&#x3D;relnamespace@entry&#x3D;2200,</span><br><span class="line">    reltablespace&#x3D;reltablespace@entry&#x3D;0, relid&#x3D;16392, relid@entry&#x3D;0, reltypeid&#x3D;reltypeid@entry&#x3D;0,</span><br><span class="line">    reloftypeid&#x3D;reloftypeid@entry&#x3D;0, ownerid&#x3D;16385, accessmtd&#x3D;2, tupdesc&#x3D;0x55ba8287c620, cooked_constraints&#x3D;0x0,</span><br><span class="line">    relkind&#x3D;114 &#39;r&#39;, relpersistence&#x3D;112 &#39;p&#39;, shared_relation&#x3D;false, mapped_relation&#x3D;false, oncommit&#x3D;ONCOMMIT_NOOP,</span><br><span class="line">    reloptions&#x3D;0, use_user_acl&#x3D;true, allow_system_table_mods&#x3D;false, is_internal&#x3D;false, relrewrite&#x3D;0, typaddress&#x3D;0x0)</span><br><span class="line">    at heap.c:1294</span><br><span class="line">#6  0x000055ba81e3782a in DefineRelation (stmt&#x3D;stmt@entry&#x3D;0x55ba82876658, relkind&#x3D;relkind@entry&#x3D;114 &#39;r&#39;, ownerId&#x3D;16385,</span><br><span class="line">    ownerId@entry&#x3D;0, typaddress&#x3D;typaddress@entry&#x3D;0x0,</span><br><span class="line">    queryString&#x3D;queryString@entry&#x3D;0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;) at tablecmds.c:885</span><br><span class="line">#7  0x000055ba81fd5b2f in ProcessUtilitySlow (pstate&#x3D;pstate@entry&#x3D;0x55ba82876548, pstmt&#x3D;pstmt@entry&#x3D;0x55ba828565a0,</span><br><span class="line">    queryString&#x3D;queryString@entry&#x3D;0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;,</span><br><span class="line">    context&#x3D;context@entry&#x3D;PROCESS_UTILITY_TOPLEVEL, params&#x3D;params@entry&#x3D;0x0, queryEnv&#x3D;queryEnv@entry&#x3D;0x0, qc&#x3D;0x7ffcf56efe50,</span><br><span class="line">    dest&#x3D;0x55ba82856860) at utility.c:1161</span><br><span class="line">#8  0x000055ba81fd4120 in standard_ProcessUtility (pstmt&#x3D;0x55ba828565a0,</span><br><span class="line">    queryString&#x3D;0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;, context&#x3D;PROCESS_UTILITY_TOPLEVEL,</span><br><span class="line">    params&#x3D;0x0, queryEnv&#x3D;0x0, dest&#x3D;0x55ba82856860, qc&#x3D;0x7ffcf56efe50) at utility.c:1069</span><br><span class="line">#9  0x000055ba81fd1962 in PortalRunUtility (portal&#x3D;0x55ba828b7dd8, pstmt&#x3D;0x55ba828565a0, isTopLevel&#x3D;&lt;optimized out&gt;,</span><br><span class="line">    setHoldSnapshot&#x3D;&lt;optimized out&gt;, dest&#x3D;&lt;optimized out&gt;, qc&#x3D;0x7ffcf56efe50) at pquery.c:1157</span><br><span class="line">#10 0x000055ba81fd23e3 in PortalRunMulti (portal&#x3D;portal@entry&#x3D;0x55ba828b7dd8, isTopLevel&#x3D;isTopLevel@entry&#x3D;true,</span><br><span class="line">    setHoldSnapshot&#x3D;setHoldSnapshot@entry&#x3D;false, dest&#x3D;dest@entry&#x3D;0x55ba82856860, altdest&#x3D;altdest@entry&#x3D;0x55ba82856860,</span><br><span class="line">    qc&#x3D;qc@entry&#x3D;0x7ffcf56efe50) at pquery.c:1310</span><br><span class="line">#11 0x000055ba81fd2f51 in PortalRun (portal&#x3D;portal@entry&#x3D;0x55ba828b7dd8, count&#x3D;count@entry&#x3D;9223372036854775807,</span><br><span class="line">    isTopLevel&#x3D;isTopLevel@entry&#x3D;true, run_once&#x3D;run_once@entry&#x3D;true, dest&#x3D;dest@entry&#x3D;0x55ba82856860,</span><br><span class="line">    altdest&#x3D;altdest@entry&#x3D;0x55ba82856860, qc&#x3D;0x7ffcf56efe50) at pquery.c:779</span><br><span class="line">#12 0x000055ba81fce967 in exec_simple_query (query_string&#x3D;0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;)</span><br><span class="line">    at postgres.c:1239</span><br><span class="line">#13 0x000055ba81fd0d7e in PostgresMain (argc&#x3D;&lt;optimized out&gt;, argv&#x3D;argv@entry&#x3D;0x55ba8287fdb0, dbname&#x3D;&lt;optimized out&gt;,</span><br><span class="line">    username&#x3D;&lt;optimized out&gt;) at postgres.c:4315</span><br><span class="line">#14 0x000055ba81f4f52a in BackendRun (port&#x3D;0x55ba82877110, port&#x3D;0x55ba82877110) at postmaster.c:4536</span><br><span class="line">#15 BackendStartup (port&#x3D;0x55ba82877110) at postmaster.c:4220</span><br><span class="line">#16 ServerLoop () at postmaster.c:1739</span><br><span class="line">#17 0x000055ba81f5063f in PostmasterMain (argc&#x3D;3, argv&#x3D;0x55ba8284fee0) at postmaster.c:1412</span><br><span class="line">#18 0x000055ba81c91c04 in main (argc&#x3D;3, argv&#x3D;0x55ba8284fee0) at main.c:210</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><h4 id="5-1-The-f-Fly-command"><a href="#5-1-The-f-Fly-command" class="headerlink" title="5.1 The f (Fly) command"></a>5.1 The <code>f</code> (Fly) command</h4><p>The <code>f</code> command followed by a stack number allows gdb to jump to a particular call stack listed by the <code>bt</code> command and allows you to print other variable in that particular stack. For example:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) f 3</span><br><span class="line">#3  0x000055ba81d946ea in TypeCreate (newTypeOid&#x3D;newTypeOid@entry&#x3D;0, typeName&#x3D;typeName@entry&#x3D;0x7ffcf56ef820 &quot;test&quot;,</span><br><span class="line">    typeNamespace&#x3D;typeNamespace@entry&#x3D;2200, relationOid&#x3D;relationOid@entry&#x3D;16392, relationKind&#x3D;relationKind@entry&#x3D;114 &#39;r&#39;,</span><br><span class="line">    ownerId&#x3D;ownerId@entry&#x3D;16385, internalSize&#x3D;-1, typeType&#x3D;99 &#39;c&#39;, typeCategory&#x3D;67 &#39;C&#39;, typePreferred&#x3D;false,</span><br><span class="line">    typDelim&#x3D;44 &#39;,&#39;, inputProcedure&#x3D;2290, outputProcedure&#x3D;2291, receiveProcedure&#x3D;2402, sendProcedure&#x3D;2403,</span><br><span class="line">    typmodinProcedure&#x3D;0, typmodoutProcedure&#x3D;0, analyzeProcedure&#x3D;0, elementType&#x3D;0, isImplicitArray&#x3D;false, arrayType&#x3D;16393,</span><br><span class="line">    baseType&#x3D;0, defaultTypeValue&#x3D;0x0, defaultTypeBin&#x3D;0x0, passedByValue&#x3D;false, alignment&#x3D;100 &#39;d&#39;, storage&#x3D;120 &#39;x&#39;,</span><br><span class="line">    typeMod&#x3D;-1, typNDims&#x3D;0, typeNotNull&#x3D;false, typeCollation&#x3D;0) at pg_type.c:484</span><br><span class="line">484                     CatalogTupleInsert(pg_type_desc, tup);</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>This forces <code>gdb</code> to jump to stack number 3, which is at pg_type.c:484. In here, you can examine all other variables in this frame (in function TypeCreate).</p><h4 id="5-2-The-p-Print-command"><a href="#5-2-The-p-Print-command" class="headerlink" title="5.2 The p (Print) command"></a>5.2 The <code>p</code> (Print) command</h4><p>The most popular command in <code>gdb</code>, which can be used to print variable addresses and values</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) p tup</span><br><span class="line">$1 &#x3D; (HeapTuple) 0x55ba8290f778</span><br><span class="line">(gdb) p pg_type_desc</span><br><span class="line">$2 &#x3D; (Relation) 0x7f872f532228</span><br><span class="line"></span><br><span class="line">(gdb)  p * tup</span><br><span class="line">$3 &#x3D; &#123;t_len &#x3D; 176, t_self &#x3D; &#123;ip_blkid &#x3D; &#123;bi_hi &#x3D; 65535, bi_lo &#x3D; 65535&#125;, ip_posid &#x3D; 0&#125;, t_tableOid &#x3D; 0,</span><br><span class="line">  t_data &#x3D; 0x55ba8290f790&#125;</span><br><span class="line"></span><br><span class="line">(gdb) p * pg_type_desc</span><br><span class="line">$4 &#x3D; &#123;rd_node &#x3D; &#123;spcNode &#x3D; 1663, dbNode &#x3D; 16384, relNode &#x3D; 1247&#125;, rd_smgr &#x3D; 0x55ba828e2a38, rd_refcnt &#x3D; 2, rd_backend &#x3D; -1,</span><br><span class="line">  rd_islocaltemp &#x3D; false, rd_isnailed &#x3D; true, rd_isvalid &#x3D; true, rd_indexvalid &#x3D; true, rd_statvalid &#x3D; false,</span><br><span class="line">  rd_createSubid &#x3D; 0, rd_newRelfilenodeSubid &#x3D; 0, rd_firstRelfilenodeSubid &#x3D; 0, rd_droppedSubid &#x3D; 0,</span><br><span class="line">  rd_rel &#x3D; 0x7f872f532438, rd_att &#x3D; 0x7f872f532548, rd_id &#x3D; 1247, rd_lockInfo &#x3D; &#123;lockRelId &#x3D; &#123;relId &#x3D; 1247, dbId &#x3D; 16384&#125;&#125;,</span><br><span class="line">  rd_rules &#x3D; 0x0, rd_rulescxt &#x3D; 0x0, trigdesc &#x3D; 0x0, rd_rsdesc &#x3D; 0x0, rd_fkeylist &#x3D; 0x0, rd_fkeyvalid &#x3D; false,</span><br><span class="line">  rd_partkey &#x3D; 0x0, rd_partkeycxt &#x3D; 0x0, rd_partdesc &#x3D; 0x0, rd_pdcxt &#x3D; 0x0, rd_partcheck &#x3D; 0x0, rd_partcheckvalid &#x3D; false,</span><br><span class="line">  rd_partcheckcxt &#x3D; 0x0, rd_indexlist &#x3D; 0x7f872f477d00, rd_pkindex &#x3D; 0, rd_replidindex &#x3D; 0, rd_statlist &#x3D; 0x0,</span><br><span class="line">  rd_indexattr &#x3D; 0x0, rd_keyattr &#x3D; 0x0, rd_pkattr &#x3D; 0x0, rd_idattr &#x3D; 0x0, rd_pubactions &#x3D; 0x0, rd_options &#x3D; 0x0,</span><br><span class="line">  rd_amhandler &#x3D; 0, rd_tableam &#x3D; 0x55ba82562c20 &lt;heapam_methods&gt;, rd_index &#x3D; 0x0, rd_indextuple &#x3D; 0x0, rd_indexcxt &#x3D; 0x0,</span><br><span class="line">  rd_indam &#x3D; 0x0, rd_opfamily &#x3D; 0x0, rd_opcintype &#x3D; 0x0, rd_support &#x3D; 0x0, rd_supportinfo &#x3D; 0x0, rd_indoption &#x3D; 0x0,</span><br><span class="line">  rd_indexprs &#x3D; 0x0, rd_indpred &#x3D; 0x0, rd_exclops &#x3D; 0x0, rd_exclprocs &#x3D; 0x0, rd_exclstrats &#x3D; 0x0, rd_indcollation &#x3D; 0x0,</span><br><span class="line">  rd_opcoptions &#x3D; 0x0, rd_amcache &#x3D; 0x0, rd_fdwroutine &#x3D; 0x0, rd_toastoid &#x3D; 0, pgstat_info &#x3D; 0x55ba828d5cb0&#125;</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>With the asteroid, you can tell the <code>p</code> command to either print the address of a pointer or the values pointed by the pointer.</p><h4 id="5-3-The-x-examine-command"><a href="#5-3-The-x-examine-command" class="headerlink" title="5.3 The x (examine) command"></a>5.3 The <code>x</code> (examine) command</h4><p>The <code>x</code> command is used to examine a memory block contents with specified size and format. The following example tries to examine the <code>t_data</code> values inside a <code>HeapTuple</code> structure. Note that we first print the <code>*tup</code> pointer to learn the size of <code>t_data</code> is 176, then we use the <code>x</code> command to examine the first 176 bytes pointed by <code>t_data</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb)  p *tup</span><br><span class="line">$6 &#x3D; &#123;t_len &#x3D; 176, t_self &#x3D; &#123;ip_blkid &#x3D; &#123;bi_hi &#x3D; 65535, bi_lo &#x3D; 65535&#125;, ip_posid &#x3D; 0&#125;, t_tableOid &#x3D; 0,</span><br><span class="line">  t_data &#x3D; 0x55ba8290f790&#125;</span><br><span class="line"></span><br><span class="line">(gdb)  p tup-&gt;t_data</span><br><span class="line">$7 &#x3D; (HeapTupleHeader) 0x55ba8290f790</span><br><span class="line">(gdb) x&#x2F;176bx  tup-&gt;t_data</span><br><span class="line">0x55ba8290f790: 0xc0    0x02    0x00    0x00    0xff    0xff    0xff    0xff</span><br><span class="line">0x55ba8290f798: 0x47    0x00    0x00    0x00    0xff    0xff    0xff    0xff</span><br><span class="line">0x55ba8290f7a0: 0x00    0x00    0x1f    0x00    0x01    0x00    0x20    0xff</span><br><span class="line">0x55ba8290f7a8: 0xff    0xff    0x0f    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7b0: 0x0a    0x40    0x00    0x00    0x74    0x65    0x73    0x74</span><br><span class="line">0x55ba8290f7b8: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7c0: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7c8: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7d0: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7d8: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7e0: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7e8: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f7f0: 0x00    0x00    0x00    0x00    0x98    0x08    0x00    0x00</span><br><span class="line">0x55ba8290f7f8: 0x01    0x40    0x00    0x00    0xff    0xff    0x00    0x63</span><br><span class="line">0x55ba8290f800: 0x43    0x00    0x01    0x2c    0x08    0x40    0x00    0x00</span><br><span class="line">0x55ba8290f808: 0x00    0x00    0x00    0x00    0x09    0x40    0x00    0x00</span><br><span class="line">0x55ba8290f810: 0xf2    0x08    0x00    0x00    0xf3    0x08    0x00    0x00</span><br><span class="line">0x55ba8290f818: 0x62    0x09    0x00    0x00    0x63    0x09    0x00    0x00</span><br><span class="line">0x55ba8290f820: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">0x55ba8290f828: 0x00    0x00    0x00    0x00    0x64    0x78    0x00    0x00</span><br><span class="line">0x55ba8290f830: 0x00    0x00    0x00    0x00    0xff    0xff    0xff    0xff</span><br><span class="line">0x55ba8290f838: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><h3 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h3><p>In this blog, we have discussed about how to generate a useful crash dump file with sufficient debug symbols to help developers troubleshoot a crash issue in PostgreSQL and also in other applications. We have also discussed about a very powerful and useful debugger <code>gdb</code> and shared some of the most common commands that can be utilized to troubleshoot a crash issue from a core file. I hope the information here can help some developers out there to troubleshoot issues better.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;In this blog post, I</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="gdb" scheme="http://caryhuang.github.io/tags/gdb/"/>
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
  </entry>
  
  <entry>
    <title>In-Memory Table with Pluggable Storage API</title>
    <link href="http://caryhuang.github.io/2020/09/23/In-memory-Table-with-Pluggable-Storage-API/"/>
    <id>http://caryhuang.github.io/2020/09/23/In-memory-Table-with-Pluggable-Storage-API/</id>
    <published>2020-09-23T21:02:38.000Z</published>
    <updated>2020-09-25T21:02:58.453Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>This blog is to follow up on the post I published back in July, 2020 about achieving an in-memory table storage using PostgreSQL’s pluggable storage API. In the past few months, my team and I have made some progress and did a few POC patches to prove some of the unknowns and hypothesis and today I would like to share our progress.</p><h3 id="2-The-PostgreSQL-Buffer-Manager"><a href="#2-The-PostgreSQL-Buffer-Manager" class="headerlink" title="2. The PostgreSQL Buffer Manager"></a>2. The PostgreSQL Buffer Manager</h3><p>In the previous post, I mentioned that we would like to build a new in-memory based storage that is based on the existing buffer manager and its related component and hooked it up with the pluggable storage API. To achieve this, my team and I underwent an in-depth study to understand how the current buffer manager works in PostgreSQL and this chapter at <a href="http://www.interdb.jp/pg/pgsql08.html">interdb.jp</a> is a good starting point for us to gain a general understanding of the buffer manager design in good details.</p><p>The current PostgreSQL buffer manager follows a 3-layer buffer design to manage the data pages as illustrated by this image below:</p><p><img src="/images/bufmgr-3layer.png" alt="bufmgr"></p><p>where it consists of </p><ul><li>Buffer Table (hash table)</li><li>Buffer Descriptors (Array)</li><li>Buffer Pool (Array)</li></ul><h4 id="2-1-Page-Table"><a href="#2-1-Page-Table" class="headerlink" title="2.1 Page Table"></a>2.1 Page Table</h4><p>Buffer Table is used like a routing table between PostgreSQL Core and the buffer manager. It is managed using the existing hash table utilities and uses <code>buffer_tag</code> to look up the page descriptor and buffer id. Buffer_tag is a structure that contains the table space, database, table name.</p><h4 id="2-2-Buffer-Descriptor"><a href="#2-2-Buffer-Descriptor" class="headerlink" title="2.2 Buffer Descriptor"></a>2.2 Buffer Descriptor</h4><p>Buffer Descriptor is used to store the status of a buffer block and also the content lock. Refcount is a part of the buffer state, will be used to indicate the insert and delete operation. it will be increased by one when there is an insertion, and decreased by one when there is a deletion. The Vacuum process will reclaim this page once refcount reaches to 0.</p><h4 id="2-3-Buffer-Pool"><a href="#2-3-Buffer-Pool" class="headerlink" title="2.3 Buffer Pool"></a>2.3 Buffer Pool</h4><p>Buffer Pool has a one to one relationship with buffer descriptor. it can be treated a simple pointer pointing to the beginning of the buffer pool, each buffer pool slot is defined as 8KB for now. This is the lowest layer in the buffer manager structure before a page is flushed to disk. The <code>BM_DIRTY</code> status flag is used to indicate if a page in the buffer pool is to be flushed to disk</p><p>In addition to buffer pool, buffer manager also utilizes a ring buffer for reading and writing a huge table whose size exceeds 1/4 of the buffer pool size. <code>Clock Sweep</code> algorithm is used to find a victim page in the ring buffer to eject and flush to disk so new page can enter, thus the name, ring buffer.</p><h3 id="3-The-In-Memory-Only-Buffer-Manager"><a href="#3-The-In-Memory-Only-Buffer-Manager" class="headerlink" title="3. The In-Memory Only Buffer Manager"></a>3. The In-Memory Only Buffer Manager</h3><p>Having a general understanding of the existing buffer manager’s strucutre, we hypothesize that we could potentially improve its IO performance by eliminating the need to flush any buffer data to disk. This means that the in-memory only version of buffer manager itself is the storage media. For this reason, its strucutre can be simplified as:</p><p><img src="/images/bufmgr-2layer.png" alt="bufmgr"></p><p>where the buffer descriptor points to a dedicated memory storage that contains the actual page and tuple. This memory space can be allocated to a certain size at initlaization and there will not be a need to flush a page to disk. All data page and tuple will reside in this memory space. In the case where there is a huge reading and writing load, the ring buffer will not be allocated as the logic to find a victim page to evict and flush to disk will be removed since everything will reside in a dedicated memory space. For this reason, if the memory space is not sufficiently allocated, the user will get “no unpin buffer” is available, which basically means “your disk is full” and you need to do delete and vacuum. </p><p>Using this approach, when the server shuts down or restarts, the data in this memory space is of course lost. So, data persistence to disk would be a topic of interest next, but right now, we already see some useful business case with this in-memory based table where data processing speed is more important than data persistence</p><h3 id="4-Initial-Results"><a href="#4-Initial-Results" class="headerlink" title="4. Initial Results"></a>4. Initial Results</h3><p>Using the same tuple structure and logic as the current heap plus the memory based buffer manager with 1GB of memory allocated, we observe some interesting increase in performance comparing to PostgreSQL with default settings. For 20 million row, we observe about 50% increase for insert, 70% increase for update, 60% increase for delete and 30% increase in vacuum. This result is not too bad considering we are still at the early stages and I am sure there are many other ways to make it even faster</p><h3 id="5-Next-Step"><a href="#5-Next-Step" class="headerlink" title="5. Next Step"></a>5. Next Step</h3><p>Having some solid results from the initial test, it would make sense for us to also be looking into having the index tuples as in-memory storage only. In addition, free space map and visibility map files that are stored in the PG cluster directory could potentially also be made as in-memory to possibly further increase the DML performance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;This blog is to foll</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
    <category term="in-memory" scheme="http://caryhuang.github.io/tags/in-memory/"/>
    
    <category term="pluggable storage" scheme="http://caryhuang.github.io/tags/pluggable-storage/"/>
    
  </entry>
  
  <entry>
    <title>TLS Related Updates in PostgreSQL 13</title>
    <link href="http://caryhuang.github.io/2020/08/21/TLS-Related-Updates-in-PostgreSQL-13/"/>
    <id>http://caryhuang.github.io/2020/08/21/TLS-Related-Updates-in-PostgreSQL-13/</id>
    <published>2020-08-21T18:30:21.000Z</published>
    <updated>2020-08-21T23:06:48.813Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>The upcoming major release of PostgreSQL 13 has several important behavioral updates related to the TLS implementation. These updates may have some to your current PostgreSQL security deployment if you were considering to upgrade to 13 when it officially releases. Today I would like to do a quick summary of these updates.</p><h3 id="2-Minimum-TLS-Version-Changed-to-TLSv1-2"><a href="#2-Minimum-TLS-Version-Changed-to-TLSv1-2" class="headerlink" title="2. Minimum TLS Version Changed to TLSv1.2"></a>2. Minimum TLS Version Changed to TLSv1.2</h3><p>There are 2 server parameters in <code>postgresql.conf</code> that influence the desired TLS versions to use during communication, <code>ssl_min_protocol_version</code> and <code>ssl_max_protocol_version</code>. In previous PG versions, the default value for <code>ssl_min_protocol_version</code> was <code>TLSv1</code>, in which many older versions of OpenSSL could support. In PG13, this default has been raised to TLSv1.2, which satisfies current industry’s best practice. This means that if your psql client is built using older version of OpenSSL, such as 1.0.0., the PG13 server by default will deny this TLS connection, until either you rebuild the psql client with newer version of OpenSSL, or you lower the <code>ssl_min_protocol_version</code> back to <code>TLSv1</code>. Lowering this default is strongly discouraged as older versions of TLS are not as secured and many industrial applications are communicating using <code>TLSv1.2</code> as a standard requirement now.</p><h3 id="3-New-libpq-connection-parameters"><a href="#3-New-libpq-connection-parameters" class="headerlink" title="3. New libpq connection parameters"></a>3. New libpq connection parameters</h3><p>There are also several updates to the libpq client side connection parameters related to TLS. See sub sections below:</p><h4 id="3-1-ssl-min-protocol-version-and-ssl-max-protocol-version"><a href="#3-1-ssl-min-protocol-version-and-ssl-max-protocol-version" class="headerlink" title="3.1 ssl_min_protocol_version and ssl_max_protocol_version"></a>3.1 ssl_min_protocol_version and ssl_max_protocol_version</h4><p>The <code>ssl_min_protocol_version</code> and <code>ssl_max_protocol_version</code> parameters defined in the <code>postgresql.conf</code> on the server side were not available in the libpq client side in the previous PG versions. This means that in previous versions, it was not possible for client to influence the desired version of TLS to used as it would always want to used the newest TLS versions to communicate with the server. This may not be ideal in some cases. In PG13, the same sets of parameters are added to the libpq client side as well such that the client can also play a part in determining the ideal TLS version for communication. These parameters can be specified to <code>psql</code> like so:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">psql -U user -d &quot;sslmode&#x3D;require dbname&#x3D;postgres ssl_max_protocol_version&#x3D;TLSv1.2&quot;</span><br><span class="line">psql -U user -d &quot;sslmode&#x3D;require dbname&#x3D;postgres ssl_min_protocol_version&#x3D;TLSv1.3&quot;</span><br></pre></td></tr></table></figure><p>The second command in the above example sets minimum TLS protocol to <code>TLSv1.3</code>, which means that this client would only want to communicate with a server that could support TLSv1.3 as minimum version requirement. TLSv1.3 is fairly new and not every PostgreSQL servers are built with this support. In this case, the client simply refuses to communicate. These parameters are great additions to the current libpq as they give the client the ability to enforce the desired TLS version to use rather than simply letting the server to decide, resulting in a much more secured environment.</p><h4 id="3-2-channel-binding"><a href="#3-2-channel-binding" class="headerlink" title="3.2 channel_binding"></a>3.2 channel_binding</h4><p>Channel binding is a new feature introduced in PG13, in which the libpq client can optionally enable to further increase the security of the TLS connection. The <code>channel_binding</code> parameter can be set to <code>require</code> to enforce the channel binding feature, <code>prefer</code> to only use channel binding if it is available or <code>disable</code> to not use channel binding at all. <code>prefer</code> is the default value for the new <code>channel_binding</code> parameter.</p><p>The channel binding feature enforces trust between client and server so that Client informs the server whom it thinks it speaks to, and Server validates whether it is correct or not. This prevents an attacker who is able to capture users’ authentication credentials (e.g. OAuth tokens, session identifiers, etc) from reusing those credentials in another TLS sessions. In PG13, the channel binding is done by tying the user’s <code>scram-sha-256</code> credentials to a unique fingerprint of the TLS session in which they are used (channel binding), so they cannot be reused in another TLS sessions initiated by the attacker.</p><p>To use this feature, we need to define a rule in <code>pg_hba.conf</code> that uses <code>scram-sha-256</code> as authentication method. Ex:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostssl    all    all    127.0.0.1&#x2F;32    scram-sha-256</span><br></pre></td></tr></table></figure><p>also set the default password authentication method to <code>scram-sha-256</code> in <code>postgresql.conf</code>. </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">password_encryption &#x3D; scram-sha-256</span><br></pre></td></tr></table></figure><p>and then sets a scram-sha-256 password for the current user or another user in a existing psql connection</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\password</span><br><span class="line"></span><br><span class="line">or \password [username]</span><br></pre></td></tr></table></figure><p>then finally, we can use psql with channel binding to connect to the server. Ex:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">psql -U user -d &quot;sslmode&#x3D;require dbname&#x3D;postgres channel_binding&#x3D;require ssl_min_protocol_version&#x3D;TLSv1.2&quot;</span><br></pre></td></tr></table></figure><h4 id="3-3-sslpassword"><a href="#3-3-sslpassword" class="headerlink" title="3.3 sslpassword"></a>3.3 sslpassword</h4><p>In previous version of PG, in a <code>sslmode=verify-full</code> case, the client will need to specify its X509 certificate, private key and CA cert to complete the entire TLS authentication. In the case where the private key supplied by the client is encrypted with a password, the psql will prompt the user to enter it before proceeding with the TLS authentication. The new <code>sslpassword</code> parameter allows the user to specify the password to the connection parameters without prompting the user to enter it. This is a useful addition, as the psql command could basically completes without having a human or a bot to enter the passphrase to unlock the private key. This parameter can be used like this:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">psql -U user -d &quot;sslmode&#x3D;verify-full dbname&#x3D;postgres sslrootcert&#x3D;cacert.pem sslcert&#x3D;client.pem sslkey&#x3D;client.key sslpassword&#x3D;mypass&quot;</span><br></pre></td></tr></table></figure><h3 id="4-Remove-support-for-OpenSSL-0-9-8-and-1-0-0"><a href="#4-Remove-support-for-OpenSSL-0-9-8-and-1-0-0" class="headerlink" title="4. Remove support for OpenSSL 0.9.8 and 1.0.0"></a>4. Remove support for OpenSSL 0.9.8 and 1.0.0</h3><p>Another major change is that PG13 will refuse to be built with OpenSSL versions 1.0.0 and 0.9.8 as these are very old and are no longer considered secured in today’s standard. If you are still using these OpenSSL versions, you will need to upgrade to newer or recent versions to be compatible with PG13.</p><h3 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h3><p>PG13 brings many exciting new features and enhancements to PostgreSQL and many of these new changes need to be carefully assessed for potential incompatibility with the previous PG versions. Today, our focus is mainly on new updates related to TLS, which may have behavioral impacts to the existing deployment and the way existing client and server communicates using TLS. For the full changes and potential incompatibilities, visit the official change log <a href="https://www.postgresql.org/docs/release/13.0/">here</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;The upcoming major r</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postgresql" scheme="http://caryhuang.github.io/tags/postgresql/"/>
    
    <category term="tls" scheme="http://caryhuang.github.io/tags/tls/"/>
    
    <category term="pg13" scheme="http://caryhuang.github.io/tags/pg13/"/>
    
  </entry>
  
  <entry>
    <title>Approaches to Achieve in-memory table storage with PostgreSQL pluggable API</title>
    <link href="http://caryhuang.github.io/2020/07/23/Approaches-to-Achieve-in-memory-table-storage-with-PostgreSQL-pluggable-API/"/>
    <id>http://caryhuang.github.io/2020/07/23/Approaches-to-Achieve-in-memory-table-storage-with-PostgreSQL-pluggable-API/</id>
    <published>2020-07-23T20:06:49.000Z</published>
    <updated>2020-07-24T21:53:51.956Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Recently, I have had an opportunity to perform some in-depth feasibility study in-memory table using PostgreSQL’s pluggable storage API. The pluggable storage API was introduced Since PostgreSQL v12 and it allowed custom table storage Access Methods (AM for short) to be developed. Some famous examples include zheap from EDB, which aims to store a much more simplified tuple structure to achieve vacuum free storage and zedstore from Greenplum to utilize columnar storage. But they do not utilize in-memory storage and still interacts with existing buffer manager to persist data to disk. </p><p>Yes, it would be nice to have in-memory tables if they would perform faster. It would definitely be a very interesting challenge if we could achieve an in-memory table storage engine and design it in a way that will give serious performance advantages using the existing pluggable storage API architecture.</p><p>Developing a custom storage AM is no easy task and it requires a very deep understanding on how the current PostgreSQL storage engine works before we are able to improve it. In addition to redefining the tuple structure and custom algorithms to store and retrieve, it is also possible to define our own in-memory based storage module to handle the insertion and retrieval of tuple instead of utilizing the same buffer manager routines like what heap, zheap and zedstore access methods are using.</p><p>In this blog, I will briefly talk about the capability of pluggable storage API and share some progress on our in-memory table analysis</p><h3 id="2-The-Pluggable-Storage-Access-Method-API"><a href="#2-The-Pluggable-Storage-Access-Method-API" class="headerlink" title="2. The Pluggable Storage Access Method API"></a>2. The Pluggable Storage Access Method API</h3><p>PostgreSQL already has a pluggable index access method API for defining different index methods such as btree, gin and gist…etc where btree is the default index method today. The desired method can be selected when issuing the <code>CREATE INDEX</code> command like:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE INDEX gin_idx ON movies USING gin (year);</span><br><span class="line">CREATE INDEX gist_idx ON movies USING gist (year);</span><br></pre></td></tr></table></figure><p>Before PostgreSQL v12, there was not a pluggable access method for defining table storage and heap was the only access method available. After the introduction of pluggable storage API in v12, it is now possible to create custom storage access methods other than the default heap using similar syntax when creating a table like:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE ACCESS METHOD myheap TYPE TABLE HANDLER myheap_handler;</span><br><span class="line">CREATE TABLE mytable (a int, b char(10)) using myheap;</span><br></pre></td></tr></table></figure><p>There are a total of 38 callback functions provided by the pluggable API that requires to be implemented to develop a new table access method. They are defined in <code>TableAmRoutine</code> structure in <code>tableam.h</code>. It is quite tedious to explain all 38 callback functions here but in short, they primarily have to deal with:</p><ul><li>slot type</li><li>table sequential scan</li><li>parallel table scan</li><li>index scan</li><li>tuple visibility check</li><li>tuple modification, update, insert, etc</li><li>DDL related function, setting relfilenode, vacuum and …etc</li><li>TOAST information</li><li>planner time estimation</li><li>bitmap and sample scan functionality</li></ul><p>As you can see, the functionalities to be provided by these callback functions are very critical as they have direct impact on how efficient or inefficient it is to retrieve and store data</p><h3 id="3-Using-Pluggable-Storage-API-to-Achieve-in-memory-table"><a href="#3-Using-Pluggable-Storage-API-to-Achieve-in-memory-table" class="headerlink" title="3. Using Pluggable Storage API to Achieve in-memory table"></a>3. Using Pluggable Storage API to Achieve in-memory table</h3><p>The pluggable API does not directly deal with data storage to disk and it has to rely on interacting with <code>buffer manager</code>, which in turn, puts the tuple on disk via <code>storage manager</code>. This is a good thing, because with this architecture, we could potentially create a memory cache module and have the pluggable API’s tuple modification and scanning callbacks to interact with this new memory cache instead for faster tuple insertion and retrieval. </p><p>It is possible to achieve a very simple in-memory table with this approach, but there are some interesting considerations here. </p><ul><li>can existing buffer manager still play a role to perform data persistence on disk?</li><li>how large can memory cache size be?</li><li>how and when should memory cache persist data to disk?</li><li>can the access method work if it does not utilize buffer manager routines at all, ie. without CTID?</li><li>is existing buffer manager already acting like a in-memory data storage if its buffer pool is allocated large enough and we don’t mark a page as dirty, so in theory the data always stays in the buffer pool?</li></ul><h4 id="3-1-Buffer-Manager"><a href="#3-1-Buffer-Manager" class="headerlink" title="3.1 Buffer Manager"></a>3.1 Buffer Manager</h4><p>Let’s talk about buffer manager. PostgreSQL buffer manager is a very well-written module that works as an intermediate data page buffer before they are flushed to the disk. Existing heap access method and others like zheap and zedstore impementations make extensive use of buffer manager to achieve data storage and persistence on disk. In our in-memory table approach, we actually would like to skip the buffer manager routines and replace it with our own memory cache or similar. So in terms of architecture, it would look something like this where the green highlighted block is what we would like to achieve with in-memory table. </p><p><img src="/images/in-mem-table.png" alt="in-mem-table"></p><p>In this approach, the design of the memory cache component would be a challenge as it essentially replaces the functionality of existing buffer manager and utilize in-memory data storage instead. For this reason, the CTID value, which points directly to a specific data page managed by buffer manager, may not be valid anymore if buffer manager is no longer to be used. The concern would be if it is possible to implement a new access method without buffer manager and CTID? Our investigation shows yes, it is possible for the access method to not use buffer manager and CTID at all, but it would require the pluggable API to manually fill the <code>TupleTableSlot</code> when it has retrieved a tuple from memory cache.</p><h4 id="3-2-Buffer-Manager-as-In-memory-storage"><a href="#3-2-Buffer-Manager-as-In-memory-storage" class="headerlink" title="3.2 Buffer Manager as In-memory storage?"></a>3.2 Buffer Manager as In-memory storage?</h4><p>Another question that rises is that If the existing buffer manager has a large enough buffer pool and we never mark a page as dirty, the buffer manager theoretically will not flush a page to disk and in this case, will we have something very similar to the memory cache module? Theoretically yes, but unfortunately it is not how current buffer manager is designed to do. Buffer manager maintains a ring buffer with limited size and data is flushed to disk when new data enters or when they are deemed as “dirty”. It uses a 3-layer buffer structure to manage the location of each data page on disk and provides comprehensive APIs to the PostgreSQL backend processes to interact, insert and retrieve a tuple. If a tuple resides on the disk instead of in the buffer, it has to retrieve it from the disk. </p><p>In terms of time ticks, this is how PG retrieves a tuple from disk:</p><p><img src="/images/bufman-t.png" alt="buffer manager time ticks"></p><p>As you can see, it takes T1+T2+T3+T4+T5 to retrieve a tuple from disk. With the approach of in-memory table, we want to cut off the time takes to retrieve tuple from disk, like:</p><p><img src="/images/bufman-t2.png" alt="memory cache time ticks"></p><p>which takes T1’ + T2’ + T3’ to retrieve a tuple from the memory cache. This is where in-memory table implementation can help with performance increase. </p><h4 id="3-3-Ideal-Size-for-this-New-Memory-Cache"><a href="#3-3-Ideal-Size-for-this-New-Memory-Cache" class="headerlink" title="3.3 Ideal Size for this New Memory Cache?"></a>3.3 Ideal Size for this New Memory Cache?</h4><p>The official PostgreSQL documentation recommends allocation of 25% of all the available memory, but no more than 40%. The rest of the available memory should be reserved for kernel and data caching purposes. From this 25% ~ 40% of reserved memory for PG, we need to minus the shared memory allocations from other backend processes. The remainder would be the maximum size the memory cache can allocate and depending on the environment it may or may not be enough. See image below.</p><p><img src="/images/mem-size.png" alt="memory size allocation"></p><h3 id="4-Our-Approach"><a href="#4-Our-Approach" class="headerlink" title="4. Our Approach?"></a>4. Our Approach?</h3><p>Since our focus is primarily on the memory cache, which is an alternative to existing buffer manager, we would prefer to use the existing Heap tuple as data strucuture to begin with. This way, we can use the existing TOAST, vacuum, WAL, scanning logics and we will primarily focus on replacing its buffer manager interactions with memory-cache equivalent function calls. All this will be done as a separate extension using pluggable API, so it is still possible to use the default Heap access methods on some tables and use in-memory access methods for some other tables.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;Recently, I have had</summary>
      
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="pluggable api" scheme="http://caryhuang.github.io/tags/pluggable-api/"/>
    
    <category term="in-memory table" scheme="http://caryhuang.github.io/tags/in-memory-table/"/>
    
  </entry>
  
  <entry>
    <title>An Overview of PostgreSQL Backend Architecture</title>
    <link href="http://caryhuang.github.io/2020/06/15/An-Overview-of-PostgreSQL-Backend-Architecture/"/>
    <id>http://caryhuang.github.io/2020/06/15/An-Overview-of-PostgreSQL-Backend-Architecture/</id>
    <published>2020-06-15T18:06:09.000Z</published>
    <updated>2020-06-16T20:08:07.916Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>PostgreSQL backend is a collection of processes forked from the main process called <code>Postmaster</code>. Each forked process has different roles and responsibilities in the backend. This article describes the responsibility of core backend processes that power the PostgreSQL system as we know it today. The overall PostgreSQL backend architecture can be illustrated by the image below:</p><p><img src="/images/pg-arch.png" alt="pg-backend-architecture"></p><a id="more"></a><p>Postmaster is the first process to be started who has control of all the backend processes and is responsible for accepting and closing the database connections. At start up, the postmaster forks several backend processes that are intended to process different aspects of backend tasks, which we will be covering in this blog. When a user initiates a connection to the PostgreSQL database, the client process will send an authentication message to the Postmaster main process. The Postmaster main process authenticates the user according to the authentication methods configured and will fork a new session to provide service to this user only if the user passes authentication.</p><h3 id="2-BgWriter-Background-Writer-Process"><a href="#2-BgWriter-Background-Writer-Process" class="headerlink" title="2. BgWriter (Background Writer) Process"></a>2. BgWriter (Background Writer) Process</h3><p>The BgWriter process is a process that writes dirty pages in shared memory to disk. It has two functions: one is to periodically flush out the dirty data from the memory buffer to the disk to reduce the blocking during the query; the other is that the PG needs to write out all the dirty pages to the disk during the regular checkpoint. By BgWriter Writing out some dirty pages in advance, it can reduce the IO operations to be performed when setting checkpoints (A type of database recovery technology), so that the system’s IO load tends to be stable. BgWriter is a process added after PostgreSQL v8.0 and it has a dedicated section in <code>postgresql.conf</code> to configure its behavior. </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># - Background Writer -</span><br><span class="line"></span><br><span class="line">#bgwriter_delay &#x3D; 200ms                 # 10-10000ms between rounds</span><br><span class="line">#bgwriter_lru_maxpages &#x3D; 100            # max buffers written&#x2F;round, 0 disables</span><br><span class="line">#bgwriter_lru_multiplier &#x3D; 2.0          # 0-10.0 multiplier on buffers scanned&#x2F;round</span><br><span class="line">#bgwriter_flush_after &#x3D; 512kB           # measured in pages, 0 disables</span><br></pre></td></tr></table></figure><ul><li><p><strong>bgwriter_delay</strong>:<br>The time interval between two consecutive flush data in the backgroud writer process. The default value is 200, and the unit is milliseconds.</p></li><li><p><strong>bgwriter_lru_maxpages</strong>:<br>The maximum amount of data written by the backgroud writer process at a time. The default value is 100, in units of buffers. If the amount of dirty data is less than this value, the write operation is all completed by the backgroud writer process; conversely, if it is greater than this value, the greater part will be completed by the server process process. When the value is set to 0, it means that the backgroud writer writing process is disabled, and it is completely completed by the server process; when it is set to -1, it means that all dirty data is done by the backgroud writer. (Checkpoint operations are not included here)</p></li><li><p><strong>bgwriter_lru_multiplier</strong>:<br>This parameter indicates the number of data blocks written to the disk each time, of course, the value must be less than bgwriter_lru_maxpages. If the setting is too small, the amount of dirty data that needs to be written is greater than the amount of data written each time, so the remaining work that needs to be written to the disk needs to be completed by the server process process, which will reduce performance; if the value configuration is too large, the amount of dirty data written More than the number of buffers required at the time, which is convenient for applying for buffer work again later, and IO waste may occur at the same time. The default value of this parameter is 2.0.</p></li><li><p><strong>bgwriter_flush_after</strong>:<br>BgWriter is triggered when the data page size reaches bgwriter_flush_after, the default is 512KB.</p></li></ul><h3 id="3-WalWriter-Process"><a href="#3-WalWriter-Process" class="headerlink" title="3. WalWriter Process"></a>3. WalWriter Process</h3><p>The core idea of ​​Write Ahead Log (also called Xlog) is that the modification of data files must only occur after these modifications have been recorded in the log, that is, the log is written first before the data is written . Using this mechanism can avoid frequent data writing to the disk, and can reduce disk I/O. The database can use these WAL logs to recover the database after a database restart. WalWriter Process is a backend process responsible for ensuring the WAL files are properly written to the disk and its behavior is configurable with the following parameters set in <code>postgresql.conf</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"># WRITE-AHEAD LOG</span><br><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># - Settings -</span><br><span class="line"></span><br><span class="line">wal_level &#x3D; logical                     # minimal, replica, or logical</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#fsync &#x3D; on                             # flush data to disk for crash safety</span><br><span class="line">                                        # (turning this off can cause</span><br><span class="line">                                        # unrecoverable data corruption)</span><br><span class="line">#synchronous_commit &#x3D; on                # synchronization level;</span><br><span class="line">                                        # off, local, remote_write, remote_apply, or on</span><br><span class="line">#wal_sync_method &#x3D; fsync                # the default is the first option</span><br><span class="line">                                        # supported by the operating system:</span><br><span class="line">                                        #   open_datasync</span><br><span class="line">                                        #   fdatasync (default on Linux)</span><br><span class="line">                                        #   fsync</span><br><span class="line">                                        #   fsync_writethrough</span><br><span class="line">                                        #   open_sync</span><br><span class="line">#full_page_writes &#x3D; on                  # recover from partial page writes</span><br><span class="line">#wal_compression &#x3D; off                  # enable compression of full-page writes</span><br><span class="line">#wal_log_hints &#x3D; off                    # also do full page writes of non-critical updates</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#wal_init_zero &#x3D; on                     # zero-fill new WAL files</span><br><span class="line">#wal_recycle &#x3D; on                       # recycle WAL files</span><br><span class="line">#wal_buffers &#x3D; -1                       # min 32kB, -1 sets based on shared_buffers</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#wal_writer_delay &#x3D; 200ms               # 1-10000 milliseconds</span><br><span class="line">#wal_writer_flush_after &#x3D; 1MB           # measured in pages, 0 disables</span><br><span class="line"></span><br><span class="line">#commit_delay &#x3D; 0                       # range 0-100000, in microseconds</span><br><span class="line">#commit_siblings &#x3D; 5                    # range 1-1000</span><br></pre></td></tr></table></figure><ul><li><p><strong>wal_level</strong>:<br>Controls the level of wal storage. wal_level determines how much information is written to the WAL. The default value is <code>replica</code>, which adds WAL archive information and includes information required by read-only servers (streaming replicattion). It can also be set to <code>minimal</code>, which only writes the information needed to recover from a crash or immediate shutdown. Setting to <code>Logical</code> allows WAL streaming to be done in logical decoding scenarios.</p></li><li><p><strong>fsync</strong>:<br>This parameter directly controls whether the log is written to disk first. The default value is ON (write first), which means that the system shall ensure the change is indeed flushed to disk, by issuing the fsync command set by <code>wal_sync_method</code>. While turning off fsync is often a performance benefit, this can result in unrecoverable data corruption in the event of a power failure or system crash. Thus it is only advisable to turn off fsync if you can easily recreate your entire database from external data.</p></li><li><p><strong>synchronous_commit</strong>:<br>This parameter configures whether the system will wait for WAL to complete before returning status information to the user transaction. The default value is ON, indicating that it must wait for WAL to complete before returning transaction status information; configuring OFF can feed back the transaction status faster.</p></li><li><p><strong>wal_sync_method</strong>:<br>This parameter controls the fsync method of WAL writing to disk. The default value is fsync. The available values ​​include open_datasync, fdatasync, fsync_writethrough, fsync, and open_sync. open_datasync and open_sync respectively.</p></li><li><p><strong>full_page_writes</strong>:<br>indicates whether to write the entire page to the WAL.</p></li><li><p><strong>wal_buffers</strong>:<br>The amount of memory space used to store WAL data. The system default value is 64K. This parameter is also affected by the two parameters <code>wal_writer_delay</code> and <code>commit_delay</code>.</p></li><li><p><strong>wal_writer_delay</strong>:<br>The write interval of the WalWriter process. The default value is 200 milliseconds. If the time is too long, it may cause insufficient memory in the WAL buffer; if the time is too short, it will cause the WAL to continuously write, increasing the disk I/O burden.</p></li><li><p><strong>wal_writer_flush_after</strong>:<br>When dirty data exceeds this threshold, it will be flushed to disk.</p></li><li><p><strong>commit_delay</strong>:<br>indicates the time that the submitted data is stored in the WAL buffer. The default value is 0 milliseconds, which means no delay; when it is set to a non-zero value, the transaction will not be written to the WAL immediately after the commit is executed, but it is still stored in the WAL In the buffer, waiting for the WalWriter process to write to the disk periodically.</p></li><li><p><strong>commit_siblings</strong>:<br>When a transaction issues a commit request, if the number of transactions in the database is greater than the value of commit_siblings, the transaction will wait for a period of time (commit_delay value); otherwise, the transaction is directly written to WAL. The system default value is 5, and this parameter also determines the validity of <code>commit_delay</code>.</p></li></ul><h3 id="4-PgArch-Process"><a href="#4-PgArch-Process" class="headerlink" title="4. PgArch Process"></a>4. PgArch Process</h3><p>Similar to the ARCH archiving process in the Oracle database, the difference is that ARCH performs archiving on redo log while PgArch performs archiving on WAL logs. This is needed because the WAL log will be recycled. In other words, the WAL log in the past will be overwritten by the newly generated ones. The PgArch process is responsible for backing up the WAL log before they are overwritten. Starting from version 8.x, these WAL logs can then be used for PITR (Point-In-Time-Recovery), which restores the database state to a certain state at certain period of time. PgArch also has a dedicated section in <code>postgresql.conf</code> to configure its behavior.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># - Archiving -</span><br><span class="line"></span><br><span class="line">#archive_mode &#x3D; off             # enables archiving; off, on, or always</span><br><span class="line">                                # (change requires restart)</span><br><span class="line">#archive_command &#x3D; &#39;&#39;           # command to use to archive a logfile segment</span><br><span class="line">                                # placeholders: %p &#x3D; path of file to archive</span><br><span class="line">                                #               %f &#x3D; file name only</span><br><span class="line">                                # e.g. &#39;test ! -f &#x2F;mnt&#x2F;server&#x2F;archivedir&#x2F;%f &amp;&amp; cp %p &#x2F;mnt&#x2F;server&#x2F;archivedir&#x2F;%f&#39;</span><br><span class="line">#archive_timeout &#x3D; 0            # force a logfile segment switch after this</span><br><span class="line">                                # number of seconds; 0 disables</span><br></pre></td></tr></table></figure><ul><li><strong>archive_mode</strong>:</li></ul><p>Indicates whether to perform the archive operation; it can be set to (off), (on) or (always), the default value is off.</p><ul><li><strong>archive_command</strong>:</li></ul><p>The command set by the administrator for archiving WAL logs. In the command for archiving, the predefined variable “%p” is used to refer to the WAL full path file name that needs to be archived while “%f” indicates the file name without a path (the paths here are relative to the current working directory). When each WAL segment file is archived, the command specified by <code>archive_command</code> will be executed. If the archive command returns 0, PostgreSQL considers the file successfully archived, and then deletes or recycles the WAL segment file. If a non-zero value is returned, PostgreSQL will consider the file was not successfully archived, and will periodically retry until it succeeds.</p><ul><li><strong>archive_timeout</strong>:</li></ul><p>Indicates the archiving period. When the time set by this parameter is exceeded, the WAL segment is forcibly switched. The default value is 0 (function disabled).</p><h3 id="5-AutoVacuum-Process"><a href="#5-AutoVacuum-Process" class="headerlink" title="5. AutoVacuum Process"></a>5. AutoVacuum Process</h3><p>In PostgreSQL database, after performing UPDATE or DELETE operations on the data, the database will not immediately delete the old version of the data. Instead, the data will be marked as deleted by PostgreSQL’s multi-version mechanism. If these old versions of data are being accessed by other transactions, it is necessary to retain them temporarily. After the transaction is submitted, the old versions of the data are no longer required (dead tuples) and therefore the database needs to clean them up to make room. This task is performed by the AutoVacuum process and the parameters related to the AutoVacuum process are also in the postgresql.conf.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"># AUTOVACUUM</span><br><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">#autovacuum &#x3D; on                        # Enable autovacuum subprocess?  &#39;on&#39;</span><br><span class="line">                                        # requires track_counts to also be on.</span><br><span class="line">#log_autovacuum_min_duration &#x3D; -1       # -1 disables, 0 logs all actions and</span><br><span class="line">                                        # their durations, &gt; 0 logs only</span><br><span class="line">                                        # actions running at least this number</span><br><span class="line">                                        # of milliseconds.</span><br><span class="line">#autovacuum_max_workers &#x3D; 3             # max number of autovacuum subprocesses</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#autovacuum_naptime &#x3D; 1min              # time between autovacuum runs</span><br><span class="line">#autovacuum_vacuum_threshold &#x3D; 50       # min number of row updates before</span><br><span class="line">                                        # vacuum</span><br><span class="line">#autovacuum_analyze_threshold &#x3D; 50      # min number of row updates before</span><br><span class="line">                                        # analyze</span><br><span class="line">#autovacuum_vacuum_scale_factor &#x3D; 0.2   # fraction of table size before vacuum</span><br><span class="line">#autovacuum_analyze_scale_factor &#x3D; 0.1  # fraction of table size before analyze</span><br><span class="line">#autovacuum_freeze_max_age &#x3D; 200000000  # maximum XID age before forced vacuum</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#autovacuum_multixact_freeze_max_age &#x3D; 400000000        # maximum multixact age</span><br><span class="line">                                        # before forced vacuum</span><br><span class="line">                                        # (change requires restart)</span><br><span class="line">#autovacuum_vacuum_cost_delay &#x3D; 2ms     # default vacuum cost delay for</span><br><span class="line">                                        # autovacuum, in milliseconds;</span><br><span class="line">                                        # -1 means use vacuum_cost_delay</span><br><span class="line">#autovacuum_vacuum_cost_limit &#x3D; -1      # default vacuum cost limit for</span><br><span class="line">                                        # autovacuum, -1 means use</span><br><span class="line">                                        # vacuum_cost_limit</span><br></pre></td></tr></table></figure><ul><li><p><strong>autovacuum</strong>:<br>whether to start the auto vacuum process automatically, the default value is on.</p></li><li><p><strong>log_autovacuum_min_duration</strong>:<br>This parameter records the execution time of autovacuum. When the execution time of autovaccum exceeds the setting of the <code>log_autovacuum_min_duration</code> parameter, this incident will be recorded in the log. The default is “-1”, which means no recording.</p></li><li><p><strong>autovacuum_max_workers</strong>:<br>Set the maximum number of autovacuum subprocesses</p></li><li><p><strong>autovacuum_naptime</strong>:<br>Set the interval time between two autovacuum processes.</p></li><li><p><strong>autovacuum_vacuum_threshold</strong> and <strong>autovacuum_analyze_threshold</strong>:<br>Set the threshold values of the number of updated tuples on the table, if number of tuple updates exceed these values, vacuum and analysis need to be performed respectively.</p></li><li><p><strong>autovacuum_vacuum_scale_factor</strong> and <strong>autovacuum_analyze_scale_factor</strong>:<br>Set the scaling factor for table size.</p></li><li><p><strong>autovacuum_freeze_max_age</strong>:<br>Set the upper limit of transaction ID that needs to be forced to clean up the database.</p></li><li><p><strong>autovacuum_vacuum_cost_delay</strong>:<br>When the autovacuum process is about to be executed, the vacuum execution cost is evaluated. If the value set by <code>autovacuum_vacuum_cost_limit</code> is exceeded, there will be a delay set by the <code>autovacuum_vacuum_cost_delay</code> parameter. If the value is -1, it means to use <code>vacuum_cost_delay</code> value instead. the default value is 20 ms.</p></li><li><p><strong>autovacuum_vacuum_cost_limi</strong>: This value is the evaluation threshold of the autovacuum process. The default is -1, which means to use the “vacuum_cost_limit” value. If the cost evaluated during the execution of the autovacuum process exceeds <code>autovacuum_vacuum_cost_limit</code>, the autovacuum process will sleep.</p></li></ul><h3 id="6-Stat-Collector"><a href="#6-Stat-Collector" class="headerlink" title="6. Stat Collector"></a>6. Stat Collector</h3><p>Stat collector is a statistical information collector of the PostgreSQL database. It collects statistical information during the operation of the database, such as the number of table additions, deletions, or updates, the number of data blocks, changes in indexes…etc. Collecting statistical information is mainly for the query optimizer to make correct judgment and choose the best execution plan. The parameters related to the Stat collector in the <code>postgresql.conf</code> file are as follows:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"># STATISTICS</span><br><span class="line">#------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># - Query and Index Statistics Collector -</span><br><span class="line"></span><br><span class="line">#track_activities &#x3D; on</span><br><span class="line">#track_counts &#x3D; on</span><br><span class="line">#track_io_timing &#x3D; off</span><br><span class="line">#track_functions &#x3D; none                 # none, pl, all</span><br><span class="line">#track_activity_query_size &#x3D; 1024       # (change requires restart)</span><br><span class="line">#stats_temp_directory &#x3D; &#39;pg_stat_tmp&#39;</span><br></pre></td></tr></table></figure><ul><li><p><strong>track_activities</strong>:<br>Indicates whether to enable the statistical information collection function for the command currently executed in the session. This parameter is only visible to the super user and session owner. The default value is on.</p></li><li><p><strong>track_counts</strong>:<br>indicates whether to enable the statistical information collection function for database activities. Since the database to be cleaned is selected in the AutoVacuum automatic cleaning process, the database statistical information is required, so the default value of this parameter is on.</p></li><li><p><strong>track_io_timing</strong>:<br>Timely call data block I/O, the default is off, because set to the on state will repeatedly call the database time, which adds a lot of overhead to the database. Only super user can set</p></li><li><p><strong>track_functions</strong>:<br>indicates whether to enable the number of function calls and time-consuming statistics.</p></li><li><p><strong>track_activity_query_size</strong>:<br>Set the number of bytes used to track the currently executed command of each active session. The default value is 1024, which can only be set after the database is started.</p></li><li><p><strong>stats_temp_directory</strong>:<br>Temporary storage path for statistical information. The path can be a relative path or an absolute path. The default parameter is pg_stat_tmp. This parameter can only be modified in the postgresql.conf file or on the server command line.</p></li></ul><h3 id="7-Checkpointer-Process"><a href="#7-Checkpointer-Process" class="headerlink" title="7. Checkpointer Process"></a>7. Checkpointer Process</h3><p>The checkpointer is a sequence of transaction points set by the system. Setting the checkpoint ensures that the WAL log information before the checkpoint is flushed to the disk. In the event of a crash, the crash recovery procedure looks at the latest checkpoint record to determine the point in the log (known as the redo record) from which it should start the REDO operation. The relevant parameters in the postgresql.conf file are:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># - Checkpoints -</span><br><span class="line"></span><br><span class="line">#checkpoint_timeout &#x3D; 5min              # range 30s-1d</span><br><span class="line">max_wal_size &#x3D; 1GB</span><br><span class="line">min_wal_size &#x3D; 80MB</span><br><span class="line">#checkpoint_completion_target &#x3D; 0.5     # checkpoint target duration, 0.0 - 1.0</span><br><span class="line">#checkpoint_flush_after &#x3D; 256kB         # measured in pages, 0 disables</span><br><span class="line">#checkpoint_warning &#x3D; 30s               # 0 disables</span><br></pre></td></tr></table></figure><ul><li><p><strong>checkpoint_timeout</strong>:<br>this parameter configures the period of performing a checkpoint. The default is 5 minutes. This means a checkpoint will occur every 5 minutes or when <code>max_wal_size</code> is about to be exceeded. Default is 1GB.</p></li><li><p><strong>max_wal_size</strong>:<br>this parameter sets the max WAL size before a checkpoint will happen</p></li><li><p><strong>min_wal_size</strong>:<br>this parameter sets a minimum on the amout of WAL files recycled for future usage</p></li><li><p><strong>checkpoint_completion_target</strong>:<br>To avoid flooding the I/O system with a burst of page writes, writing dirty buffers during a checkpoint is spread over a period of time. That period is controlled by <code>checkpoint_completion_target</code>, which is given as a fraction of the checkpoint interval.</p></li><li><p><strong>checkpoint_flush_after</strong>:<br>This parameter allows to force the OS that pages written by the checkpoint should be flushed to disk after a configurable number of bytes. Otherwise, these pages may be kept in the OS’s page cache. Default value is 256kB</p></li><li><p><strong>checkpoint_warning</strong>:<br>Checkpoints are faily expensive operation. This parameter configures a threshold between each checkpoint and if checkpoints happen too close together than <code>checkpoint_warning</code>period, the system will output a warning in server log to recommend user to increase <code>max_wal_size</code></p></li></ul><h3 id="8-Shared-Memory-and-Local-Memory"><a href="#8-Shared-Memory-and-Local-Memory" class="headerlink" title="8. Shared Memory and Local Memory"></a>8. Shared Memory and Local Memory</h3><p>When PostgreSQL server starts, a shared memory will be allocated to be used as a buffer of data blocks to improve the reading and writing capabilities.The WAL log buffer and CLOG buffer also exist in shared memory. Some global information such as process information, lock information, global statistics, etc are all stored in shared memory</p><p>In addition to the shared memory, the background services will also allocate some local memory to temporarily store data that does not require global storage. These memory buffers mainly include the following categories:</p><ul><li>Temporary buffer: local buffer used to access temporary tables</li><li>work_mem: Memory buffering used by memory sort operations and hash tables before using temporary disk files.</li><li>maintenance_work_mem: Memory buffer used in maintenance operations (such as vacuum, create index, and alter table add foreign key, etc.).</li></ul><h3 id="9-Summary"><a href="#9-Summary" class="headerlink" title="9. Summary"></a>9. Summary</h3><p>This blog provides an overview of the core backend processes that drive the PostgreSQL as we see it today and they serve as foundations to database performance tuning. There are many parameters that can be changed to influence the behavior of these backend processes to make the database perform better, safer and faster. This will be a topic for the future.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;PostgreSQL backend is a collection of processes forked from the main process called &lt;code&gt;Postmaster&lt;/code&gt;. Each forked process has different roles and responsibilities in the backend. This article describes the responsibility of core backend processes that power the PostgreSQL system as we know it today. The overall PostgreSQL backend architecture can be illustrated by the image below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pg-arch.png&quot; alt=&quot;pg-backend-architecture&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="postmaster" scheme="http://caryhuang.github.io/tags/postmaster/"/>
    
    <category term="backend" scheme="http://caryhuang.github.io/tags/backend/"/>
    
    <category term="architecture" scheme="http://caryhuang.github.io/tags/architecture/"/>
    
  </entry>
  
  <entry>
    <title>Benefits of External Key Management System Over the Internal and how these could help securing PostgreSQL</title>
    <link href="http://caryhuang.github.io/2020/05/14/Benefits-of-External-Key-Management-System-Over-the-Internal-and-how-these-could-help-securing-PostgreSQL/"/>
    <id>http://caryhuang.github.io/2020/05/14/Benefits-of-External-Key-Management-System-Over-the-Internal-and-how-these-could-help-securing-PostgreSQL/</id>
    <published>2020-05-14T20:18:25.000Z</published>
    <updated>2020-05-15T20:52:22.806Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Data and user security have always been important considerations for small to large enterprises during the deployment of their database or application servers. PostgreSQL today has rich support for many network level and user level security features. These include TLS to secure database connections, internal user authentication, integration with external user authentication services such as RADIUS, LDAP and GSSAPI, and TLS certificate based user authentication …etc. However, it does not yet support Transparent Data Encryption (TDE) feature where all the database files and logs have an option to be encrypted before written to disk or decrypted when retrieving from the disk. This adds extra security measure to protect against disk theft. </p><p>All these features have something in common; they all use cryptographic keys (either symmetrical or asymmetrical, statically generated or exchanged on the fly using Diffie Hellman) in some ways to achieve the security goals. It is quite common for an organization to focus entirely on the actual data encryption part but pay minimal attention to the cryptographic keys that make the encryption possible. In fact, data encryption is the easy part, the protection of the cryptographic keys is often the hardest as it has several levels of complexities. </p><a id="more"></a><p>A group of members (including myself) from the PostgreSQL community are actively working on TDE and internal KMS features towards PG14 and there has been some good work done on the internal KMS module with future support of integrating with an external KMS already in discussion. You may find the current status and the work in progress from these links below:</p><p><a href="https://wiki.postgresql.org/wiki/Transparent_Data_Encryption#Scope_for_the_first_release_of_TDE">PostgreSQL TDE Wiki Page</a><br><a href="https://commitfest.postgresql.org/28/2196/">Internal KMS for PostgreSQL</a></p><p>Today I would like to discuss the benefits of external key management system over the internal.</p><h3 id="2-Compliance"><a href="#2-Compliance" class="headerlink" title="2. Compliance"></a>2. Compliance</h3><p>Perhaps one of the biggest benefits of having an external KMS is compliance. For organizations that are mandated by the government to have the <a href="https://csrc.nist.gov/publications/detail/fips/140/2/final">FIPS 140-2</a> compliance certification, the external KMS could potentially help them achieve FIPS 140-2 level 3 (tamper resistant key storage) certification. Internal KMS, on the other hand, may provide FIPS 140-2 level 1 (stop the use of unsafe algorithms) and at most up to FIPS 140-2 level 2 (store keys in tamper proof evident hardware). </p><p>Depending on the industry governance, there may be a strict requirement that any key materials and encrypted data must be stored separately. For example, in the Payment Card Industry Data Security Standard (PCI DSS) requires that the cardholder data and encryption keys must be protected and stored separately. </p><p>For this reason alone, certain data sensitive organizations cannot consider PostgreSQL as their choice of database due to lack of support to external KMS. </p><h3 id="3-Deployment-Flexibility"><a href="#3-Deployment-Flexibility" class="headerlink" title="3. Deployment Flexibility"></a>3. Deployment Flexibility</h3><p>Another benefit of external KMS is the flexibility in deployment. When deploying their IT infrastructure, many organizations face the decision whether to maintain all applications including key management system on site or host them in a dedicated data center or even to the cloud. With external KMS, it is possible for these organizations to have a future proof deployment where all the cryptographic keys are centrally managed and will ensure their solution will work in any of the deployment scenarios. With internal KMS, these organizations will not have the flexibility in the deployment because the application will be tied to the storage space within itself. </p><p>Taking PostgreSQL as an example, each database cluster has a different storage space. With the internal KMS, each cluster manages its own set of cryptographic keys. In a larger deployment scenario where multiple PG instances will be deployed, it will become very difficult to manage the life cycle of the cryptographic keys used in each cluster.</p><h3 id="4-Complete-Life-Cycle-Management-for-Encryption-Keys"><a href="#4-Complete-Life-Cycle-Management-for-Encryption-Keys" class="headerlink" title="4. Complete Life Cycle Management for Encryption Keys"></a>4. Complete Life Cycle Management for Encryption Keys</h3><p>Normally, a complete life cycle of an encryption key is very likely involved in the following phases. Depending on the business cases, some phases can be omitted. </p><ul><li>Key generation</li><li>Key registration</li><li>Key storage</li><li>Key distribution and installation</li><li>Key usage</li><li>Key rotation</li><li>Key backup</li><li>Key recovery</li><li>Key revocation</li><li>Key suspension</li><li>Key destruction</li></ul><p>It is possible for an Internal KMS to support all of the phases within its storage space and allows an user to manage the key life cycle but this process will not scale as scope requirement increases. External KMS, on the other hand, provides a centralized life cycle management of all the keys that it is managing. This is a much simpler approach to manage all the key materials in a larger deployment scenario. While it is not a security best practice, it is quite common to start a project with the internal KMS and later migrate to external one. Many of the key manager software vendors have support for key migration. </p><h3 id="5-Security-Audit"><a href="#5-Security-Audit" class="headerlink" title="5. Security Audit"></a>5. Security Audit</h3><p>This is again a requirement for certain corporate and industry compliance where there must be a detailed audit log capturing all the key usages, rotations, who accessed the key and at what time. Depending on the requirement, certain alert mechanism may be required to be implemented to alert the key administrator about any potential issues that could rise from the cryptographic key operations. With an external KMS system, it tends to be much easier to streamline the key audit reports for all the keys it is managing and easier to prove to customers or potential auditors that the keys are indeed very secured and closely monitored. </p><p>Taking the PostgreSQL as an example and it’s current work on internal KMS, it is quite difficult to prove that the keys are stored securely without the auditing mechanism to closely monitor the key usages and their lifecycles. Even if there is a complete suite of auditing mechanism in place, the auditing is still a difficult and costly operation to perform especially when there are multiple servers deployed. This would result in all storage systems to be audited individually.</p><h3 id="6-Key-Management-Duty-Separation"><a href="#6-Key-Management-Duty-Separation" class="headerlink" title="6. Key Management Duty Separation"></a>6. Key Management Duty Separation</h3><p>Normally, the key administrators of an external KMS has the ability to configure permissions for all the cryptographic keys that it manages. Permissions such as intended purpose, owner, validity period and other user attributes. PostgreSQL and the current work on internal KMS, on the other hand, does not have this level of granularity in administrative roles. The database administrator is also the encryption key administrator. This may be an issue in certain compliance requirement like HIPAA where it is required that both roles must be separated for proper data access.</p><h3 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h3><p>External KMS indeed has several more benefits over the internal KMS as it provides additional management, compliance and control. It is an increasing trend that more security-conscious organizations are driving their integrations and deployments with an external KMS in the design. However, it does not mean the internal KMS should not be used at all. For smaller organziation and deployment, it is quite normal to start with the internal KMS and later migrate the key to an external KMS as the deployment gets larger in size. </p><p>There is an active internal KMS development in PostgreSQL community and it can achieve basic key life cycle management and the next big focus would be on Transparent Data Encryption (TDE) and eventually an extension that supports communication to an external KMS.</p><p>Key Management Interoperability Protocol (KMIP) is a communication standard set out by Organization for the Advancement of Structured Information Standards (OASIS) to enable a secured communication between key management systems and cryptographically-enabled applications such as PostgreSQL. There is not many C-based open source KMIP client implementations today that can be utilized to develop a PostgreSQL extension that acts as a KMIP client to talk to external KMS, but I am sure as externa key management becomes a mainstream, there will be many different versions of implementation emerging in the market.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;Data and user security have always been important considerations for small to large enterprises during the deployment of their database or application servers. PostgreSQL today has rich support for many network level and user level security features. These include TLS to secure database connections, internal user authentication, integration with external user authentication services such as RADIUS, LDAP and GSSAPI, and TLS certificate based user authentication …etc. However, it does not yet support Transparent Data Encryption (TDE) feature where all the database files and logs have an option to be encrypted before written to disk or decrypted when retrieving from the disk. This adds extra security measure to protect against disk theft. &lt;/p&gt;
&lt;p&gt;All these features have something in common; they all use cryptographic keys (either symmetrical or asymmetrical, statically generated or exchanged on the fly using Diffie Hellman) in some ways to achieve the security goals. It is quite common for an organization to focus entirely on the actual data encryption part but pay minimal attention to the cryptographic keys that make the encryption possible. In fact, data encryption is the easy part, the protection of the cryptographic keys is often the hardest as it has several levels of complexities. &lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="security" scheme="http://caryhuang.github.io/tags/security/"/>
    
    <category term="key management system" scheme="http://caryhuang.github.io/tags/key-management-system/"/>
    
    <category term="external key management system" scheme="http://caryhuang.github.io/tags/external-key-management-system/"/>
    
    <category term="tde" scheme="http://caryhuang.github.io/tags/tde/"/>
    
  </entry>
  
  <entry>
    <title>Can Sequence Relation be Logically Replicated?</title>
    <link href="http://caryhuang.github.io/2020/04/22/Can-Sequence-Relation-be-Logically-Replicated/"/>
    <id>http://caryhuang.github.io/2020/04/22/Can-Sequence-Relation-be-Logically-Replicated/</id>
    <published>2020-04-22T17:56:57.000Z</published>
    <updated>2020-04-23T17:09:01.368Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>I have noticed that there is a page on the offical PostgreSQL documentation (<a href="https://www.postgresql.org/docs/current/logical-replication-restrictions.html">https://www.postgresql.org/docs/current/logical-replication-restrictions.html</a>) that states several restrictions to the current logical replication design. One of the restrictions is about sequence relation type where any changes associated with a sequence relation type is not logically replicated to the subscriber or to the decoding plugin. This is an interesting restriction and I took the initiative to look into this restriction further and evaluate if it is possible to have it supported. I have consulted several senior members in the PostgreSQL communitiy and got some interesting responses from them. In this blog, I will share my current work in the area of supporting sequence replication.</p><a id="more"></a><h3 id="2-What-is-a-Sequence"><a href="#2-What-is-a-Sequence" class="headerlink" title="2. What is a Sequence?"></a>2. What is a Sequence?</h3><p>Sequence is a special type of relation that is used as a number generator manager, which allows an user to request the next number from the sequence, reset the current value, change the size of increment (or decrement) and perform several other configurations that suit their needs. A sequence is automatically created when an user creates a regular table that contains a column of type <code>SERIAL</code>. Alternatively, a sequence can also be created manually by using the <code>CREATE SEQUENCE seqname;</code> command. A sequence is similar to a regular table except that it can only contain 1 single row, is created with a special schema by default that contains several control parameters for managing the number generation and user cannot use <code>UPDATE</code> clause on a sequence. SQL functions such as <code>nextval()</code>, <code>currval()</code>, <code>setval()</code> and <code>ALTER</code> commands are the proper methods of accessing or modifying sequence data.</p><h3 id="3-Why-is-Sequence-not-Replicated-in-Current-Design"><a href="#3-Why-is-Sequence-not-Replicated-in-Current-Design" class="headerlink" title="3. Why is Sequence not Replicated in Current Design?"></a>3. Why is Sequence not Replicated in Current Design?</h3><p>This is the question I ask myself and the PostgreSQL community for several times and I have received several interesting responses to this question. Like a regular table, sequence also emits a WAL update upon a change to the sequence value but with a major difference. Instead of emitting a WAL update at every <code>nextval()</code> call, sequence actually does this at every 32 increments and it logs a future value 32 increments after instead of current value. Doing WAL logging every 32 increments adds a significant gain in performance according to a benchmark report shared by the community. For example, if current sequence value is 50 with increment of 5, the value that is written to WAL record will be 210, because ( 50 + (32x5) = 210). This also means that in an events of a crash, some sequence values will be lost. Since sequence does not guarentee free of gap and is not part of user data, such a sequence loss is generally ok. </p><p>Logical replication is designed to track the WAL changes and report to subscribers about the current states and values. It would be quite contradicting to replicate sequence because the current sequence value does not equal to the value stored in the WAL. The subscriber in the sequence case will receive a value that is 32 increments in the future.</p><p>Another response I have got is that the implementation of sequence intermixed a bunch of transactional and non-transactional states in a very messy way, thus making it difficult to achieve sensible behaviour for logical decoding. </p><h3 id="4-Can-Sequence-Relation-be-Logically-Replicated"><a href="#4-Can-Sequence-Relation-be-Logically-Replicated" class="headerlink" title="4. Can Sequence Relation be Logically Replicated?"></a>4. Can Sequence Relation be Logically Replicated?</h3><p>In the current PostgreSQL logical replication architecture, yes it is possible to have a patch to replicate changes to a sequence relation. Before we dive in further, we have to understand what the benefit would be if we were able to replicate a sequence. In the current design, an user is able to set up a PostgreSQL publisher and subscriber to replicate a table that could be associated with a sequence if it has a column of data type <code>SERIAL</code>. The values of the table will be copied to the subscriber of course, but the state of sequence will not. In the case of a failover, the subscriber may not be able to insert more data to the table because <code>SERIAL</code> data is often declared as <code>PRIMARY KEY</code> and it could use an unexpected sequence value that conflicts with existing records. To remedy this, PostgreSQL documentation suggests manually copying over the sequence values or use utility such as <code>pg_dump</code> to do the copying. I believe it is the biggest benefit if sequence relation can be replicated such that in a fail over case, the user is no longer required to manually synchronize the sequence states. </p><h3 id="5-Where-to-Add-the-Sequence-Replication-Support"><a href="#5-Where-to-Add-the-Sequence-Replication-Support" class="headerlink" title="5. Where to Add the Sequence Replication Support?"></a>5. Where to Add the Sequence Replication Support?</h3><p>Logical replication actually has 2 routes, first is via the logical decoding plugin to a third party subscriber, second is between a PostgreSQL publisher and subscriber. Both routes are achieved differently in multiple source files but both do invoke the same common modules in the replication module in the PostgreSQL source repository. This section will describe briefly these common modules </p><h4 id="5-1-Define-a-New-Change-Type"><a href="#5-1-Define-a-New-Change-Type" class="headerlink" title="5.1 Define a New Change Type"></a>5.1 Define a New Change Type</h4><p>Since sequence change has some fundamental difference between the usual changes caused by INSERT, UPDATE or DELETE, it is better to define a new change type for sequence in reorderbuffer.h first:</p><figure class="highlight c"><figcaption><span>src/include/replication/reorderbuffer.h</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> ReorderBufferChangeType</span><br><span class="line">&#123;</span><br><span class="line">REORDER_BUFFER_CHANGE_INSERT,</span><br><span class="line">REORDER_BUFFER_CHANGE_UPDATE,</span><br><span class="line">REORDER_BUFFER_CHANGE_DELETE,</span><br><span class="line">REORDER_BUFFER_CHANGE_MESSAGE,</span><br><span class="line">REORDER_BUFFER_CHANGE_INTERNAL_SNAPSHOT,</span><br><span class="line">REORDER_BUFFER_CHANGE_INTERNAL_COMMAND_ID,</span><br><span class="line">REORDER_BUFFER_CHANGE_INTERNAL_TUPLECID,</span><br><span class="line">REORDER_BUFFER_CHANGE_INTERNAL_SPEC_INSERT,</span><br><span class="line">REORDER_BUFFER_CHANGE_INTERNAL_SPEC_CONFIRM,</span><br><span class="line">REORDER_BUFFER_CHANGE_TRUNCATE,</span><br><span class="line"><span class="comment">/* added a new CHANGE TYPE */</span></span><br><span class="line">REORDER_BUFFER_CHANGE_SEQUENCE,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Create a new struct that stores the context data for sequence changes within the <code>ReorderBufferChange</code> union</p><figure class="highlight c"><figcaption><span>src/include/replication/reorderbuffer.h</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ReorderBufferChange</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">union</span></span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Context data for Sequence changes</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">RelFileNode relnode;</span><br><span class="line">ReorderBufferTupleBuf *newtuple;</span><br><span class="line">&#125; sequence;</span><br><span class="line">&#125;data;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125; ReorderBufferChange;</span><br></pre></td></tr></table></figure><p>As you can see, for sequence change, we will only have the newtuple that represents the new sequence value. Old tuple is not needed here.</p><h4 id="5-2-The-Logical-Decoder-Module-src-backend-replication-logical-decode-c"><a href="#5-2-The-Logical-Decoder-Module-src-backend-replication-logical-decode-c" class="headerlink" title="5.2 The Logical Decoder Module (src/backend/replication/logical/decode.c)"></a>5.2 The Logical Decoder Module (src/backend/replication/logical/decode.c)</h4><p>This module decodes WAL records for the purpose of logical decoding, utilizes <code>snapbuild</code> module to build a fitting catalog snapshot and passes information to the <code>reorderbuffer</code> module for properly decoding the changes. </p><p>For every WAL log read, the handle will be passed to <code>LogicalDecodingProcessRecord</code> for further decoding. As you can see for the type <code>RM_SEQ_ID</code>, there is no dedicated decoding function invoked. We should create a dedicated decoding function called <code>DecodeSequence</code> and update the switch statement such that the sequence type will use this decoding method.</p><figure class="highlight c"><figcaption><span>src/backend/replication/logical/decode.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> </span><br><span class="line">LogicalDecodingProcessRecord(LogicalDecodingContext *ctx, XLogReaderState *record)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/* cast so we get a warning when new rmgrs are added */</span></span><br><span class="line"><span class="keyword">switch</span> ((RmgrId) XLogRecGetRmid(record))</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> RM_HEAP_ID:</span><br><span class="line">DecodeHeapOp(ctx, &amp;buf);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> RM_LOGICALMSG_ID:</span><br><span class="line">DecodeLogicalMsgOp(ctx, &amp;buf);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* added a new decoder function to handle the sequence type */</span></span><br><span class="line"><span class="keyword">case</span> RM_SEQ_ID:</span><br><span class="line">DecodeSequence(ctx, &amp;buf);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now, we shall define the <code>DecodeSequence</code> function to actually do the decoding. Comments are embedded in the below code block to explain what each line is doing briefly.</p><figure class="highlight c"><figcaption><span>src/backend/replication/logical/decode.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">DecodeSequence(LogicalDecodingContext *ctx, XLogRecordBuffer *buf)</span><br><span class="line">&#123;</span><br><span class="line">ReorderBufferChange *change;</span><br><span class="line">RelFileNode target_node;</span><br><span class="line">XLogReaderState *r = buf-&gt;record;</span><br><span class="line"><span class="keyword">char</span>   *tupledata = <span class="literal">NULL</span>;</span><br><span class="line">Sizetuplelen;</span><br><span class="line">Sizedatalen = <span class="number">0</span>;</span><br><span class="line">uint8info = XLogRecGetInfo(buf-&gt;record) &amp; ~XLR_INFO_MASK;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* only decode changes flagged with XLOG_SEQ_LOG  */</span></span><br><span class="line"><span class="keyword">if</span> (info != XLOG_SEQ_LOG)</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* only interested in our database */</span></span><br><span class="line">XLogRecGetBlockTag(r, <span class="number">0</span>, &amp;target_node, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"><span class="keyword">if</span> (target_node.dbNode != ctx-&gt;slot-&gt;data.database)</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* output plugin doesn't look for this origin, no need to queue */</span></span><br><span class="line"><span class="keyword">if</span> (FilterByOrigin(ctx, XLogRecGetOrigin(r)))</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Obtain the change from the decoding context */</span></span><br><span class="line">change = ReorderBufferGetChange(ctx-&gt;reorder);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Set the new Sequence change type */</span></span><br><span class="line">change-&gt;action = REORDER_BUFFER_CHANGE_SEQUENCE;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Set origin of the change. Used in logical decoding plugin to filter the sources of incoming changes */</span></span><br><span class="line">change-&gt;origin_id = XLogRecGetOrigin(r);</span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(&amp;change-&gt;data.sequence.relnode, &amp;target_node, <span class="keyword">sizeof</span>(RelFileNode));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* read the entire raw tuple data as a series of char */</span></span><br><span class="line">tupledata = XLogRecGetData(r);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* read the length of raw tuple data as a series of char */</span></span><br><span class="line">datalen = XLogRecGetDataLen(r);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* calculate the size of actual tuple by minusing the headers */</span></span><br><span class="line">tuplelen = datalen - SizeOfHeapHeader - <span class="keyword">sizeof</span>(xl_seq_rec);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* allocate a new tuple  */</span></span><br><span class="line">change-&gt;data.sequence.newtuple =</span><br><span class="line">ReorderBufferGetTupleBuf(ctx-&gt;reorder, tuplelen);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* decode the raw tuple data and save the results as new tuple */</span></span><br><span class="line">DecodeSeqTuple(tupledata, datalen, change-&gt;data.sequence.newtuple);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* set the catalog change, so snapbuild module will be called to build a snapshot for this sequence change */</span></span><br><span class="line">ReorderBufferXidSetCatalogChanges(ctx-&gt;reorder, XLogRecGetXid(buf-&gt;record), buf-&gt;origptr);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* queue this change in reorderbuffer module */</span></span><br><span class="line">ReorderBufferQueueChange(ctx-&gt;reorder, XLogRecGetXid(r), buf-&gt;origptr, change);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The above will call a new function <code>DecodeSeqTuple</code> to actually turn raw tuple data into a <code>ReorderBufferTupleBuf</code> which is needed in <code>reorderbuffer</code> module. This function tries to break down each section of the WAL (written by sequence.c) into a <code>ReorderBufferTupleBuf</code>. </p><figure class="highlight c"><figcaption><span>src/backend/replication/logical/decode.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">DecodeSeqTuple(<span class="keyword">char</span> *data, Size len, ReorderBufferTupleBuf *tuple)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span>datalen = len - <span class="keyword">sizeof</span>(xl_seq_rec) - SizeofHeapTupleHeader;</span><br><span class="line"></span><br><span class="line">Assert(datalen &gt;= <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">tuple-&gt;tuple.t_len = datalen + SizeofHeapTupleHeader;;</span><br><span class="line"></span><br><span class="line">ItemPointerSetInvalid(&amp;tuple-&gt;tuple.t_self);</span><br><span class="line"></span><br><span class="line">tuple-&gt;tuple.t_tableOid = InvalidOid;</span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(((<span class="keyword">char</span> *) tuple-&gt;tuple.t_data),</span><br><span class="line">   data + <span class="keyword">sizeof</span>(xl_seq_rec),</span><br><span class="line">   SizeofHeapTupleHeader);</span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(((<span class="keyword">char</span> *) tuple-&gt;tuple.t_data) + SizeofHeapTupleHeader,</span><br><span class="line">   data + <span class="keyword">sizeof</span>(xl_seq_rec) + SizeofHeapTupleHeader,</span><br><span class="line">   datalen);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-The-Reorder-Buffer-Module-src-backend-replication-reorderbuffer-c"><a href="#5-3-The-Reorder-Buffer-Module-src-backend-replication-reorderbuffer-c" class="headerlink" title="5.3 The Reorder Buffer Module (src/backend/replication/reorderbuffer.c)"></a>5.3 The Reorder Buffer Module (src/backend/replication/reorderbuffer.c)</h4><p>reorderbuffer module receives transaction records in the order they are written to the WAL and is primarily responsible for reassembling and passing them to the logical decoding plugin (test_decoding for example) with individual changes. The <code>ReorderBufferCommit</code> is the last function before the change is passed down to the logical decoding plugin by calling the <code>begin</code>, <code>change</code> and <code>commit</code> callback handlers. This is where we will add a new logics to pass a sequence change.</p><figure class="highlight c"><figcaption><span>src/backend/replication/reorderbuffer.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span></span><br><span class="line">ReorderBufferCommit(ReorderBuffer *rb, TransactionId xid,</span><br><span class="line">XLogRecPtr commit_lsn, XLogRecPtr end_lsn,</span><br><span class="line">TimestampTz commit_time,</span><br><span class="line">RepOriginId origin_id, XLogRecPtr origin_lsn)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">PG_TRY();</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">/* call the begin callback */</span></span><br><span class="line">rb-&gt;<span class="built_in">begin</span>(rb, txn);</span><br><span class="line"></span><br><span class="line">ReorderBufferIterTXNInit(rb, txn, &amp;iterstate);</span><br><span class="line"><span class="keyword">while</span> ((change = ReorderBufferIterTXNNext(rb, iterstate)) != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">Relationrelation = <span class="literal">NULL</span>;</span><br><span class="line">Oidreloid;</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (change-&gt;action)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> REORDER_BUFFER_CHANGE_SEQUENCE:</span><br><span class="line"><span class="comment">/* check on snapshot */</span></span><br><span class="line">Assert(snapshot_now);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* get the relation oid from sequence change context */</span></span><br><span class="line">reloid = RelidByRelfilenode(change-&gt;data.sequence.relnode.spcNode,</span><br><span class="line">change-&gt;data.sequence.relnode.relNode);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* check on relation oid */</span></span><br><span class="line"><span class="keyword">if</span> (reloid == InvalidOid)</span><br><span class="line">elog(ERROR, <span class="string">"could not map filenode \"%s\" to relation OID"</span>,</span><br><span class="line"> relpathperm(change-&gt;data.tp.relnode,</span><br><span class="line"> MAIN_FORKNUM));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* get the relation struct from relation oid */</span></span><br><span class="line">relation = RelationIdGetRelation(reloid);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* check on relation struct */</span></span><br><span class="line"><span class="keyword">if</span> (!RelationIsValid(relation))</span><br><span class="line">elog(ERROR, <span class="string">"could not open relation with OID %u (for filenode \"%s\")"</span>,</span><br><span class="line"> reloid,</span><br><span class="line"> relpathperm(change-&gt;data.sequence.relnode,</span><br><span class="line"> MAIN_FORKNUM));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* call the change callback */</span></span><br><span class="line"><span class="keyword">if</span> (RelationIsLogicallyLogged(relation))</span><br><span class="line">rb-&gt;apply_change(rb, txn, relation, change);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/* call commit callback */</span></span><br><span class="line">rb-&gt;commit(rb, txn, commit_lsn);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">PG_CATCH();</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">PG_END_TRY();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Once the decoding plugin receives a change of type <code>REORDER_BUFFER_CHANGE_SEQUENCE</code>, it will need to handle it and look up the proper change context to get the tuple information</p><figure class="highlight c"><figcaption><span>contrib/test_decoding/test_decoding.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">pg_decode_change(LogicalDecodingContext *ctx, ReorderBufferTXN *txn,</span><br><span class="line"> Relation relation, ReorderBufferChange *change)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (change-&gt;action)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> REORDER_BUFFER_CHANGE_INSERT:</span><br><span class="line">...</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> REORDER_BUFFER_CHANGE_UPDATE:</span><br><span class="line">...</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> REORDER_BUFFER_CHANGE_DELETE:</span><br><span class="line">...</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> REORDER_BUFFER_CHANGE_SEQUENCE:</span><br><span class="line"><span class="comment">/* print the sequence tuple out */</span></span><br><span class="line">appendStringInfoString(ctx-&gt;out, <span class="string">" SEQUENCE:"</span>);</span><br><span class="line"><span class="keyword">if</span> (change-&gt;data.sequence.newtuple == <span class="literal">NULL</span>)</span><br><span class="line">appendStringInfoString(ctx-&gt;out, <span class="string">" (no-tuple-data)"</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">tuple_to_stringinfo(ctx-&gt;out, tupdesc,</span><br><span class="line">&amp;change-&gt;data.sequence.newtuple-&gt;tuple,</span><br><span class="line"><span class="literal">false</span>);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">Assert(<span class="literal">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h3><p>We have discussed about the current implementation of logical decoding and some potential reasons why sequence is not supported in PostgreSQL logical replication. We have also gone through some important source files that could be updated to allow sequence replication. In the above approach, whenever the sequence module emits a WAL update, (which is a future value 32 increments later as discussed previously), the logical decoding plugin will receive this same future value, which is in fact different from the actual sequence value currently. This can be justified if we think about the purpose of sequence replication for a second, which is useful in fail over cases. With this future sequence value, the subsequent data insersions will be able to continue starting from this future sequence value.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;I have noticed that there is a page on the offical PostgreSQL documentation (&lt;a href=&quot;https://www.postgresql.org/docs/current/logical-replication-restrictions.html&quot;&gt;https://www.postgresql.org/docs/current/logical-replication-restrictions.html&lt;/a&gt;) that states several restrictions to the current logical replication design. One of the restrictions is about sequence relation type where any changes associated with a sequence relation type is not logically replicated to the subscriber or to the decoding plugin. This is an interesting restriction and I took the initiative to look into this restriction further and evaluate if it is possible to have it supported. I have consulted several senior members in the PostgreSQL communitiy and got some interesting responses from them. In this blog, I will share my current work in the area of supporting sequence replication.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="logical replication" scheme="http://caryhuang.github.io/tags/logical-replication/"/>
    
    <category term="logical decoding" scheme="http://caryhuang.github.io/tags/logical-decoding/"/>
    
    <category term="sequence" scheme="http://caryhuang.github.io/tags/sequence/"/>
    
  </entry>
  
  <entry>
    <title>Logical Replication Between PostgreSQL and MongoDB</title>
    <link href="http://caryhuang.github.io/2020/03/12/Logical-Replication-Between-PostgreSQL-and-MongoDB/"/>
    <id>http://caryhuang.github.io/2020/03/12/Logical-Replication-Between-PostgreSQL-and-MongoDB/</id>
    <published>2020-03-12T18:17:22.000Z</published>
    <updated>2020-03-24T18:11:21.323Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>PostgreSQL and MongoDB are two popular open source relational (SQL) and non-relational (NoSQL) databases available today. Both are maintained by groups of very experienced development teams globally and are widely used in many popular industries for adminitration and analytical purposes. MongoDB is a NoSQL Document-oriented Database which stores the data in form of key-value pairs expressed in JSON or BSON; it provides high performance and scalability along with data modelling and data management of huge sets of data in an enterprise application. PostgreSQL is a SQL database designed to handle a range of workloads in many applications supporting many concurrent users; it is a feature-rich database with high extensibility, which allows users to create custom plugins, extensions, data types, common table expressions to expand existing features </p><p>I have recently been involved in the development of a MongoDB Decoder Plugin for PostgreSQL, which can be paired with a logical replication slot to publish WAL changes to a subscriber in a format that MongoDB can understand. Basically, we would like to enable logical replication between MongoDB (as subscriber) and PostgreSQL (as publisher) in an automatic fashion. Since both databases are very different in nature, physical replication of WAL files is not applicable in this case. The logical replication supported by PostgreSQL is a method of replicating data objects changes based on replication identity (usually a primary key) and it would be the ideal choice for this purpose as it is designed to allow sharing the object changes between PostgreSQL and multiple other databases. The MongoDB Decoder Plugin will play a very important role as it is directly responsible for producing a series of WAL changes in a format that MongoDB can understand (ie. Javascript and JSON).</p><p>In this blog, I would like to share some of my initial research and design approach towards the development of MongoDB Decoder Plugin.</p><a id="more"></a><h3 id="2-Architecture"><a href="#2-Architecture" class="headerlink" title="2. Architecture"></a>2. Architecture</h3><p>Since it is not possible yet to establish a direct logical replication connection between PostgreSQL and MongoDB due to two very different implementations, some kind of software application is ideally required to act as a bridge between PostgreSQL and MongoDB to manage the subscription and publication. As you can see in the image below, the MongoDB Decoder Plugin associated with a logical replication slot and the bridge software application are required to achieve a fully automated replication setup. </p><p><img src="/images/mongo-logical-rep-arch.png" alt="architecutre"></p><p>Unfortunately, the bridge application does not exist yet, but we do have a plan to develop such application in near future. So, for now, we will not be able to have a fully automated logical replication setup. Fortunately, we can utilize the existing <code>pg_recvlogical</code> front end tool to act as a subscriber of database changes and publish these changes to MongoDb in the form of output file, as illustrated below.</p><p><img src="/images/mongo-logical-rep-arch-test.png" alt="architecutre"></p><p>With this setup, we are able to verify the correctness of the MongoDB Decoder Plugin output against a running MongoDB in a semi-automatic fashion.</p><h3 id="3-Plugin-Usage"><a href="#3-Plugin-Usage" class="headerlink" title="3. Plugin Usage"></a>3. Plugin Usage</h3><p>Based on the second architecture drawing above without the special bridge application, we expect the plugin to be used in similar way as normal logical decoding setup. The Mongodb Decoder Plugin is named <code>wal2mongo</code> as of now and the following examples show the envisioned procedures to make use of such plugin and replicate data changes to a MongoDB instance.</p><p>First, we will have to build and install <code>wal2mongo</code> in the <code>contrib</code> source folder and start a PostgreSQL cluster with the following parameters in <code>postgresql.conf</code>. The <code>wal_level = logical</code> tells PostgreSQL that the replication should be done logically rather than physically (<code>wal_level = replica</code>). Since we are setting up replication between 2 very different database systems in nature (PostgreSQL vs MongoDB), physical replication is not possible. All the table changes will be replicated to MongoDB in the form of logical commands. <code>max_wal_senders = 10</code> limits the maximum number of <code>wal_sender</code> proccesses that can be forked to publish changes to subscriber. The default value is 10, and is sufficient for our setup.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wal_level &#x3D; logical</span><br><span class="line">max_wal_senders &#x3D; 10</span><br></pre></td></tr></table></figure><p>On a psql client session, we create a new logical replication slot and associate it to the MongoDB logical decoding plugin. Replication slot is an important utility mechanism in logical replication and this blog from 2ndQuadrant has really good explaination of its purpose: (<a href="https://www.2ndquadrant.com/en/blog/postgresql-9-4-slots/">https://www.2ndquadrant.com/en/blog/postgresql-9-4-slots/</a>)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pg_create_logical_replication_slot(<span class="string">'mongo_slot'</span>, <span class="string">'wal2mongo'</span>);</span><br></pre></td></tr></table></figure><p>where <code>mongo_slot</code> is the name of the new logical replication slot and <code>wal2mongo</code> is the name of the logical decoding plugin that you have previously installed in the <code>contrib</code> folder. We can check the created replication slot with this command:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pg_replication_slots;</span><br></pre></td></tr></table></figure><p>At this point, the PostgreSQL instance will be tracking the changes done to the database. We can verify this by creating a table, inserting or deleting some values and checking the change with the command:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> pg_logical_slot_get_changes(<span class="string">'mongo_slot'</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure><p>Alternatively, one can use <code>pg_recvlogical</code> front end tool to subscribe to the created replication slot, automatically receives streams of changes in MongoDB format and outputs the changes to a file.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_recvlogical --slot mongo_slot --start -f mongodb.js</span><br></pre></td></tr></table></figure><p>Once initiated, <code>pg_recvlogical</code> will continuously stream database changes from the publisher and output the changes in MongoDB format and in <code>mongodb.js</code> as output file. It will continue to stream the changes until user manually terminates or the publisher has shutdown. This file can then be loaded to MongoDB using the <code>Mongo</code> client tool like this:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mongo &lt; mongodb.js</span><br><span class="line">MongoDB shell version v4.2.3</span><br><span class="line">connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodb</span><br><span class="line">Implicit session: session &#123; <span class="string">"id"</span> : UUID(<span class="string">"39d478df-b8ca-4030-8a05-0e1ebbf6bc44"</span>) &#125;</span><br><span class="line">MongoDB server version: 4.2.3</span><br><span class="line">switched to db mydb</span><br><span class="line">WriteResult(&#123; <span class="string">"nInserted"</span> : 1 &#125;)</span><br><span class="line">WriteResult(&#123; <span class="string">"nInserted"</span> : 1 &#125;)</span><br><span class="line">WriteResult(&#123; <span class="string">"nInserted"</span> : 1 &#125;)</span><br><span class="line"><span class="built_in">bye</span></span><br></pre></td></tr></table></figure><p>where the mongodb.js file contains:</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">use mydb;</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">1</span>, <span class="string">"b"</span>: <span class="string">"Cary"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;);</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">2</span>, <span class="string">"b"</span>: <span class="string">"David"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;);</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">3</span>, <span class="string">"b"</span>: <span class="string">"Grant"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;);</span><br></pre></td></tr></table></figure><h3 id="4-Terminology"><a href="#4-Terminology" class="headerlink" title="4. Terminology"></a>4. Terminology</h3><p>Both databases use different terminologies to describe the data storage. Before we can replicate the changes of PostgreSQL objects and translate them to MongoDB equivalent, it is important to gain clear understanding of the terminologies used on both databases. The table below is our initial terminology mappings:</p><table><thead><tr><th>PostgreSQL Terms</th><th>MongoDB Terms</th><th>MongoDB Description</th></tr></thead><tbody><tr><td>Database</td><td>Database</td><td>A physical container for collections</td></tr><tr><td>Table</td><td>Collection</td><td>A grouping of MongoDB documents, do not enforce a schema</td></tr><tr><td>Row</td><td>Document</td><td>A record in a MongoDB collection, can have difference fields within a collection</td></tr><tr><td>Column</td><td>Field</td><td>A name-value pair in a document</td></tr><tr><td>Index</td><td>Index</td><td>A data structure that optimizes queries</td></tr><tr><td>Primary Key</td><td>Primary Key</td><td>A record’s unique immutable identified. The _id field holds a document’s primary key which is usually a BSON ObjectID</td></tr><tr><td>Transaction</td><td>Transaction</td><td>Multi-document transactions are atomic and available in v4.2</td></tr></tbody></table><h3 id="5-Supported-Change-Operations"><a href="#5-Supported-Change-Operations" class="headerlink" title="5. Supported Change Operations"></a>5. Supported Change Operations</h3><p>Our initial design of the MongoDB Decoder Plugin is to support database changes caused by clauses “INSERT”, “UPDATE” and “DELETE”, with future support of “TRUNCATE”, and “DROP”. These are few of the most common SQL commands used to alter the contents of the database and they serve as a good starting point. To be able to replicate changes caused by these commands, it is important that the table is created with one or more primary keys. In fact, defining a primary key is required for logical replication to work properly because it serves as replication identity so the PostgreSQL can accurately track a table change properly. For example, if a row is deleted from a table that does not have a primary key defined, the logical replication process will only detect that there has been a delete event, but it will not be able to figure out which row is deleted. This is not what we want. The following is some basic examples of the SQL change commands and their previsioned outputs:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">BEGIN</span>;</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c) <span class="keyword">VALUES</span>(<span class="number">1</span>, <span class="string">'Cary'</span>, <span class="string">'2020-02-01'</span>);</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c) <span class="keyword">VALUES</span>(<span class="number">2</span>, <span class="string">'David'</span>, <span class="string">'2020-02-02'</span>);</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c) <span class="keyword">VALUES</span>(<span class="number">3</span>, <span class="string">'Grant'</span>, <span class="string">'2020-02-03'</span>);</span><br><span class="line">$ <span class="keyword">UPDATE</span> table1 <span class="keyword">SET</span> b=<span class="string">'Cary'</span>; </span><br><span class="line">$ <span class="keyword">UPDATE</span> table1 <span class="keyword">SET</span> b=<span class="string">'David'</span> <span class="keyword">WHERE</span> a = <span class="number">3</span>;</span><br><span class="line">$ <span class="keyword">DELETE</span> <span class="keyword">FROM</span> table1;</span><br><span class="line">$ <span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><p>The simple SQL commands above can be translated into the following MongoDB commands. This is a simple example to showcase the potential input and output from the plugin and we will introduce more blogs in the near future as the development progresses further to show case some more advanced cases.</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">db.table1.insert(&#123;“a”: <span class="number">1</span>, “b”: “Cary”, “c”: “<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;)</span><br><span class="line">db.table1.insert(&#123;“a”: <span class="number">2</span>, “b”: “David”, “c”: “<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;)</span><br><span class="line">db.table1.insert(&#123;“a”: <span class="number">3</span>, “b”: “Grant”, “c”: “<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;)</span><br><span class="line">db.table1.updateMany(&#123;“a”: <span class="number">1</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;, &#123;<span class="attr">$set</span>:&#123;“b”: “Cary”&#125;&#125;) </span><br><span class="line">db.table1.updateMany(&#123;“a”: <span class="number">2</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;, &#123;<span class="attr">$set</span>:&#123;“b”: “Cary”&#125;&#125;) </span><br><span class="line">db.table1.updateMany(&#123;“a”: <span class="number">3</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;, &#123;<span class="attr">$set</span>:&#123;“b”: “Cary”&#125;&#125;) </span><br><span class="line">db.table1.updateMany(&#123;“a”: <span class="number">3</span>, “c”: “<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”, &#123;<span class="attr">$set</span>:&#123;“b”: “David”&#125;&#125;)</span><br><span class="line">db.table1.remove(&#123;“a”: <span class="number">1</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;, <span class="literal">true</span>)</span><br><span class="line">db.table1.remove (&#123;“a”: <span class="number">2</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;, <span class="literal">true</span>)</span><br><span class="line">db.table1.remove (&#123;“a”: <span class="number">3</span>, “c”: ”<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><h3 id="6-Atomicity-and-Transactions"><a href="#6-Atomicity-and-Transactions" class="headerlink" title="6. Atomicity and Transactions"></a>6. Atomicity and Transactions</h3><p>A write operation in MongoDB is atomic on the level of a single document, and since MongoDB v4.0, multi-document transaction control is supported to ensure the atomicity of multi-document write operations. For this reason, the MongoDB Deocoder Plugin shall support 2 output modes, normal and transaction mode. </p><p>In normal mode, all the PostgreSQL changes will be translated to MongoDB equivalent without considering transactions. In other words, users cannot tell from the output if these changes are issued by the same or different transactions. The output can be fed directly to MongoDB, which can gurantee certain level of atomicity involving the same document</p><p>Since MongoDB v4.0, there is a support for multi-document transaction mechanism, which acts similarly to the transaction control in PostgreSQL. Consider a normal insert operation like this with transaction ID = 500 within database named “mydb” and having <code>cluster_name</code> = “mycluster” configured in <code>postgresql.conf</code>:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">BEGIN</span>;</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c)  </span><br><span class="line">  <span class="keyword">VALUES</span>(<span class="number">1</span>, <span class="string">'Cary'</span>, <span class="string">'2020-02-01'</span>);</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c) </span><br><span class="line">  <span class="keyword">VALUES</span>(<span class="number">2</span>, <span class="string">'Michael'</span>, <span class="string">'2020-02-02'</span>);</span><br><span class="line">$ <span class="keyword">INSERT</span> <span class="keyword">INTO</span> table1(a, b, c) </span><br><span class="line">  <span class="keyword">VALUES</span>(<span class="number">3</span>, <span class="string">'Grant'</span>, <span class="string">'2020-02-03'</span>);</span><br><span class="line">$ <span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><p>In normal output mode, the plugin will generate:</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">use mydb;</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">1</span>, <span class="string">"b"</span>: <span class="string">"Cary"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;);</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">2</span>, <span class="string">"b"</span>: <span class="string">"David"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;);</span><br><span class="line">db.table1.insert(&#123;<span class="string">"a"</span>: <span class="number">3</span>, <span class="string">"b"</span>: <span class="string">"Grant"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;);</span><br></pre></td></tr></table></figure><p>In transaction output mode, the plugin will generate:</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">session500_mycluster = db.getMongo().startSession();</span><br><span class="line">session500_mycluster.startTransaction();</span><br><span class="line">use mydb;</span><br><span class="line">session500_mycluster.getDatabase(<span class="string">"mydb"</span>).table1.insert(&#123;<span class="string">"a"</span>: <span class="number">1</span>, <span class="string">"b"</span>: <span class="string">"Cary"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-01</span>”&#125;);</span><br><span class="line">session500_mycluster.getDatabase(<span class="string">"mydb"</span>).table1.insert(&#123;<span class="string">"a"</span>: <span class="number">2</span>, <span class="string">"b"</span>: <span class="string">"David"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-02</span>”&#125;);</span><br><span class="line">session500_mycluster.getDatabase(<span class="string">"mydb"</span>).table1.insert(&#123;<span class="string">"a"</span>: <span class="number">3</span>, <span class="string">"b"</span>: <span class="string">"Grant"</span>, <span class="string">"c"</span>: “<span class="number">2020</span><span class="number">-02</span><span class="number">-03</span>”&#125;);</span><br><span class="line">session500_mycluster.commitTransaction();</span><br><span class="line">session500_mycluster.endSession();</span><br></pre></td></tr></table></figure><p>Please note that the session variable used in the MongoDB output is composed of the word <code>session</code> concatenated with the transaction ID and the cluster name. This is to gurantee that the variable name will stay unique when multiple PostgrSQL databases are publishing using the same plugin towards a single MongoDB instance. The <code>cluster_name</code> is a configurable parameter in <code>postgresql.conf</code> that is used to uniquely identify the PG cluster.</p><p>The user has to choose the desired output modes between normal and transaction depending on the version of the MongoDB instance. MongoDB versions before v4.0 do not support multi-document transaction mechanism so user will have to stick with the normal output mode. MongoDB versions after v4.0 have transaction mechanism supported and thus user can use either normal or transaction output mode. Generally, transaction output mode is recommended to be used when there are multiple PostgreSQL publishers in the network publishing changes to a single MongoDB instance.</p><h3 id="7-Data-Translation"><a href="#7-Data-Translation" class="headerlink" title="7. Data Translation"></a>7. Data Translation</h3><p>PostgreSQL supports far more data types than those supported by MongoDB, so some of the similar data types will be treated as one type before publishing to MongoDB. Using the same database name, transaction ID and cluster name in previous section, the table below shows some of the popular data types and their MongoDB transaltions. </p><table><thead><tr><th>PostgreSQL Datatype</th><th>MongoDB Datatype</th><th>Normal Output</th><th>Transaction Output</th></tr></thead><tbody><tr><td>smallint integer bigint numeric</td><td>integer</td><td>db.table1.insert({“a”:1})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert(“db”).table1.insert({“a”: 1})</td></tr><tr><td>character character varying text json xml composite default other types</td><td>string</td><td>db.table1.insert({“a”: “string_value”})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: “string_value”})</td></tr><tr><td>boolean</td><td>boolean</td><td>db.table1.insert({“a”:true})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: true})</td></tr><tr><td>double precision real serial arbitrary precision</td><td>double</td><td>db.table1.insert({“a”:34.56})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: 34.56})</td></tr><tr><td>interval timestamp data time with timezone time without timezone</td><td>timestamp</td><td>db.table1.insert({“a”: new Date(“2020-02-25T19:33:10Z”)})  db.table1.insert({“a”: new Date(“2020-02-25T19:33:10+06:00”)})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10Z”)})  session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10+06:00”)})</td></tr><tr><td>hex bytea bytea UUID</td><td>binary data</td><td>db.table1.insert({“a”: UUID(“123e4567-e89b-12d3-a456-426655440000”)})  db.table1.insert({“a”:HexData(0,”feffc2”)})</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:UUID(“123e4567-e89b-12d3-a456-426655440000”)})  session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:HexData(0,”feffc2”)})</td></tr><tr><td>array</td><td>array</td><td>db.table1.insert({ a: [ 1, 2, 3, 4, 5 ] } )  db.table1.insert({ a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } )</td><td>session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ 1, 2, 3, 4, 5 ] } )  session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } )</td></tr></tbody></table><h3 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8. Conclusion"></a>8. Conclusion</h3><p>MongoDB has gained a lot of popularity in recent years for its ease of development and scaling and is ideal database for data analytic purposes. Having the support to replicate data from multiple PostgreSQL clusters to a single MongoDB instance can bring a lot of value to industries focusing on data analytics and business intelligence. Building a compatible MongoDB Decoder Plugin for PostgreSQL is the first step for us and we will be sharing more information as development progresses further. The wal2mongo project is at WIP/POC stage and current work can be found here: <a href="https://github.com/HighgoSoftware/wal2mongo">https://github.com/HighgoSoftware/wal2mongo</a>.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;PostgreSQL and MongoDB are two popular open source relational (SQL) and non-relational (NoSQL) databases available today. Both are maintained by groups of very experienced development teams globally and are widely used in many popular industries for adminitration and analytical purposes. MongoDB is a NoSQL Document-oriented Database which stores the data in form of key-value pairs expressed in JSON or BSON; it provides high performance and scalability along with data modelling and data management of huge sets of data in an enterprise application. PostgreSQL is a SQL database designed to handle a range of workloads in many applications supporting many concurrent users; it is a feature-rich database with high extensibility, which allows users to create custom plugins, extensions, data types, common table expressions to expand existing features &lt;/p&gt;
&lt;p&gt;I have recently been involved in the development of a MongoDB Decoder Plugin for PostgreSQL, which can be paired with a logical replication slot to publish WAL changes to a subscriber in a format that MongoDB can understand. Basically, we would like to enable logical replication between MongoDB (as subscriber) and PostgreSQL (as publisher) in an automatic fashion. Since both databases are very different in nature, physical replication of WAL files is not applicable in this case. The logical replication supported by PostgreSQL is a method of replicating data objects changes based on replication identity (usually a primary key) and it would be the ideal choice for this purpose as it is designed to allow sharing the object changes between PostgreSQL and multiple other databases. The MongoDB Decoder Plugin will play a very important role as it is directly responsible for producing a series of WAL changes in a format that MongoDB can understand (ie. Javascript and JSON).&lt;/p&gt;
&lt;p&gt;In this blog, I would like to share some of my initial research and design approach towards the development of MongoDB Decoder Plugin.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="replication" scheme="http://caryhuang.github.io/tags/replication/"/>
    
    <category term="mongodb" scheme="http://caryhuang.github.io/tags/mongodb/"/>
    
    <category term="wal2mongo" scheme="http://caryhuang.github.io/tags/wal2mongo/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Security Features in PostgreSQL - Part 3</title>
    <link href="http://caryhuang.github.io/2020/01/20/Understanding-Security-Features-in-PostgreSQL-Part3/"/>
    <id>http://caryhuang.github.io/2020/01/20/Understanding-Security-Features-in-PostgreSQL-Part3/</id>
    <published>2020-01-20T21:47:59.000Z</published>
    <updated>2020-10-19T19:26:36.454Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>This is part 3 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing how to apply TLS in both PostgreSQL server and client using the principles we have learned in part 2 of the blog. In the end, I will also briefly talk about Transparent Data Encryption (TDE) and security vulnerability.</p><a id="more"></a><p>Here is the overview of the <code>security</code> topics that will be covered in all parts of the blog:</p><p>Part 1:   </p><ul><li>PostgreSQL Server Listen Address</li><li>Host-Based Authentication</li><li>Authentication with LDAP Server</li><li>Authentication with PAM</li><li>Role-Based Access Control </li><li>Assign Table and Column Level Privileges to Users</li><li>Assign User Level Privileges as Roles</li><li>Assign and Column Level Privileges via Roles</li><li>Role Inheritance</li></ul><p>Part 2:</p><ul><li>Security Concepts around TLS</li><li>Symmetrical Encryption</li><li>Asymmetrical Encryption (a.k.a Public Key Cryptography)</li><li>Block Cipher Mode of Operation (a.k.a Stream Cipher)</li><li>Key Exchange Algorithm</li><li>TLS Certificate and Chain of Trust</li><li>Data Integrity Check / Data Authentication</li><li>TLS Cipher Suite and TLS handshake</li><li>TLS versions</li></ul><p>Part 3: </p><ul><li>Preparing TLS Certificates</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Server</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Client</li><li>TLS Connect Examples</li><li>Transparent Data Encryption (TDE)</li><li>Security Vulnerability</li></ul><h3 id="2-Preparing-TLS-Certificates"><a href="#2-Preparing-TLS-Certificates" class="headerlink" title="2. Preparing TLS Certificates"></a>2. Preparing TLS Certificates</h3><p>Before we can utilize TLS to secure both the server and the client, we must prepare a set of TLS certificates to ensure mutual trust. Normally the CA (Certificate Authority) certificates can be purchased from a trusted organization and used it to create more CA-Signed certificates for services and applications. In this section, I will show you how to create your own CA Certificate and CA-Signed certificates using OpenSSL command line tool for both PostgreSQL server and client. </p><p>You may also have heard the term <code>self-signed</code> certificate. This type of certificate is not signed by a trusted CA and is normally considered insecured in many applications. We will not go over the self-signed certificate generation in this blog.</p><h4 id="2-1-Generate-a-Private-Key-for-CA-Certificate"><a href="#2-1-Generate-a-Private-Key-for-CA-Certificate" class="headerlink" title="2.1 Generate a Private Key for CA Certificate"></a>2.1 Generate a Private Key for CA Certificate</h4><p>Remember in last blog we mention that each certificate contains organization information and public key, which is paired with a private key file. Let’s generate a private key file for our CA first.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl genrsa -des3 -out cacert.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus (2 primes)</span><br><span class="line">...............+++++</span><br><span class="line">........................+++++</span><br><span class="line">e is 65537 (0x010001)</span><br><span class="line">Enter pass phrase <span class="keyword">for</span> cacert.key:</span><br><span class="line">Verifying - Enter pass phrase <span class="keyword">for</span> cacert.key:</span><br></pre></td></tr></table></figure><p>Your will be prompted with pass phrase, which is recommended to provide as it will prevent someone else from generating more root CA certificate from this key.</p><h4 id="2-2-Generate-CA-Certificate-Using-the-Private-key"><a href="#2-2-Generate-CA-Certificate-Using-the-Private-key" class="headerlink" title="2.2 Generate CA Certificate Using the Private key"></a>2.2 Generate CA Certificate Using the Private key</h4><p>Now, let’s generate the CA Certificate with the private key</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl req -x509 -new -nodes -key cacert.key -sha256 -days 3650 -out cacert.pem</span><br><span class="line">Enter pass phrase <span class="keyword">for</span> cacert.key:</span><br><span class="line">You are about to be asked to enter information that will be incorporated</span><br><span class="line">into your certificate request.</span><br><span class="line">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class="line">There are quite a few fields but you can leave some blank</span><br><span class="line">For some fields there will be a default value,</span><br><span class="line">If you enter <span class="string">'.'</span>, the field will be left blank.</span><br><span class="line">-----</span><br><span class="line">Country Name (2 letter code) [AU]:CA</span><br><span class="line">State or Province Name (full name) [Some-State]:BC</span><br><span class="line">Locality Name (eg, city) []:Vancouver</span><br><span class="line">Organization Name (eg, company) [Internet Widgits Pty Ltd]:HighGo</span><br><span class="line">Organizational Unit Name (eg, section) []:Software</span><br><span class="line">Common Name (e.g. server FQDN or YOUR name) []:va.highgo.com</span><br><span class="line">Email Address []:cary.huang@highgo.ca</span><br></pre></td></tr></table></figure><p>Please note that OpenSSL will prompt you to enter several pieces of organizational information that identifies the CA certificate. You should enter these information suited to your organization. The most important field is <code>Common Name</code>, which is commonly checked against the hostname or domain name of the service. Depending on the security policy, some server will enforce the rule that common name must equal its host / domain name; some servers do not have this restriction. </p><h4 id="2-3-Generate-a-private-key-for-CA-Signed-certificate"><a href="#2-3-Generate-a-private-key-for-CA-Signed-certificate" class="headerlink" title="2.3 Generate a private key for CA-Signed certificate"></a>2.3 Generate a private key for CA-Signed certificate</h4><p>Like in the CA case, CA-signed certificate is also paired with a private key file</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl genrsa -out server.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus (2 primes)</span><br><span class="line">...................+++++</span><br><span class="line">............................................................................+++++</span><br><span class="line">e is 65537 (0x010001)</span><br></pre></td></tr></table></figure><h4 id="2-4-Generate-a-Certificate-Signing-Request-for-CA-Signed-certificate"><a href="#2-4-Generate-a-Certificate-Signing-Request-for-CA-Signed-certificate" class="headerlink" title="2.4 Generate a Certificate Signing Request for CA-Signed certificate"></a>2.4 Generate a Certificate Signing Request for CA-Signed certificate</h4><p>Then we create a Certificate Signing Request (CSR), which contains a list of organizational information to be presented to the CA server for verification. The CA server then decide if the CSR should be granted a new certificate according to the security policy configured. Since we are using OpenSSL for certificate generation, the <code>CA server</code> here refers to OpenSSL itself, and the security policy configuration is located in <code>openssl.cnf</code>, which is commonly located in <code>/usr/local/ssl/openssl.cnf</code>. In an enterprise environment where Public Key Infrastructure (PKI) is deployed, the CA Server could refer to an actual service whose sole purpose is to verify incoming CSRs and renew or issue new certificates to requesting clients.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl req -new -key server.key -out server.csr</span><br><span class="line">You are about to be asked to enter information that will be incorporated</span><br><span class="line">into your certificate request.</span><br><span class="line">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class="line">There are quite a few fields but you can leave some blank</span><br><span class="line">For some fields there will be a default value,</span><br><span class="line">If you enter <span class="string">'.'</span>, the field will be left blank.</span><br><span class="line">-----</span><br><span class="line">Country Name (2 letter code) [AU]:CA</span><br><span class="line">State or Province Name (full name) [Some-State]:BC</span><br><span class="line">Locality Name (eg, city) []:Vancouver</span><br><span class="line">Organization Name (eg, company) [Internet Widgits Pty Ltd]:HighGo</span><br><span class="line">Organizational Unit Name (eg, section) []:Software</span><br><span class="line">Common Name (e.g. server FQDN or YOUR name) []:va.highgo.ca</span><br><span class="line">Email Address []:cary.huang@highgo.ca</span><br><span class="line"></span><br><span class="line">Please enter the following <span class="string">'extra'</span> attributes</span><br><span class="line">to be sent with your certificate request</span><br><span class="line">A challenge password []:</span><br><span class="line">An optional company name []:HighGo Canada</span><br></pre></td></tr></table></figure><h4 id="2-5-Generate-a-CA-Signed-certificate"><a href="#2-5-Generate-a-CA-Signed-certificate" class="headerlink" title="2.5 Generate a CA-Signed certificate"></a>2.5 Generate a CA-Signed certificate</h4><p>Since we are generating CA-signed certificate with OpenSSL locally, we can configure how the certificate should be generated using <code>openssl.cnf</code> file. We will just be using the default policy set in <code>openssl.cnf</code>. Here’s a snapshot of the default settings:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ usr_cert ]</span><br><span class="line"></span><br><span class="line"># These extensions are added when &#39;ca&#39; signs a request.</span><br><span class="line"></span><br><span class="line"># This goes against PKIX guidelines but some CAs do it and some software</span><br><span class="line"># requires this to avoid interpreting an end user certificate as a CA.</span><br><span class="line"></span><br><span class="line">basicConstraints&#x3D;CA:FALSE</span><br><span class="line"></span><br><span class="line"># Here are some examples of the usage of nsCertType. If it is omitted</span><br><span class="line"># the certificate can be used for anything *except* object signing.</span><br><span class="line"></span><br><span class="line"># This is OK for an SSL server.</span><br><span class="line"># nsCertType&#x3D; server</span><br><span class="line"></span><br><span class="line"># For an object signing certificate this would be used.</span><br><span class="line"># nsCertType &#x3D; objsign</span><br><span class="line"></span><br><span class="line"># For normal client use this is typical</span><br><span class="line"># nsCertType &#x3D; client, email</span><br><span class="line"></span><br><span class="line"># and for everything including object signing:</span><br><span class="line"># nsCertType &#x3D; client, email, objsign</span><br><span class="line"></span><br><span class="line"># This is typical in keyUsage for a client certificate.</span><br><span class="line"># keyUsage &#x3D; nonRepudiation, digitalSignature, keyEncipherment</span><br></pre></td></tr></table></figure><p>Let’s generate the CA-signed certificate. Note that the command will take <code>cacert.pem</code>, <code>cacert.key</code> and <code>server.csr</code> as inputs, in which we have already generated from previous steps. <code>server.pem</code> will be the output.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl x509 -req -<span class="keyword">in</span> server.csr -CA cacert.pem -CAkey cacert.key -CAcreateserial -out server.pem -days 3650 -sha256</span><br><span class="line">Signature ok</span><br><span class="line">subject=C = CA, ST = BC, L = Vancouver, O = HighGo, OU = Software, CN = va.highgo.ca, emailAddress = cary.huang@highgo.ca</span><br><span class="line">Getting CA Private Key</span><br><span class="line">Enter pass phrase <span class="keyword">for</span> cacert.key:</span><br></pre></td></tr></table></figure><p>We can repeat from step 2.3 to 2.5 to generate a new pair for the client application.</p><p>To conclude, we have the following files generated:</p><ul><li>cacert.pem - Root CA certificate that is at the top of the chain of trust. We use it to sign and create other certificates</li><li>cacert.key - key for the Root CA Certificate - must keep it secured.</li><li>server.pem - CA-signed certificate for server application</li><li>server.key - key for the server certificate</li><li>client.pem - CA-signed certificate for client application</li><li>client.key - key for the client certificate</li></ul><h3 id="3-Enabling-Transport-Layer-Security-TLS-to-PostgreSQL-Server"><a href="#3-Enabling-Transport-Layer-Security-TLS-to-PostgreSQL-Server" class="headerlink" title="3. Enabling Transport Layer Security (TLS) to PostgreSQL Server"></a>3. Enabling Transport Layer Security (TLS) to PostgreSQL Server</h3><p>PostgreSQL has native support for TLS to secure connection between client and server. The TLS support has to be enabled during build time and requires OpenSSL libraries. Depending on the versions of OpenSSL that the client or server is built with, TLS versions and ciphersuites may differ as well. This does not mean that both client and server must be linked with the same version of OpenSSL. It is possible that a client with older OpenSSL can connect to a server with newer OpenSSL if the server is configured to accept it. The TLS handshake process is initiated when a client first connects to the server in which they will evaluate TLS version used and negotiate ciphersuite that both ends are able to support. In this case, the server may use less secured ciphersuite and TLS version to communicate with the client, which may not be ideal.</p><p>The TLS support for a PostgreSQL server can be enabled in postgresql.conf.</p><figure class="highlight bash"><figcaption><span>postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">ssl = on</span><br><span class="line">ssl_ca_file = <span class="string">'~/cert/cacert.pem'</span></span><br><span class="line">ssl_cert_file = <span class="string">'~/cert/server.pem'</span></span><br><span class="line">ssl_crl_file = <span class="string">''</span></span><br><span class="line">ssl_key_file = <span class="string">'~/cert/server.key'</span></span><br><span class="line">ssl_ciphers = <span class="string">'HIGH:MEDIUM:+3DES:!aNULL'</span> <span class="comment"># allowed SSL ciphers</span></span><br><span class="line">ssl_prefer_server_ciphers = on</span><br><span class="line">ssl_ecdh_curve = <span class="string">'prime256v1'</span></span><br><span class="line">ssl_min_protocol_version = <span class="string">'TLSv1.2'</span></span><br><span class="line">ssl_max_protocol_version = <span class="string">''</span></span><br><span class="line">ssl_dh_params_file = <span class="string">''</span></span><br><span class="line">ssl_passphrase_command = <span class="string">''</span></span><br><span class="line">ssl_passphrase_command_supports_reload = off</span><br></pre></td></tr></table></figure><p>Let’s examine the configuration parameters.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssl = on</span><br></pre></td></tr></table></figure><p>This line turns on the TLS support. Please note that even if TLS is turned on, the server will still be able to accept connections that do not use TLS. Normally, the client is the entity that decides if TLS should be used or not. The server can also enforce the incoming connections to use TLS by modifying the <code>pg_hba.conf</code> file like this, where the connections from 172.16.30.0/24 must be TLS, otherwise the server will deny.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hostssl sales_team       all             172.16.30.0/24          trust</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssl_ca_file = <span class="string">'~/cert/cacert.pem'</span></span><br><span class="line">ssl_cert_file = <span class="string">'~/cert/server.pem'</span></span><br><span class="line">ssl_crl_file = <span class="string">''</span></span><br><span class="line">ssl_key_file = <span class="string">'~/cert/server.key'</span></span><br></pre></td></tr></table></figure><p>These 4 lines tell PostgreSQL where to load the X509 certificate, the CA certificate, server private key and the certificate revocation list. These certificates must be pre-generated by OpenSSL command or purchased from a trusted organization. For TLS to work, <code>ssl_ca_file</code>, <code>ssl_cert_file</code> and <code>ssl_key_file</code> must be provided. We will use the certificates we have generated for server in the previous section.</p><p>The file pointed by <code>ssl_ca_file</code> will be used to determined if the certificate can be trusted by deriving the chain of trust.<br>The file pointed by <code>ssl_cert_file</code> will be sent to the connecting client during TLS handshake for authentication purposes.<br>The file pointed by <code>ssl_key_file</code> will be used for asymmetrical encryption during authentication </p><p>The file pointed by <code>ssl_crl_file</code> is optional and it contains a list of certificates that cannot be trusted (or revoked). Distributing revoked certificates using this file is not the most ideal but still being practice today. It may have performance impact if the list is very large and it introduces a problem of when the list should be renewed and how often. Online Certificate Status Protocol (OCSP. ref:<a href="https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol">https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol</a>) is a newer protocol designed for Public Key Infrastructure (PKI) for querying certificate revocation status that addresses some of the issues with revocation file. Feel free to give a read on OCSP in the link above.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssl_ciphers = <span class="string">'HIGH:MEDIUM:+3DES:!aNULL'</span> <span class="comment"># allowed SSL ciphers</span></span><br><span class="line">ssl_prefer_server_ciphers = on</span><br><span class="line">ssl_ecdh_curve = <span class="string">'prime256v1'</span></span><br></pre></td></tr></table></figure><p>During TLS handshake, both client and server will present to each other a list of desired ciphersuites ordered by preference. Handshake process will go through both lists and find a common ciphersuite supported by both sides or abort if there is nothing in common. The <code>ssl_ciphers</code> configuration is used to configure the size of the ciphersuite lists to be presented to the client during handshake.</p><p><code>ssl_ciphers</code> is a string list consisting of one or more <code>cipher strings</code> separated by colons ( ref: <a href="https://www.openssl.org/docs/man1.1.1/man1/ciphers.html">https://www.openssl.org/docs/man1.1.1/man1/ciphers.html</a>) and defaults to <code>HIGH:MEDIUM:+3DES:!aNULL</code> which translates to:</p><ul><li>allows high strength ciphersuites (HIGH)</li><li>allows medium strength ciphersuites (MEDIUM)</li><li>move any ciphersuite using 3DES algorithm to the end of the list (+3DES)</li><li>remove any ciphersuite that does not have authentication algorithm (!aNULL)</li></ul><p>For example, “HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256” will use high strength ciphersuites while removing any ciphersuites containing ADH, MD5, RC4…etc.</p><p>Before applying the cipher string to PostgreSQL, it is recommended to check the output cipher list after tuning the cipher string using Openssl client tool.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ openssl ciphers -v <span class="string">'HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256'</span></span><br><span class="line">TLS_AES_256_GCM_SHA384  TLSv1.3 Kx=any      Au=any  Enc=AESGCM(256) Mac=AEAD</span><br><span class="line">TLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any      Au=any  Enc=CHACHA20/POLY1305(256) Mac=AEAD</span><br><span class="line">TLS_AES_128_GCM_SHA256  TLSv1.3 Kx=any      Au=any  Enc=AESGCM(128) Mac=AEAD</span><br><span class="line">AES256-GCM-SHA384       TLSv1.2 Kx=RSA      Au=RSA  Enc=AESGCM(256) Mac=AEAD</span><br><span class="line">AES256-CCM8             TLSv1.2 Kx=RSA      Au=RSA  Enc=AESCCM8(256) Mac=AEAD</span><br><span class="line">AES256-CCM              TLSv1.2 Kx=RSA      Au=RSA  Enc=AESCCM(256) Mac=AEAD</span><br><span class="line">ARIA256-GCM-SHA384      TLSv1.2 Kx=RSA      Au=RSA  Enc=ARIAGCM(256) Mac=AEAD</span><br><span class="line">AES128-GCM-SHA256       TLSv1.2 Kx=RSA      Au=RSA  Enc=AESGCM(128) Mac=AEAD</span><br><span class="line">AES128-CCM8             TLSv1.2 Kx=RSA      Au=RSA  Enc=AESCCM8(128) Mac=AEAD</span><br><span class="line">AES128-CCM              TLSv1.2 Kx=RSA      Au=RSA  Enc=AESCCM(128) Mac=AEAD</span><br><span class="line">ARIA128-GCM-SHA256      TLSv1.2 Kx=RSA      Au=RSA  Enc=ARIAGCM(128) Mac=AEAD</span><br><span class="line">AES256-SHA256           TLSv1.2 Kx=RSA      Au=RSA  Enc=AES(256)  Mac=SHA256</span><br><span class="line">AES128-SHA256           TLSv1.2 Kx=RSA      Au=RSA  Enc=AES(128)  Mac=SHA256</span><br><span class="line">CAMELLIA128-SHA256      TLSv1.2 Kx=RSA      Au=RSA  Enc=Camellia(128) Mac=SHA256</span><br><span class="line">AES256-SHA              SSLv3 Kx=RSA      Au=RSA  Enc=AES(256)  Mac=SHA1</span><br><span class="line">AES128-SHA              SSLv3 Kx=RSA      Au=RSA  Enc=AES(128)  Mac=SHA1</span><br><span class="line">CAMELLIA128-SHA         SSLv3 Kx=RSA      Au=RSA  Enc=Camellia(128) Mac=SHA1</span><br></pre></td></tr></table></figure><p><code>ssl_prefer_server_ciphers</code> specifies whether to use the server’s SSL cipher preferences, rather than the client’s. It should always be <code>on</code> for more control in terms of ciphersuite selection.</p><p><code>ssl_ecdh_curve</code> specifies the name of the curve to use in ECDH key exchange algorithms and is useful only if the ciphersuite uses ECDHE key exchange algorithm. The most common curves are : prime256v1, secp384r1 and secp521r1 and normally leaving it default should suffice.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssl_min_protocol_version = <span class="string">'TLSv1.2'</span></span><br><span class="line">ssl_max_protocol_version = <span class="string">''</span></span><br></pre></td></tr></table></figure><p>These 2 lines configure the minimum and maximum TLS versions to accept. By default the server will only serve the TLS client using TLSv1.2 and above. TLSv1.2 is a very secured TLS version and it is widely used in the world. Normally, we only change the minimum TLS version with assumption that all future versions will be more secured and for this reason, we normally don’t put restriction on the max version.</p><p>TLSv1.3 is recently introduced that has new ciphersuite support and has more improvement in the handshake process. To enforce TLSv1.3 to be used, set the <code>ssl_min_protocol_version</code> to ‘TLSv1.3’ will suffice.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssl_dh_params_file = <span class="string">''</span></span><br><span class="line">ssl_passphrase_command = <span class="string">''</span></span><br><span class="line">ssl_passphrase_command_supports_reload = off</span><br></pre></td></tr></table></figure><p><code>ssl_dh_params_file</code> points to a file that contains custom diffie-hellman key exchange algorithm parameter. This is an optional parameter and is only useful if the ciphersuite uses DHE key exchange algorithm. If left empty, compiled-in defaults will be used. Custom DH parameters can be generated using command <code>openssl dhparam -out dhparams.pem 2048</code> and will normally reduce the attack exposure as attacker will have hard time cracking the key exchange process using custom parameter instead of the well-known default. </p><p><code>ssl_passphrase_command</code> is the command to obtain the password for the private key file specified by <code>ssl_key_file</code>. There is an option to add a password to a private key file during its generation and if password is used, <code>ssl_passphrase_command</code> must be set with the system command that will retrieve such password. Otherwise, TLS handshake will abort as PostgreSQL will not be able to access private key without password.</p><p><code>ssl_passphrase_command_supports_reload</code> configures if the <code>ssl_passphrase_command</code> should be re-run at every reload (ie. SIGHUP). It is default to <code>off</code>, so the <code>ssl_passphrase_command</code> will not be run at every reload.</p><h3 id="4-Enabling-Transport-Layer-Security-TLS-to-PostgreSQL-Client"><a href="#4-Enabling-Transport-Layer-Security-TLS-to-PostgreSQL-Client" class="headerlink" title="4. Enabling Transport Layer Security (TLS) to PostgreSQL Client"></a>4. Enabling Transport Layer Security (TLS) to PostgreSQL Client</h3><p>Now that we have a PostgreSQL server with TLS setup, we can use psql client to connect to the server also using TLS. Depending on the client connect parameters given, we can utilize TLS in different security levels. I will show the most common usages here:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Case 1: connect to server in TLS mode</span></span><br><span class="line">$ psql -U user -h localhost -d <span class="string">"sslmode=require dbname=postgres"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 2: connect to server in TLS mode if server supports it</span></span><br><span class="line">$ psql -U user -h localhost -d <span class="string">"sslmode=prefer dbname=postgres"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 3: connect to server in TLS mode and verify server CA against client CA</span></span><br><span class="line">$ psql -U user -h localhost -d <span class="string">"sslmode=verify-ca dbname=postgres sslrootcert=~/cert/cacert.pem"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Case 4: connect to server in TLS mode and present client certificate. Verify all certificate details and trust chain. Check certificate revocation list does not contain server cert.</span></span><br><span class="line">$ psql -U user -h localhost -d <span class="string">"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key"</span></span><br></pre></td></tr></table></figure><p>The usage in Case 4 is the most secured because both server and client will verify each other’s certificate and decide if both can be mutually trusted. The common name field in the certificate is checked against the server hostname; certificate validity period is checked, organization details are checked; certificate trust chain is checked; revocation list is checked.</p><p>Please note that PostgreSQL server with TLS enabled by default does not force the client to present a TLS certificate for verification. If client presents one like in Case 4 above, the server will verify and deny connection is certificate is bad. If client does not provide a certificate like in Case 1 ~ 3, the server will skip the client certificate verification as there is nothing to verify, which is less secure.</p><p>To enforce the connecting client to present a TLS certificate for verification, we will need to add a special <code>clientcert=1</code> argument in existing authentication rules defined in <code>pg_hba.conf</code>. </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TYPE  DATABASE         USER                 ADDRESS                 METHOD</span></span><br><span class="line">hostssl production    production_user      0.0.0.0/0            trust clientcert=1</span><br><span class="line">hostssl marketing     marketing_user       0.0.0.0/0            pam   clientcert=1</span><br><span class="line">hostssl sales         sales_user           0.0.0.0/0            md5   clientcert=1</span><br><span class="line">hostssl software      software_user        0.0.0.0/0            password clientcert=1</span><br><span class="line">hostssl hardware      hardware_user        0.0.0.0/0            scram-sha-256 clientcert=1</span><br><span class="line">hostssl hardware      hardware_user        0.0.0.0/0            cert  clientcert=1</span><br></pre></td></tr></table></figure><p>The example above will enforce connecting client to present TLS certificate to access <code>production_team</code> database as <code>production_user</code>. If a TLS certificate is not provided by client, the connection will abort.</p><h3 id="5-TLS-Connect-Examples"><a href="#5-TLS-Connect-Examples" class="headerlink" title="5. TLS Connect Examples"></a>5. TLS Connect Examples</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -U user -h localhost -d <span class="string">"sslmode=require dbname=postgres"</span></span><br><span class="line">psql (13devel)</span><br><span class="line">SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)</span><br><span class="line">Type <span class="string">"help"</span> <span class="keyword">for</span> <span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">postgres=<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>Please note that psql prints the TLS version used (TLSv1.2) and the cipher suite negotiated during handshake (ECDHE-RSA-AES256-GCM-SHA384). Below is the wireshark capture of the above TLS connection:</p><p><img src="/images/tls-case1.png" alt="tls handshake"></p><p><img src="/images/tls-case1-cipher.png" alt="tls handshake"></p><p>Another Example:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -U cary -h localhost -d <span class="string">"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key"</span></span><br><span class="line"></span><br><span class="line">psql: error: could not connect to server: server certificate <span class="keyword">for</span> <span class="string">"va.highgo.ca"</span> does not match host name <span class="string">"localhost"</span></span><br></pre></td></tr></table></figure><p>Here, we have an error when we set sslmode to <code>verify-full</code>, where both server and client will verify each other with the most strict criteria. This error happens because the <code>Common Name</code> field in the certificate does not match the host name. Did I mention that <code>Common Name</code> is the most important field of a certificate? To resolve this error, we can either re-generate certificate with matching Common name, or change the host name.</p><p>I simply add an entry to <code>/etc/hosts</code> to resolve the error</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1       localhost</span><br><span class="line">127.0.0.1       va.highgo.ca</span><br></pre></td></tr></table></figure><p>and the error will disappear when both Common Name and Hostname match</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -U cary -h va.highgo.ca -d <span class="string">"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key"</span></span><br><span class="line"></span><br><span class="line">psql (13devel)</span><br><span class="line">SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)</span><br><span class="line">Type <span class="string">"help"</span> <span class="keyword">for</span> <span class="built_in">help</span>.</span><br></pre></td></tr></table></figure><p>Please note that this command also forces the client to submit a certificate to server as well as seen from the wireshark capture. We can tell by looking at the <code>length</code> field of the packet capture. There are 2 exchanges having lengths = 2675 and 2446. Those are the actual certificate contents being transmitted. Previous capture only has 1 exchanges having packet length = 2675; it means only server is providing certificate to client for verification.</p><p><img src="/images/tls-case4.png" alt="tls handshake"></p><h3 id="6-Transparent-Data-Encryption-TDE"><a href="#6-Transparent-Data-Encryption-TDE" class="headerlink" title="6. Transparent Data Encryption (TDE)"></a>6. Transparent Data Encryption (TDE)</h3><p>Transparent Data Encryption refers to the process of protecting data at rest by encrypting database files on the hard disk level and decrypting them while reading from hard disk. This is to prevent physical storage media theft. This is called <code>transparent</code> because the encryption and decryption happen between PostgreSQL server and the physical hard disk and it is not visible to the client applications. TDE uses symmetrical encryption for securing blocks of database files such as shared buffer and WAL files, and it is designed to accompany with a internal Key Management System (KMS) to manage the lifecycle of the encryption keys. </p><p>TDE and KMS are still under development by the PostgreSQL community. The KMS feature is expected to be released in PG13 while the TDE feature to be in PG14. With its completion, it will add another layer of security feature on top of already security-rich PostgreSQL database.</p><h3 id="7-Security-Vulnerability"><a href="#7-Security-Vulnerability" class="headerlink" title="7. Security Vulnerability"></a>7. Security Vulnerability</h3><p>Security Vulnerability is a weakness which can be exploited by an attacker to perform unauthorized actions, sabotage a service, or inject malicious software or virus. These weaknesses are generally implementation mistakes, undiscovered bugs or a legacy problem that require an update to the server to resolve.</p><p>PostgreSQL also has a list of known security vulnerability that has been discovered and fixed by the community. The list can be found here: <a href="https://www.postgresql.org/support/security/">https://www.postgresql.org/support/security/</a>. These vulnerability ranges from different severity levels, from simple memory leak to crash the server. </p><p>This is why doing regular PostgreSQL server upgrade is important because each minor release fixes some of the discovered security vulnerabilities and therefore reducing the attack surface on your server. </p><h3 id="8-Summary"><a href="#8-Summary" class="headerlink" title="8. Summary"></a>8. Summary</h3><p>In part 3 of the blog, we have learned and understood what each TLS related configuration means in <code>postgresql.conf</code> and how to initiate TLS connection with psql client. We learned that keeping PostgreSQL server up-to-date can reduce the attack surface on some of the discovered vulnerabilities. We can ensure a fairly secured database network environment with TLS having adequate understanding of its fundamentals and practices. With the TDE feature coming in near future, we can further secure the database environment in the disk level and prevent possible data loss due to disk theft.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;This is part 3 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing how to apply TLS in both PostgreSQL server and client using the principles we have learned in part 2 of the blog. In the end, I will also briefly talk about Transparent Data Encryption (TDE) and security vulnerability.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="networking" scheme="http://caryhuang.github.io/tags/networking/"/>
    
    <category term="security" scheme="http://caryhuang.github.io/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Security Features in PostgreSQL - Part 2</title>
    <link href="http://caryhuang.github.io/2020/01/13/Understanding-Security-Features-in-PostgreSQL-Part2/"/>
    <id>http://caryhuang.github.io/2020/01/13/Understanding-Security-Features-in-PostgreSQL-Part2/</id>
    <published>2020-01-13T23:58:47.000Z</published>
    <updated>2020-06-16T20:07:10.578Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>This is part 2 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing TLS in greater details. I will begin by going over some of the most important security concepts around TLS before jumping into enabling TLS on PostgreSQL server. I believe it is crucial to have sufficient background information on TLS before tweaking the TLS settings in both client and server sides.</p><p>In part 1 of this blog, we mostly discussed about authentication and authorization (AA), which is important to identify which client is permitted to connect and which table or column he/she is permitted to operate. Even with the strongest authentication and authorization, the actual communication between client and server will not be encrypted unless Transport Layer Security (TLS) is specifically enabled in the database server. TLS is one of the least understood but commonly used security protocol that ensures the security of many HTTPS sites and other services. TLS is a big protocol and this blog will describe how it works and how to enable TLS in your PostgreSQL server.</p><a id="more"></a><p>Here is the overview of the <code>security</code> topics that will be covered in all parts of the blog:</p><p>Part 1:   </p><ul><li>PostgreSQL Server Listen Address</li><li>Host-Based Authentication</li><li>Authentication with LDAP Server</li><li>Authentication with PAM</li><li>Role-Based Access Control </li><li>Assign Table and Column Level Privileges to Users</li><li>Assign User Level Privileges as Roles</li><li>Assign and Column Level Privileges via Roles</li><li>Role Inheritance</li></ul><p>Part 2:</p><ul><li>Security Concepts around TLS</li><li>Symmetrical Encryption</li><li>Asymmetrical Encryption (a.k.a Public Key Cryptography)</li><li>Block Cipher Mode of Operation (a.k.a Stream Cipher)</li><li>Key Exchange Algorithm</li><li>TLS Certificate and Chain of Trust</li><li>Data Integrity Check / Data Authentication</li><li>TLS Cipher Suite and TLS handshake</li><li>TLS versions</li></ul><p>Part 3: </p><ul><li>Preparing TLS Certificates</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Server</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Client</li><li>TLS Connect Examples</li><li>Transparent Data Encryption (TDE)</li><li>Security Vulnerability</li></ul><h3 id="2-Security-Concepts-around-TLS"><a href="#2-Security-Concepts-around-TLS" class="headerlink" title="2. Security Concepts around TLS"></a>2. Security Concepts around TLS</h3><p>Before we jump into configuring TLS in PostgreSQL. It is super important to have some background information on the following security topics build around TLS.</p><h4 id="2-1-Symmetrical-Encryption"><a href="#2-1-Symmetrical-Encryption" class="headerlink" title="2.1 Symmetrical Encryption"></a>2.1 Symmetrical Encryption</h4><p>Symmetrical Encryption is a type of encryption where only one secret key is used to encrypt and decrypt a message. In other words, the connecting client will use the secret key to encrypt the message and send to server, the server uses the same key to decrypt the ciphered message and obtain the original message. This is a very fast encryption operation and may sound simple, but the challenge here is how to securely share this one and only secret key between the client and server, how long should the secret key be used before next rotation? Should the secret key be pre-configured on both client and server sides? Should third-party key management software be integrated? These are some of the common challenges with symmetrical encryption.</p><p><img src="/images/symetrical.png" alt="symmetrical encryption"></p><p>The following is some of the most common symmetrical encryption algorithms today with the AES being the most popular: (reference: <a href="https://en.wikipedia.org/wiki/Symmetric-key_algorithm">https://en.wikipedia.org/wiki/Symmetric-key_algorithm</a>). Each algorithm supports key lengths having multiple sizes and normally is denoted after the encryption algorithm name, for example, AES-128, AES-256…etc.</p><ul><li>AES (Advanced Encryption Standard)</li><li>DES (Data Encryption Standard)</li><li>Triple DES</li><li>Blowfish </li></ul><p>Symmetrical encryption is normally paired with a <code>Block Cipher Mode of Operation</code> to encrypt or decrypt a stream of data. Imagine there is a data stream of size 30GB that needs to be encrypted. Without Block Cipher Mode, we will have to load all 30GB of data into memory and encrypt it with (say AES128) and most likely we do not large enough memory to load all the data stream. This is where Block Cipher Mode of Operations come in handy, it encrypts the data stream block by block (most likely 16 byte block) until the entire block is encrypted. We basically can encrypt the 30GB data stream without having to have at least 30GB of memory. </p><h4 id="2-2-Asymmetrical-Encryption-a-k-a-Public-Key-Cryptography"><a href="#2-2-Asymmetrical-Encryption-a-k-a-Public-Key-Cryptography" class="headerlink" title="2.2 Asymmetrical Encryption (a.k.a Public Key Cryptography)"></a>2.2 Asymmetrical Encryption (a.k.a Public Key Cryptography)</h4><p>Unlike symmetrical encryption, asymmetrical encryption uses two distinct keys called public and private keys; Public key is used for encryption and private key is used for decryption. Both keys are different but related by math and it is much slower than symmetrical encryptions. As name implies, public key can be distributed publicly while private key is to be kept private as it is the only key that is able to decrypt the messages encrypted by public key. This essentially forms a secured one-way communication. </p><p>Generally, asymmetrical encryption is not desirable to be used as stream data encryption algorithm though it is more secured; it requires more computational power to perform encryption and decryption and this is a major drawback. Asymmetrical encryption is commonly used as authentication protocol for a client to verify that server is indeed valid. During a TLS handshake for example, server will present its TLS certificate, which contains a public key, to the client, client uses the public key to encrypt a message and asks the server to decrypt with its private key and send back the result. If message match, then client is sure that the server possess the private key and therefore is valid.</p><p><img src="/images/asymmetrical.png" alt="asymmetrical encryption"></p><p>The following is some of the most common asymmetrical encryption algorithms today with the RSA and Elliptic curve being the most popular: (reference: <a href="https://en.wikipedia.org/wiki/Public-key_cryptography">https://en.wikipedia.org/wiki/Public-key_cryptography</a>).</p><ul><li>RSA</li><li>DSS</li><li>Elliptic curve</li></ul><h4 id="2-3-Block-Cipher-Mode-of-Operation-a-k-a-Stream-Cipher"><a href="#2-3-Block-Cipher-Mode-of-Operation-a-k-a-Stream-Cipher" class="headerlink" title="2.3 Block Cipher Mode of Operation (a.k.a Stream Cipher)"></a>2.3 Block Cipher Mode of Operation (a.k.a Stream Cipher)</h4><p>Block Cipher Mode of Operation is normally used with Symmetrical encryption to encrypt or decrypt a stream of data block by block. There are several available modes of block cipher operations that have different strengths and weaknesses. Most modes require a complete block of 16 bytes to be able to encrypt. In the case where the input stream is not in multiple of 16, padding is normally use to fill the block. </p><p>The following is some of the most common block cipher mode of operations today with the CBC and CTR being the most popular and ECB being the least secured: (reference: <a href="https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation">https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation</a>).</p><ul><li>Cipher Block Chaining (CBC)</li><li>Counter (CTR)</li><li>Cipher Feedback (CFB)</li><li>Output Feedback (OFB)</li><li>Electronic Codebook (ECB)</li></ul><p>The mode is normally denoted with the desired symmetrical encryption algorithm, for example, AES-128-CBC or AES-256-CTR are quite common.</p><h4 id="2-4-Key-Exchange-Algorithm"><a href="#2-4-Key-Exchange-Algorithm" class="headerlink" title="2.4 Key Exchange Algorithm"></a>2.4 Key Exchange Algorithm</h4><p>Key exchange algorithm is a math algorithm designed to make both client and server agree on a secret key without actually sending the key to each other. This is done by pure math equations and require several steps of intermediate token exchange. In the end both client and server will end up with the same value, which is to be used as the secret key for <code>symmetrical encryption algorithms</code>. Key exchange algorithm is common used in many services such as SSH and TLS. Services like these normally have a <code>handshake</code> stage where both client and server have to agree on the subsequent algorithms to use for encryption and perform key exchange algorithm to get the secret key for symmetrical encryption. </p><p>The following is some of the most common key exchange algorithms with diffie-hellman and elliptic curve diffie-hellman being the most popular (reference: <a href="https://en.wikipedia.org/wiki/Key_exchange">https://en.wikipedia.org/wiki/Key_exchange</a>).</p><ul><li>Diffie-Hellman (DH)</li><li>Elliptic Curve Diffie-Hellman (ECDH)</li><li>Ephemeral Diffie-Hellman (DHE)</li><li>RSA </li></ul><h4 id="2-6-Data-Integrity-Check-Authentication"><a href="#2-6-Data-Integrity-Check-Authentication" class="headerlink" title="2.6 Data Integrity Check / Authentication"></a>2.6 Data Integrity Check / Authentication</h4><p>The data integrity authentication is not to be confused with host-based or role-based authentication mentioned in part 1. Data integrity authentication refers to the methods to ensure that the data stream has been received without being altered during transmission. Think of it as a data checksum. In addition to encryption, ensuring data integrity is also very important security measure to avoid man-in-the-middle attack. Please note that data integrity check and data encryption are 2 separate processes, meaning that you can have data authentication without encryption, or encryption without authentication. SNMPv3 is a good example that treats data authentication and encryption separately while TLS requires both at the same time.</p><p>The following is some of the most common hash algorithms with SHA1 and MD5 being the most common (reference: <a href="https://en.wikipedia.org/wiki/Message_authentication">https://en.wikipedia.org/wiki/Message_authentication</a>).</p><ul><li>SHA1</li><li>SHA2</li><li>MD5</li><li>BLAK2</li></ul><h4 id="2-5-TLS-Certificate-and-Chain-of-Trust"><a href="#2-5-TLS-Certificate-and-Chain-of-Trust" class="headerlink" title="2.5 TLS Certificate and Chain of Trust"></a>2.5 TLS Certificate and Chain of Trust</h4><p>TLS certificate and chain of trust are the core concepts in TLS to ensure maximum trust between a client and a server. The certificate used by PostgreSQL is X509 version 3 certificate, which has extension support to further refine the purpose of the certificate issued. </p><p>The certificates are created and signed in hierarchy. The certificate created at the top hierarchy is called a <code>root CA</code> (root Certificate Authority) and is normally created by a trusted organization. This <code>root CA</code> is able to sign additional <code>Intermediate CA</code> that can be distributed to other organizations. The intermediate CA can then be used to create and sign individual certificates to be used by services like HTTPS, FTPS…etc.</p><p>There are several types of TLS certificate and each has its own place in the certificate hierarchy and serve different purposes. A TLS certificate is a small data file that contains the public key, organization details, trustee’s digital signature, extensions and validity dates. Normally a TLS certificate is generated with a private key. The key pair bounded with the certificate is important as they are required for authentication when a TLS client wishes to connect to the server.</p><p>The following image illustrates the idea of certificate trust chain:</p><p><img src="/images/cert_chain.png" alt="certificate chain"></p><p>As you can see, the root CA is on top of hierarchy and is able to generate and sign additional <code>intermediate CA</code> and issue to several organizations. The organization then is able to take the intermediate CA and generate additional <code>CA-signed certificates</code> and matching <code>private keys</code> to use in their services such as PostgreSQL server, FTPS and HTTPS server.</p><p>A CA certificate can be purchased from a trusted organization or generated by oneself using openssl and java key tool. We will go over the procedure to generate these certificates using OpenSSL as examples in part 3 of this blog.</p><h4 id="2-7-TLS-versions"><a href="#2-7-TLS-versions" class="headerlink" title="2.7 TLS versions"></a>2.7 TLS versions</h4><p>TLS is a newer protocol that replaces its predecessor, Secured Sockets Layer (SSL). Below is a list of TLS versions that we should use as of today:</p><ul><li>TLSv1.0</li><li>TLSv1.1</li><li>TLSv1.2</li><li>TLSv1.3</li></ul><p>TLSv1.3 is the newest TLS version that has significant improvement in the handshake process and introduces many more cipher suites specifically designed for TLSv1.3. Before TLSv1.3, TLSv1.2 is the most popular TLS version deployed in the world today. PostgreSQL server defaults to accept client connection that supports minimum TLS version to be TLSv1.2 and will reject any connection in the versions earlier than v1.2</p><h4 id="2-8-TLS-Cipher-Suite-and-TLS-handshake"><a href="#2-8-TLS-Cipher-Suite-and-TLS-handshake" class="headerlink" title="2.8 TLS Cipher Suite and TLS handshake"></a>2.8 TLS Cipher Suite and TLS handshake</h4><p>TLS cipher suite refers to a set of algorithms that help secure a network connection. The suite of algorithms normally contains </p><ul><li>Key exchange algorithm</li><li>Authentication algorithm</li><li>Asymmetrical encryption algorithm</li><li>Message authentication algorithm</li></ul><p>for example, a TLSv1.2 cipher suite TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 indicates the following</p><ul><li>DHE - use Ephemeral Diffie-Hellman key exchange algorithm</li><li>RSA - use RSA asymmetrical keys for authentication</li><li>AES_256_CBC - use AES-256 symmetrical encryption with CBC block cipher mode</li><li>SHA256 - use SHA-256 as message authentication algorithm to make sure exchanged messages are not tempered with.</li></ul><p>When a TLS client initiates a TLS connection to a server, a <code>TLS handshake</code> process takes place that roughly performs the following:</p><ul><li>Agree on the TLS version to use. Abort if version cannot be agreed </li><li>Agree on the cipher suite to use. Abort if cipher suite cannot be agreed</li><li>Certificate exchange</li><li>Client authenticates the server using agreed algorithm</li><li>perform key exchange using agreed algorithm</li><li>ensure handshake message is not tempered with the agreed message authentication algorithm</li><li>secured communication then begins.</li></ul><h3 id="7-Summary"><a href="#7-Summary" class="headerlink" title="7. Summary"></a>7. Summary</h3><p>TLS is a big protocol involving a lot of steps including certificate exchange, chain of trust verification, key exchange, cipher suite exchange, authentication, data integrity check and finally symmetrical encryption of application data with appropriate block cipher mode. Having adequate fundamental understanding to TLS is crucial to ensure a correct and secured database environment setup. Of course there is more to what we have discussed here so far and I will be producing more articles in the near future to address some of the advanced TLS related practices.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;This is part 2 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing TLS in greater details. I will begin by going over some of the most important security concepts around TLS before jumping into enabling TLS on PostgreSQL server. I believe it is crucial to have sufficient background information on TLS before tweaking the TLS settings in both client and server sides.&lt;/p&gt;
&lt;p&gt;In part 1 of this blog, we mostly discussed about authentication and authorization (AA), which is important to identify which client is permitted to connect and which table or column he/she is permitted to operate. Even with the strongest authentication and authorization, the actual communication between client and server will not be encrypted unless Transport Layer Security (TLS) is specifically enabled in the database server. TLS is one of the least understood but commonly used security protocol that ensures the security of many HTTPS sites and other services. TLS is a big protocol and this blog will describe how it works and how to enable TLS in your PostgreSQL server.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="networking" scheme="http://caryhuang.github.io/tags/networking/"/>
    
    <category term="security" scheme="http://caryhuang.github.io/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Security Features in PostgreSQL - Part 1</title>
    <link href="http://caryhuang.github.io/2020/01/10/Understanding-Security-Features-in-PostgreSQL-Part1/"/>
    <id>http://caryhuang.github.io/2020/01/10/Understanding-Security-Features-in-PostgreSQL-Part1/</id>
    <published>2020-01-10T22:02:06.000Z</published>
    <updated>2020-06-16T20:07:10.584Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>PostgreSQL is packed with several security features for a database administrator to utilize according to his or her organizational security needs. The word <code>Security</code> is a very broad concept and could refer to completely different procedures and methodology to achieve in different PostgreSQL components. This blog is divided into part 1, 2 and 3 and I will explain the word <code>Security</code> with regards to PostgreSQL version 12.1 and how it is practiced in different areas within the system. </p><p>In Part 1 of the blog, I will be discussing the basic security features that exist in PostgreSQL with emphasis on Host-based authentication methods as well as user-based access control with the concept of roles. If done right, we could have a much more robust database server and potentially reduce the attack surface on the server, protecting it from attacks like SQL injections. I will also briefly discuss a few of the advanced authentication methods such as LDAP and PAM authentication. There are many more advanced authentication methods supported and we will be producing more articles in the near future to cover more of these methods.</p><p>In Part 2 of the blog, I will be discussing TLS in greater detail, which I believe is crucial for a database administrator to understand first before enabling TLS in the PostgreSQL server. TLS is a fairly large and one of the least understood protocol today, which contains a lot of security components and methodology related to cryptography that could be quite confusing. </p><p>In Part 3 of the blog, I will be discussing how to apply TLS configurations to both PostgreSQL server and client following the TLS principles that have been discussed in Part 2. I will also briefly discuss Transparent Data Encryption (TDE) that the PG community is currently working on that introduces another layer of secured database environment.</p><a id="more"></a><p>Below is the overview of the <code>security</code> topics that will be covered in all parts of the blog:</p><p>Part 1:   </p><ul><li>PostgreSQL Server Listen Address</li><li>Host-Based Authentication</li><li>Authentication with LDAP Server</li><li>Authentication with PAM</li><li>Role-Based Access Control </li><li>Assign Table and Column Level Privileges to Users</li><li>Assign User Level Privileges as Roles</li><li>Assign and Column Level Privileges via Roles</li><li>Role Inheritance</li></ul><p>Part 2:</p><ul><li>Security Concepts around TLS</li><li>Symmetrical Encryption</li><li>Asymmetrical Encryption (a.k.a Public Key Cryptography)</li><li>Block Cipher Mode of Operation (a.k.a Stream Cipher)</li><li>Key Exchange Algorithm</li><li>TLS Certificate and Chain of Trust</li><li>Data Integrity Check / Data Authentication</li><li>TLS Cipher Suite and TLS handshake</li><li>TLS versions</li></ul><p>Part 3: </p><ul><li>Preparing TLS Certificates</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Server</li><li>Enabling Transport Layer Security (TLS) to PostgreSQL Client</li><li>TLS Connect Examples</li><li>Transparent Data Encryption (TDE)</li><li>Security Vulnerability</li></ul><h3 id="2-PostgreSQL-Server-Listen-Address"><a href="#2-PostgreSQL-Server-Listen-Address" class="headerlink" title="2. PostgreSQL Server Listen Address"></a>2. PostgreSQL Server Listen Address</h3><p>PostgreSQL server is a TCP server that by default listens on localhost at port 5432. The server listen address may seem very trivial at first in terms of security but it is actually very important because understanding how the PostgreSQL is serving the incoming connections is fundamental to building a more secured network environment.</p><p>Connection settings are located in <code>postgresql.conf</code> </p><p>The <code>listen_addresses</code> parameter tells PostgreSQL which addresses to listen on. This value should match the IP address of the network interface cards in the host machine. <code>ifconfig</code> on Unix-based systems (or <code>ipconfig</code> for Windows) is a handy command that lists all the network interfaces and their IP addresses. <code>listen_address</code> supports the less secured <code>*</code> configuration, which will listen to all the network interfaces available</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#listen_addresses &#x3D; &#39;localhost&#39;# what IP address(es) to listen on;</span><br><span class="line">                # comma-separated list of addresses;</span><br><span class="line">                # defaults to &#39;localhost&#39;; use &#39;*&#39; for all</span><br><span class="line">                # (change requires restart)</span><br><span class="line">#port &#x3D; 5432                     # (change requires restart)</span><br></pre></td></tr></table></figure><p>Another important connection configuration is the maximum connections allowed. By default PostgreSQL allows 100 simultaneous connections to be active at a time with 3 connections reserved for super user. That is 97 connections for regular database users. These numbers should be configured accordingly depending on the usage case of the database server and we definitely don’t want too many unintentional connections to access the database</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">max_connections &#x3D; 100               # (change requires restart)</span><br><span class="line">superuser_reserved_connections &#x3D; 3  # (change requires restart)</span><br></pre></td></tr></table></figure><h3 id="3-Host-Based-Authentication"><a href="#3-Host-Based-Authentication" class="headerlink" title="3. Host-Based Authentication"></a>3. Host-Based Authentication</h3><p>Host-based authentication refers to the process of verifying the identity of a user connection based on the IP addresses of the connecting host. PostgreSQL supports host-based authentication by adding and removing desired entries in the <code>pg_hba.conf</code> file. This file works in a similar way as defining firewall rules. The official documentation on <code>pg_hba.conf</code> can be found here: <a href="https://www.postgresql.org/docs/current/auth-pg-hba-conf.html">https://www.postgresql.org/docs/current/auth-pg-hba-conf.html</a></p><p>A simple example below defines the following rules:</p><ul><li>Allows connections from subnet 192.168.3.0/24 to access the database named “software_team”</li><li>Allows connections from subnet 192.168.4.0/24 to access the database named “marketing_team”</li><li>Allows connections from subnet 192.168.5.0/24 to access the database named “sales_team”</li><li>Allows connections from subnet 192.168.6.0/24 to access the database named “management”</li><li>The admin user has permission to access all the database given that the admin is connecting from localhost (both IPv4 and IPv6) or from a UNIX domain socket.</li><li>Allows all user connections coming from subnet 192.168.7.0/24 to access the database named “production_team” and is able to do replication connection. Please note that the word “replication” is a special term reserved to allow replication connections rather than database name.</li><li>Allows user from a unsecured network “172.16.30.0/24” to access the “sales_team” database only if the connection uses SSL (a.k.a TLS)</li><li>Rejects all connections from 172.16.50.5</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># TYPE  DATABASE         USER            ADDRESS                 METHOD</span></span><br><span class="line"><span class="built_in">local</span>   all              admin                                   trust</span><br><span class="line"><span class="built_in">local</span>   all              admin           127.0.0.1/32            trust</span><br><span class="line"><span class="built_in">local</span>   all              admin           ::1/128                 trust</span><br><span class="line"></span><br><span class="line">host    software_team    all             192.168.3.0/24          trust</span><br><span class="line">host    marketing_team   all             192.168.4.0/24          trust</span><br><span class="line">host    sales_team       all             192.168.5.0/24          trust</span><br><span class="line">host    management       all             192.168.6.0/24          trust</span><br><span class="line">host    production_team  all             192.168.7.0/24          trust</span><br><span class="line">host    replication      all             192.168.7.0/24          trust</span><br><span class="line"></span><br><span class="line">host    all              all             172.16.50.5/32          reject</span><br><span class="line">hostssl sales_team       all             172.16.30.0/24          trust</span><br></pre></td></tr></table></figure><p>The simple example above uses 2 basic methods to control the access, <code>trust</code> and <code>reject</code>. This will suffice in a small database server environment. However, depending on the infrastructure, the application’s nature and data security, stronger authentication methods are encouraged, such as LDAP, GSSPI with Kerberos, SSPI, RADIUS SCRAM-SHA-256…etc. </p><p>Generally speaking, most of these stronger authentication methods require PostgreSQL to communicate with foreign authentication servers to complete the authentication process in a more secured way and provide automatic “single-sign-on” authentications through means of shared secrets, token exchange, or user name mappings. I will briefly introduces LDAP and PAM authentication in this blog.</p><h3 id="4-Authentication-with-LDAP-Server"><a href="#4-Authentication-with-LDAP-Server" class="headerlink" title="4. Authentication with LDAP Server"></a>4. Authentication with LDAP Server</h3><p>LDAP stands for Light-weight Directory Access Protocol, which is commonly deployed as centralized authentication system for medium to large organizations. This authentication server provides user credential authentication and stores related user details like distinguished name, domain names and business units..etc. Every entry in an LDAP directory server has a distinguished name (DN). It is the name that uniquely identifies an entry in the directory and made up of <code>attribute=value</code> pairs. As a LDAP client on the PostgreSQL side, <code>attribute=value</code> pairs are required to be supplied in <code>pg_hba.conf</code> file separated by commas. For example:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TYPE  DATABASE         USER                 ADDRESS             METHOD</span></span><br><span class="line">host production_team    production_user       0.0.0.0/0           ldap dapserver=192.168.7.100 ldapport=389 ldapprefix=<span class="string">"cn="</span> ldapsuffix=<span class="string">", dc=organization, dc=com"</span></span><br></pre></td></tr></table></figure><p>Please note that LDAP by default is not encrypted and communicating user credential unencrypted is never a good idea. LDAP over TLS is supported by appending <code>ldaptls=1</code> to the ldap attributes in pg_hba.conf file. Please also note that <code>ldaptls=1</code> only provides secured connection between PostgreSQL server and LDAP server. The connection between PostgreSQL server and client is not using TLS by default, so it needs to be enabled as well. TLS is discussed in details in part 2 of this blog.</p><h3 id="5-Authentication-with-PAM"><a href="#5-Authentication-with-PAM" class="headerlink" title="5. Authentication with PAM"></a>5. Authentication with PAM</h3><p>PAM stands for Pluggable Authentication Module and it operates similarly to password. The default service name is <code>postgresql</code>. First we need to create a linux user (Example based on Ubuntu 18.04).</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ useradd production_user</span><br><span class="line">$ passwd production_user</span><br><span class="line">Changing password <span class="keyword">for</span> user production_user.</span><br><span class="line">New UNIX password:</span><br><span class="line">Retype new UNIX password:</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure><p>Create <code>/etc/pam.d/postgresql</code> with the content:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#%PAM-1.0</span><br><span class="line">auth      include      system-auth</span><br><span class="line">ccount    include      system-auth</span><br><span class="line">password  include      system-auth</span><br><span class="line">session   include      system-auth</span><br></pre></td></tr></table></figure><p>Create <code>production_user</code> in PostgreSQL server</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">USER</span> production_user;</span><br></pre></td></tr></table></figure><p>Then finally update the pg_hba.conf with <code>pam</code> authentication method.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TYPE  DATABASE         USER                 ADDRESS                 METHOD</span></span><br><span class="line">host production_team     production_user      0.0.0.0/0               pam</span><br></pre></td></tr></table></figure><h3 id="6-Authentication-with-Certificate"><a href="#6-Authentication-with-Certificate" class="headerlink" title="6. Authentication with Certificate"></a>6. Authentication with Certificate</h3><p>Authentication with certificate can be applied to all the authentication methods by appending <code>clientcert=1</code> in method parameters. This is only useful with <code>hostssl</code> type records in <code>pg_hba.conf</code> file and requires that the PostgreSQL server has TLS enabled in <code>postgresql.conf</code> with path to CA certificate specified. We will discuss TLS and certificates in part 2 of the blog in more details.</p><p>With <code>clientcert=1</code> in place, the server will require that the client to send its TLS certificate for verification. The connection will abort if client fails to provide a certificate. The server will verify the common name (CN) in the certificate against the server’s hostname. Both should match. In addition, certificate validity dates will be verified and most importantly, the server will try to determine the chain of trust from the client certificate against the CA certificate configured in the server to determine if the client can be trusted.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TYPE  DATABASE         USER                 ADDRESS                 METHOD</span></span><br><span class="line">hostssl production_team     production_user      0.0.0.0/0            pam clientcert=1</span><br></pre></td></tr></table></figure><h3 id="7-Role-Based-Access-Control"><a href="#7-Role-Based-Access-Control" class="headerlink" title="7. Role-Based Access Control"></a>7. Role-Based Access Control</h3><p>Role-based access control refers to the process of verifying database access permissions based on the pre-defined roles and user privileges. PostgreSQL supports role-based access in several levels, such as table, function, procedural language and user levels. I will explain the concept in table and user level access control that follow the general guidelines below:</p><ul><li>A user with super user privilege can do any activities in the database</li><li>A user who creates a table owns the table and can set its permission</li><li>A user needs to belong to a proper role to perform administrative operations such as create another user or role</li><li>Other users need proper permissions to view or operate on a table created by another user.</li></ul><p>When a PostgreSQL database cluster has been initialized, a super user will be created by default that equals to the system user that initializes the cluster. This super user is the starting point to define other role and other users and privileges to ensure proper database access.</p><h3 id="8-Assign-Table-and-Column-Level-Privileges-to-Users"><a href="#8-Assign-Table-and-Column-Level-Privileges-to-Users" class="headerlink" title="8. Assign Table and Column Level Privileges to Users"></a>8. Assign Table and Column Level Privileges to Users</h3><p>The <code>GRANT</code> clause supported in PostgreSQL is used to configure the access privileges (official documentation here: <a href="https://www.postgresql.org/docs/current/sql-grant.html">https://www.postgresql.org/docs/current/sql-grant.html</a>). <code>GRANT</code> is a very universal clause that can be used to add access privileges to tables, databases, roles, table spaces…etc. The opposite of <code>GRANT</code> is <code>REVOKE</code>, which removes privileges (official documentation here: <a href="https://www.postgresql.org/docs/current/sql-revoke.html">https://www.postgresql.org/docs/current/sql-revoke.html</a>). In this blog, I will use <code>GRANT</code> on table and role level. When a table is created, it is assigned an owner. The owner is normally the user that executed the creation statement. The initial state is that only the owner (or a superuser) can do anything with the table. To allow other users to use it, privileges must be granted.</p><p>There are many types of privileges that can be granted to a table or a table column. The image below is taken directly from the official PostgreSQL documentation that lists all the available privileges and their applicable objects.</p><p><img src="/images/table5.1.PNG" alt="privilege_table"></p><p>Consider a simple SQL command example below that assigns table and column access privileges to other users</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> userA;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> userA;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> userA;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> userB;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> userB;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> userB;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> userC;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span>(column1), <span class="keyword">UPDATE</span>(column3) <span class="keyword">ON</span> table3 <span class="keyword">TO</span> userC;</span><br></pre></td></tr></table></figure><p>The above SQL commands can be illustrated as:</p><p><img src="/images/user-priv.png" alt="user_privilege"></p><h3 id="9-Assign-User-Level-Privileges-as-Roles"><a href="#9-Assign-User-Level-Privileges-as-Roles" class="headerlink" title="9. Assign User Level Privileges as Roles"></a>9. Assign User Level Privileges as Roles</h3><p>A <code>ROLE</code> is an entity that can own database objects and have database privileges; a role can be considered a “user”, a “group”, or both depending on how it is used.(official documentation here: <a href="https://www.postgresql.org/docs/current/sql-createrole.html">https://www.postgresql.org/docs/current/sql-createrole.html</a>). Similar to a table, a created role can be altered with the <code>ALTER</code> clause or deleted with the <code>DROP</code> clause. </p><p>Please note that when a role is created initially, the permission to <code>LOGIN</code> is not allowed by default and has to be manually set such that the users belonging to this role can log in to the database server. The same can be done with <code>CREATE USER</code> clause, which allows <code>LOGIN</code> by default. So the following 2 commands are essentially the same</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> username LOGIN;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">USER</span> username;</span><br></pre></td></tr></table></figure><p>The following image is taken directly from the official PostgreSQL documentation that lists all the privilege keywords that can be associated to a role.</p><p><img src="/images/role-synopsis.PNG" alt="role-synopsis"></p><p>Consider the following simple example that creates 3 users and 4 different roles having different user level access privileges.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Create 3 users */</span></span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">USER</span> userA;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">USER</span> userB;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">USER</span> userC;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Create 4 roles */</span></span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role1 LOGIN CREATEDB CREATEROLE;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role2 <span class="keyword">WITH</span> <span class="keyword">PASSWORD</span> <span class="string">'12345678'</span> LOGIN <span class="keyword">REPLICATION</span>;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role3 LOGIN CREATEDB INHERIT;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role4 LOGIN <span class="keyword">CONNECTION</span> <span class="keyword">LIMIT</span> <span class="number">5</span> ;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> role1 <span class="keyword">TO</span> userA;</span><br><span class="line">$ <span class="keyword">GRANT</span> role1 <span class="keyword">TO</span> userB;</span><br><span class="line">$ <span class="keyword">GRANT</span> role3 <span class="keyword">TO</span> userC;</span><br><span class="line">$ <span class="keyword">GRANT</span> role4 <span class="keyword">TO</span> userC;</span><br><span class="line"></span><br><span class="line">postgres=<span class="comment"># \du+</span></span><br><span class="line">                                          List of roles</span><br><span class="line"> Role name |                         Attributes                         | Member of      | Description </span><br><span class="line"><span class="comment">-----------+------------------------------------------------------------+----------------+-------------</span></span><br><span class="line"> postgres  | Superuser, <span class="keyword">Create</span> <span class="keyword">role</span>, <span class="keyword">Create</span> DB, <span class="keyword">Replication</span>, Bypass RLS | &#123;&#125;             | </span><br><span class="line"> userA     |                                                            | &#123;role1&#125;        | </span><br><span class="line"> userB     |                                                            | &#123;role1&#125;        | </span><br><span class="line"> userC     |                                                            | &#123;role3, role4&#125; | </span><br><span class="line"> role1     | <span class="keyword">Replication</span>, <span class="keyword">Create</span> DB, <span class="keyword">Create</span> <span class="keyword">role</span>                        | &#123;&#125;             | </span><br><span class="line"> role2     | <span class="keyword">Replication</span>                                                | &#123;&#125;             | </span><br><span class="line"> role3     | <span class="keyword">Create</span> DB                                                  | &#123;&#125;             | </span><br><span class="line"> role4     | <span class="number">5</span> connections                                              | &#123;&#125;             | </span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role1;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role1;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role3;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span>(column1), <span class="keyword">UPDATE</span>(column3) <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role3;</span><br></pre></td></tr></table></figure><p>Use the <code>\du+</code> meta command to see all the roles that have been created with summary of the attributes associated with each role. To see the full list of attributes per role, use the SQL command <code>SELECT * FROM pg_roles;</code>.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">postgres&#x3D;# \du+</span><br><span class="line">                                          List of roles</span><br><span class="line"> Role name |                         Attributes                         | Member of | Description </span><br><span class="line">-----------+------------------------------------------------------------+-----------+-------------</span><br><span class="line"> postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS | &#123;&#125;        | </span><br><span class="line"> userA     |                                                            | &#123;&#125;        | </span><br><span class="line"> userB     |                                                            | &#123;&#125;        | </span><br><span class="line"> userC     |                                                            | &#123;&#125;        | </span><br><span class="line"> role1     | Replication, Create DB, Create role                        | &#123;&#125;        | </span><br><span class="line"> role2     | Replication                                                | &#123;&#125;        | </span><br><span class="line"> role3     | Create DB                                                  | &#123;&#125;        | </span><br><span class="line"> role4     | 5 connections                                              | &#123;&#125;        |</span><br></pre></td></tr></table></figure><h3 id="10-Assign-Table-Level-Privileges-via-Roles"><a href="#10-Assign-Table-Level-Privileges-via-Roles" class="headerlink" title="10. Assign Table Level Privileges via Roles"></a>10. Assign Table Level Privileges via Roles</h3><p>Section 3.1 illustrates privilege assignments directly to each individual users, which is desirable in smaller database servers. Imagine a larger database server where there could potentially be hundreds of users exist in the entire database cluster. Managing the table level privileges would get quite complicated and tedious. Luckily, PostgreSQL supports assigning users to roles for better privilege management </p><p>Following the examples in section 3.2, we can use the <code>GRANT</code> command again to assign users to roles. Note that the <code>Member of</code> will display the relationship between users and roles after we have related them with <code>GRANT</code> clause.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ GRANT role1 TO userA;</span><br><span class="line">$ GRANT role1 TO userB;</span><br><span class="line">$ GRANT role3 TO userC;</span><br><span class="line">$ GRANT role4 TO userC;</span><br><span class="line"></span><br><span class="line">postgres&#x3D;# \du+</span><br><span class="line">                                          List of roles</span><br><span class="line"> Role name |                         Attributes                         | Member of      | Description </span><br><span class="line">-----------+------------------------------------------------------------+----------------+-------------</span><br><span class="line"> postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS | &#123;&#125;             | </span><br><span class="line"> userA     |                                                            | &#123;role1&#125;        | </span><br><span class="line"> userB     |                                                            | &#123;role1&#125;        | </span><br><span class="line"> userC     |                                                            | &#123;role3, role4&#125; | </span><br><span class="line"> role1     | Replication, Create DB, Create role                        | &#123;&#125;             | </span><br><span class="line"> role2     | Replication                                                | &#123;&#125;             | </span><br><span class="line"> role3     | Create DB                                                  | &#123;&#125;             | </span><br><span class="line"> role4     | 5 connections                                              | &#123;&#125;             |</span><br></pre></td></tr></table></figure><p>Following the examples in section 3.1, we can use the <code>GRANT</code> command again to assign table level privileges to roles that we have created instead of to users directly</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role1;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table1 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role1;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">INSERT</span> <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role1;</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> table2 <span class="keyword">TO</span> role3;</span><br><span class="line">$ <span class="keyword">GRANT</span> <span class="keyword">SELECT</span>(column1), <span class="keyword">UPDATE</span>(column3) <span class="keyword">ON</span> table3 <span class="keyword">TO</span> role3;</span><br></pre></td></tr></table></figure><p>The above SQL commands can be illustrated as:</p><p><img src="/images/role-priv.png" alt="role-privilege"></p><h3 id="11-Role-Inheritance"><a href="#11-Role-Inheritance" class="headerlink" title="11. Role Inheritance"></a>11. Role Inheritance</h3><p><code>INHERIT</code> and <code>NOINHERIT</code> are one of the special attributes that can be assigned to a role. When a role (say role 1) contains <code>INHERIT</code> attribute and is a member of another role (say role 2). All the attributes existing in both role 1 and role 2 will be applied to the user. </p><p>Consider a simple example below:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role1 LOGIN CREATEDB <span class="keyword">REPLICATION</span>;</span><br><span class="line">$ <span class="keyword">CREATE</span> <span class="keyword">ROLE</span> role2 LOGIN CREATEROLE INHERIT;</span><br><span class="line">$ <span class="keyword">GRANT</span> role1 <span class="keyword">TO</span> role2;</span><br><span class="line">$ <span class="keyword">GRANT</span> role2 <span class="keyword">TO</span> userA;</span><br></pre></td></tr></table></figure><p>Which can be visualized as:</p><p><img src="/images/inherit.png" alt="role-privilege"></p><p>In this case, role2 is created with <code>INHERIT</code>, userA will be assigned the privileges defined in both role1 and role2. </p><h3 id="12-Summary"><a href="#12-Summary" class="headerlink" title="12. Summary"></a>12. Summary</h3><p>In this blog, we went over several mechanisms in postgreSQL that allows a database administrator to configure the authentication of incoming user connections and the privilege configuration in table, column and user level via the concept of roles. PostgreSQL provides <code>pg_hba.conf</code> file that configures simple authentication and supports stronger authentication methods against remote authentication services such as GSSAPI, kerberos, RADIUS, PAM and LDAP..etc. So far we have only talked about authentication and authorization (AA) in PostgreSQL terms, in part 2 of this blog, I will explain the general concept of data encryption, how to secure data communication between server and client with TLS and how to achieve encryption on storage devices.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;PostgreSQL is packed with several security features for a database administrator to utilize according to his or her organizational security needs. The word &lt;code&gt;Security&lt;/code&gt; is a very broad concept and could refer to completely different procedures and methodology to achieve in different PostgreSQL components. This blog is divided into part 1, 2 and 3 and I will explain the word &lt;code&gt;Security&lt;/code&gt; with regards to PostgreSQL version 12.1 and how it is practiced in different areas within the system. &lt;/p&gt;
&lt;p&gt;In Part 1 of the blog, I will be discussing the basic security features that exist in PostgreSQL with emphasis on Host-based authentication methods as well as user-based access control with the concept of roles. If done right, we could have a much more robust database server and potentially reduce the attack surface on the server, protecting it from attacks like SQL injections. I will also briefly discuss a few of the advanced authentication methods such as LDAP and PAM authentication. There are many more advanced authentication methods supported and we will be producing more articles in the near future to cover more of these methods.&lt;/p&gt;
&lt;p&gt;In Part 2 of the blog, I will be discussing TLS in greater detail, which I believe is crucial for a database administrator to understand first before enabling TLS in the PostgreSQL server. TLS is a fairly large and one of the least understood protocol today, which contains a lot of security components and methodology related to cryptography that could be quite confusing. &lt;/p&gt;
&lt;p&gt;In Part 3 of the blog, I will be discussing how to apply TLS configurations to both PostgreSQL server and client following the TLS principles that have been discussed in Part 2. I will also briefly discuss Transparent Data Encryption (TDE) that the PG community is currently working on that introduces another layer of secured database environment.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="networking" scheme="http://caryhuang.github.io/tags/networking/"/>
    
    <category term="security" scheme="http://caryhuang.github.io/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>Steaming-Replication-Setup-in-PG12-How-to-do-it-right</title>
    <link href="http://caryhuang.github.io/2019/10/29/Steaming-Replication-Setup-in-PG12-How-to-do-it-right/"/>
    <id>http://caryhuang.github.io/2019/10/29/Steaming-Replication-Setup-in-PG12-How-to-do-it-right/</id>
    <published>2019-10-30T06:36:39.000Z</published>
    <updated>2020-01-13T23:39:22.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>PostgreSQL 12 has been considered as a major update consisting of major performance boost with partitioning enhancements, indexing improvements, optimized planner logics and several others. One of the major changes is noticeably the removal of <code>recovery.conf</code> in a standby cluster. For this reason, the procedure to set up a streaming replication clusters has changed, and in this blog, I will demonstrate how to properly setup a streaming replication setup in PG12.</p><p>Streaming replication setup requires a master cluster and one or more slave clusters that will replicate the data inserted to the master by streaming the archived WAL files generated by master. The master and slaves can reside on different machines connected via network but in this blog, we will use one master and one slave setup and both will be run on the same machine with different port number.</p><p>The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04</p><a id="more"></a><h3 id="2-Master-Database-Cluster-Setup"><a href="#2-Master-Database-Cluster-Setup" class="headerlink" title="2. Master Database Cluster Setup"></a>2. Master Database Cluster Setup</h3><p>Create a master database cluster using initdb tool:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ initdb /home/caryh/streaming-replication/db-master</span><br><span class="line">$ <span class="built_in">cd</span> /home/caryh/streaming-replication</span><br></pre></td></tr></table></figure><p>/home/caryh/streaming-replication is the root folder to all the database clusters that we will be creating in this blog and db-master directory will be created here as a result of above commands. Let’s modify the default postgreql.conf and enable several important configuration options as shown below for streaming replication setup.</p><figure class="highlight bash"><figcaption><span>db-master/postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">wal_level = replica</span><br><span class="line">archive_mode = on</span><br><span class="line">max_wal_senders = 10 </span><br><span class="line">wal_keep_segments = 10</span><br><span class="line">hot_standby = on</span><br><span class="line">archive_command = <span class="string">'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'</span></span><br><span class="line">port = 5432</span><br><span class="line">wal_log_hints = on</span><br></pre></td></tr></table></figure><p>The configuration above enables Postgres to archive the WAL files in the directory /home/caryh/streaming-replication/archivedir/ when it has completed writing to a full block of WAL file or when pg_basebackup command has been issued. The %f and %p used within <code>archive_command</code> are internal to Postgres and %f will be replaced with the filename of the target WAL file and %p replaced with path to the targeted WAL file. </p><p>It is very important when setting the <code>archive_command</code> to ensure the WAL files are archived to a location where the slave cluster can access. </p><p>Please note that <code>wal_log_hints</code> must be enabled for pg_rewind tool to work properly. We will discuss more about pg_rewind in the next blog post.</p><p>Examine the client authentication file <code>db-master/pg_hba.conf</code> and make sure the master cluster allows replication connections from a slave cluster remotely. In my case, both my master and slave will be run on the same host, so I will leave the loopback IP address as it is. If your slave cluster is located in another machine, make sure to replace the loopback address with the right one.</p><figure class="highlight bash"><figcaption><span>db-master/pg_hba.conf </span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Allow replication connections from 127.0.0.1, by a user with the replication privilege.</span></span><br><span class="line"><span class="comment"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span></span><br><span class="line">host    replication     all             127.0.0.1/32            trust</span><br></pre></td></tr></table></figure><p>Let’s go ahead and start the master database cluster with the above configuration files, create a super user with permission to do replication, and a database called clusterdb</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-master start</span><br><span class="line">$ createuser cary -s --replication</span><br><span class="line">$ createdb clusterdb</span><br></pre></td></tr></table></figure><p>Insert some test data to the master cluster. For simplicity, we will insert 100 integers to <code>test_table</code>.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c "<span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table(x <span class="built_in">integer</span>)<span class="string">"</span></span><br><span class="line"><span class="string">CREATE TABLE</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ psql -d clusterdb -U cary -c "</span><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(x) <span class="keyword">SELECT</span> y <span class="keyword">FROM</span> generate_series(<span class="number">1</span>, <span class="number">100</span>) a(y)<span class="string">"</span></span><br><span class="line"><span class="string">INSERT 0 100</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ psql -d clusterdb -U cary -c "</span><span class="keyword">SELECT</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> test_table<span class="string">"</span></span><br><span class="line"><span class="string"> count </span></span><br><span class="line"><span class="string">-------</span></span><br><span class="line"><span class="string">   100</span></span><br><span class="line"><span class="string">(1 row)</span></span><br></pre></td></tr></table></figure><h3 id="3-Slave-Database-Cluster-Setup"><a href="#3-Slave-Database-Cluster-Setup" class="headerlink" title="3. Slave Database Cluster Setup"></a>3. Slave Database Cluster Setup</h3><p>The goal of setting up the slave cluster is to make a backup of the current master and set it up as a standby server, meaning it will stream the WAL file updates from the master and perform replication of the data.</p><p>Postgres provides several tools and methods to perform physical database backup. Exclusive methods such as <code>pg_start_backup(&#39;label&#39;)</code> and <code>pg_stop_backup()</code> are quite common in earlier Postgres versions. In this blog, we will use the newer, and simpler non-exclusive <code>pg_basebackup</code> fronend tool to execute the backup. There are advantages and disadvantaged for both methods and this discussion is not within the scope of this blog. This article here provides very good explaination on both methods: <a href="https://www.cybertec-postgresql.com/en/exclusive-backup-deprecated-what-now/">https://www.cybertec-postgresql.com/en/exclusive-backup-deprecated-what-now/</a></p><p>Let’s use pg_basebackup to create the slave cluster.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_basebackup -h 127.0.0.1 -U cary -p 5432 -D db-slave -P -Xs -R</span><br><span class="line">31373/31373 kB (100%), 1/1 tablespace</span><br></pre></td></tr></table></figure><p>where:<br>-h is the IP of the master cluster<br>-U is the username that is permitted to do replication<br>-p is the port number of the running master cluster<br>-D is the directory where we want to set up the slave database cluster<br>-P to show the progress<br>-Xs to select WAL streaming method<br>-R to write a recovery.conf file.</p><p>This step is where it would differ from the previous PG versions. The -R command will no longer output a recovery.conf file in the db-slave directory. </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ls db-slave</span><br><span class="line">backup_label  pg_dynshmem    pg_multixact  pg_snapshots  pg_tblspc    pg_xact</span><br><span class="line">base          pg_hba.conf    pg_notify     pg_stat       pg_twophase  postgresql.auto.conf</span><br><span class="line">global        pg_ident.conf  pg_replslot   pg_stat_tmp   PG_VERSION   postgresql.conf</span><br><span class="line">pg_commit_ts  pg_logical     pg_serial     pg_subtrans   pg_wal       standby.signal</span><br></pre></td></tr></table></figure><p>The contents of the old recovery.conf file are moved to <code>postgresql.conf</code> and <code>postgresql.auto.conf</code> instead.</p><p>Let’s examine <code>db-slave/postgresql.auto.conf</code> first, and we will see that pg_basebackup already created the <code>primary_conninfo</code> for us. This line used to be located in <code>recovery.conf</code> and it tells where and how a slave cluster should stream from the master cluster. Make sure this line is present in the <code>postgresql.auto.conf</code>. </p><figure class="highlight bash"><figcaption><span>db-slave/postgresql.auto.conf</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Do not edit this file manually!</span></span><br><span class="line"><span class="comment"># It will be overwritten by the ALTER SYSTEM command.</span></span><br><span class="line">primary_conninfo = <span class="string">'user=cary passfile='</span><span class="string">'/home/caryh/.pgpass'</span><span class="string">' host=127.0.0.1 port=5432 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any'</span></span><br></pre></td></tr></table></figure><p>Let’s examine <code>db-slave/postgresql.conf</code> and update some of the parameters.</p><figure class="highlight bash"><figcaption><span>db-slave/postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">wal_level = replica</span><br><span class="line">archive_mode = on</span><br><span class="line">max_wal_senders = 10 </span><br><span class="line">wal_keep_segments = 10</span><br><span class="line">hot_standby = on</span><br><span class="line">archive_command = <span class="string">'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'</span></span><br><span class="line">wal_log_hints = on</span><br><span class="line">port = 5433</span><br><span class="line">restore_command = <span class="string">'cp /home/caryh/streaming-replication/archivedir/%f %p'</span></span><br><span class="line">archive_cleanup_command = <span class="string">'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r'</span></span><br></pre></td></tr></table></figure><p>Since <code>db-slave/postgresql.conf</code> is directly copied from master cluster via pg_basebackup, we will need to change the <code>port</code> to some port different (5433 in this case) from the master since both are running on the same machine. We will need to fill the <code>restore_command</code> and <code>archive_cleanup_command</code> so the slave cluster knows how to get the archived WAL files for streaming purposes. These two parameters used to be defined in <code>recovery.conf</code> and are moved to <code>postgresql.conf</code> in PG12.</p><p>In the db-slave directory, please note that a new <code>standby.signal</code> file is created automatically by <code>pg_basebackup</code> to indicate that this slave cluster will be run in <code>standby</code> mode. The <code>standby.signal</code> file is a new addition in PG12 to replace <code>standby_mode = &#39;on&#39;</code> that used to be defined in <code>recovery.conf</code>. If this file is not present, make sure it is created by:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ touch db-slave/standby.signal</span><br></pre></td></tr></table></figure><p>Now, let’s start the slave cluster:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-slave start</span><br></pre></td></tr></table></figure><h3 id="4-Verify-the-Streaming-Replication-Setup"><a href="#4-Verify-the-Streaming-Replication-Setup" class="headerlink" title="4. Verify the Streaming Replication Setup"></a>4. Verify the Streaming Replication Setup</h3><p>Once both master and slave clusters are setup and running, we should see from the <code>ps -ef</code> command that some of the backend processes are started to achieve the replication, namely, <code>walsender</code> and <code>walreceiver</code>.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ps -ef | grep postgres</span><br><span class="line">caryh    12782  2921  0 16:12 ?        00:00:00 /usr/<span class="built_in">local</span>/pgsql/bin/postgres -D db-master</span><br><span class="line">caryh    12784 12782  0 16:12 ?        00:00:00 postgres: checkpointer   </span><br><span class="line">caryh    12785 12782  0 16:12 ?        00:00:00 postgres: background writer   </span><br><span class="line">caryh    12786 12782  0 16:12 ?        00:00:00 postgres: walwriter   </span><br><span class="line">caryh    12787 12782  0 16:12 ?        00:00:00 postgres: autovacuum launcher   </span><br><span class="line">caryh    12788 12782  0 16:12 ?        00:00:00 postgres: archiver   last was 000000010000000000000002.00000028.backup</span><br><span class="line">caryh    12789 12782  0 16:12 ?        00:00:00 postgres: stats collector   </span><br><span class="line">caryh    12790 12782  0 16:12 ?        00:00:00 postgres: logical replication launcher   </span><br><span class="line">caryh    15702  2921  0 17:06 ?        00:00:00 /usr/<span class="built_in">local</span>/pgsql/bin/postgres -D db-slave</span><br><span class="line">caryh    15703 15702  0 17:06 ?        00:00:00 postgres: startup   recovering 000000010000000000000003</span><br><span class="line">caryh    15708 15702  0 17:06 ?        00:00:00 postgres: checkpointer   </span><br><span class="line">caryh    15709 15702  0 17:06 ?        00:00:00 postgres: background writer   </span><br><span class="line">caryh    15711 15702  0 17:06 ?        00:00:00 postgres: stats collector   </span><br><span class="line">caryh    15713 15702  0 17:06 ?        00:00:00 postgres: walreceiver   streaming 0/3000148</span><br><span class="line">caryh    15714 12782  0 17:06 ?        00:00:00 postgres: walsender cary 127.0.0.1(59088) streaming 0/3000148</span><br><span class="line">caryh    15728 10962  0 17:06 pts/5    00:00:00 grep --color=auto post</span><br></pre></td></tr></table></figure><p>We can also check the replication status in details by issuing a query to the master cluster:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_stat_replication;"</span> -x -p 5432</span><br><span class="line">-[ RECORD 1 ]----+------------------------------</span><br><span class="line">pid              | 15714</span><br><span class="line">usesysid         | 16384</span><br><span class="line">usename          | cary</span><br><span class="line">application_name | walreceiver</span><br><span class="line">client_addr      | 127.0.0.1</span><br><span class="line">client_hostname  | </span><br><span class="line">client_port      | 59088</span><br><span class="line">backend_start    | 2019-10-29 17:06:49.072082-07</span><br><span class="line">backend_xmin     | </span><br><span class="line">state            | streaming</span><br><span class="line">sent_lsn         | 0/3000148</span><br><span class="line">write_lsn        | 0/3000148</span><br><span class="line">flush_lsn        | 0/3000148</span><br><span class="line">replay_lsn       | 0/3000148</span><br><span class="line">write_lag        | </span><br><span class="line">flush_lag        | </span><br><span class="line">replay_lag       | </span><br><span class="line">sync_priority    | 0</span><br><span class="line">sync_state       | async</span><br><span class="line">reply_time       | 2019-10-29 17:10:09.515563-07</span><br></pre></td></tr></table></figure><p>Lastly, we can insert additional data to the master cluster and verify that slave also has the data updated.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Query slave cluster</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5433</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   100</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query master cluster</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5432</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   100</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Insert more data to master cluster</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y)"</span> -p 5432</span><br><span class="line">INSERT 0 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># Query slave cluster again</span></span><br><span class="line">psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5433</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   200</span><br><span class="line">(1 row)</span><br></pre></td></tr></table></figure><p>Both master and slave clusters are now in sync.</p><h3 id="5-Setup-Replication-Slots"><a href="#5-Setup-Replication-Slots" class="headerlink" title="5. Setup Replication Slots"></a>5. Setup Replication Slots</h3><p>The previous steps illustrate how to correctly setup streaming replication between a master and slave cluster. However, there may be a case where the slave can be disconnected for some reason for extended period of time and may fail to replicate with the master when some of the un-replicated WAL files are recycled or deleted from the master cluster controlled by <code>wal_keep_segments</code> parameter. </p><p>Replication slots ensure master can retain enough WAL segments for all slaves to receive them and prevent the master from removing rows that could cause a recovery conflict on the slaves.</p><p>Let’s create a replication slot on the master cluster called <code>slave</code>:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_create_physical_replication_slot('slave')"</span> -p 5432</span><br><span class="line"> slot_name | lsn </span><br><span class="line">-----------+-----</span><br><span class="line"> slave     | </span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_replication_slots"</span> -x -p 5432</span><br><span class="line">-[ RECORD 1 ]-------+---------</span><br><span class="line">slot_name           | slave</span><br><span class="line">plugin              | </span><br><span class="line">slot_type           | physical</span><br><span class="line">datoid              | </span><br><span class="line">database            | </span><br><span class="line">temporary           | f</span><br><span class="line">active              | f</span><br><span class="line">active_pid          | </span><br><span class="line">xmin                | </span><br><span class="line">catalog_xmin        | </span><br><span class="line">restart_lsn         | </span><br><span class="line">confirmed_flush_lsn |</span><br></pre></td></tr></table></figure><p>We have just created replication slot on master called <code>slave</code> and it is currently not active (active = f).</p><p>Let’s modify slave’s <code>postgresql.conf</code> and make it connect to the master’s replication slot</p><figure class="highlight bash"><figcaption><span>db-slave/postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">primary_slot_name = <span class="string">'slave'</span></span><br></pre></td></tr></table></figure><p>Please note that this argument <code>primary_slot_name</code> us also used to be defined in <code>recovery.conf</code> and moved to <code>postgresql.conf</code> in PG12. After the change, we are required to restart the slave.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-slave stop</span><br><span class="line">$ pg_ctl -D db-slave start</span><br></pre></td></tr></table></figure><p>If all is good, checking the replication slots on master should have the slot status as active.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_replication_slots"</span> -x -p 5432</span><br><span class="line">-[ RECORD 1 ]-------+----------</span><br><span class="line">slot_name           | slave</span><br><span class="line">plugin              | </span><br><span class="line">slot_type           | physical</span><br><span class="line">datoid              | </span><br><span class="line">database            | </span><br><span class="line">temporary           | f</span><br><span class="line">active              | t</span><br><span class="line">active_pid          | 16652</span><br><span class="line">xmin                | </span><br><span class="line">catalog_xmin        | </span><br><span class="line">restart_lsn         | 0/3003B98</span><br><span class="line">confirmed_flush_lsn |</span><br></pre></td></tr></table></figure><h3 id="6-Summary"><a href="#6-Summary" class="headerlink" title="6. Summary"></a>6. Summary</h3><p>In this blog, we have discussed the updated procedures to setup streaming replication clusters in PG12, in which several steps have been changed from the older versions, particularly the removal of <code>recovery.conf</code>. </p><p>Here is a short list of changes related to replication setup that have been moved from <code>recovery.conf</code></p><ul><li>restore_command             =&gt; moved to <code>postgresql.conf</code></li><li>recovery_target_timeline    =&gt; moved to <code>postgresql.conf</code></li><li>standby_mode                =&gt; replaced by <code>standby.signal</code></li><li>primary_conninfo            =&gt; moved to <code>postgresql.conf</code> or <code>postgresql.auto.conf</code></li><li>archive_cleanup_command     =&gt; moved to <code>postgresql.conf</code></li><li>primary_slot_name           =&gt; moved to <code>postgresql.conf</code></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h3&gt;&lt;p&gt;PostgreSQL 12 has been considered as a major update consisting of major performance boost with partitioning enhancements, indexing improvements, optimized planner logics and several others. One of the major changes is noticeably the removal of &lt;code&gt;recovery.conf&lt;/code&gt; in a standby cluster. For this reason, the procedure to set up a streaming replication clusters has changed, and in this blog, I will demonstrate how to properly setup a streaming replication setup in PG12.&lt;/p&gt;
&lt;p&gt;Streaming replication setup requires a master cluster and one or more slave clusters that will replicate the data inserted to the master by streaming the archived WAL files generated by master. The master and slaves can reside on different machines connected via network but in this blog, we will use one master and one slave setup and both will be run on the same machine with different port number.&lt;/p&gt;
&lt;p&gt;The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="replication" scheme="http://caryhuang.github.io/tags/replication/"/>
    
  </entry>
  
  <entry>
    <title>Replication-Failover-with-pg_rewind-in-PG12</title>
    <link href="http://caryhuang.github.io/2019/09/27/Replication-Failover-with-pg-rewind-in-PG12/"/>
    <id>http://caryhuang.github.io/2019/09/27/Replication-Failover-with-pg-rewind-in-PG12/</id>
    <published>2019-09-28T06:36:52.000Z</published>
    <updated>2020-01-13T23:39:42.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>In the previous blog, we have discussed how to correctly set up streaming replication clusters between one master and one slave in Postgres version 12. In this blog, we will simulate a failover scenario on the master database, which causes the replica (or slave) database cluster to be promoted as new master and continue the operation. We will also simulate a failback scenario to reuse the old master cluster after the failover scenario with the help of pg_rewind. </p><p>Normally it is quite easy to do a failback to the old master after slave gets promoted to master but if there is data written to the old master after slave promotion, we will have an out-of-sync case between them both and we will have to use the pg_rewind tool to synchronize the two data directories in order to bring the old master to match the state of the new master. Please note that the pg_rewind tool will remove transactions from the old master in order to match up with the new, so certain pre-caution is needed to use this tool.</p><p>Here’s a brief overview of list of actions we are going to perform:</p><ul><li>simulate failover by promoting <code>slave</code> cluster, so it becomes a <code>new master</code></li><li>simulate data insertion to <code>master</code> cluster, also referred as <code>old master</code> after promotion</li><li>shutdown the <code>old master</code> cluster and set it up as a standby server</li><li>run pg_rewind on <code>old master</code> to synchronize transaction states with <code>new master</code></li><li>bring up <code>old master</code> as a standby server to synchronize with the <code>new master</code></li></ul><p>This blog assumes you already have streaming replication setup between one master and one slave from previous blog. If you have not checked out the previous blog titled “Streaming Replication Setup in PG12 - How to Do it Right”, it is recommended to give that a read first.</p><p>The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04</p><a id="more"></a><h3 id="2-Simulate-a-Failover-Case"><a href="#2-Simulate-a-Failover-Case" class="headerlink" title="2. Simulate a Failover Case"></a>2. Simulate a Failover Case</h3><p>We will simply promote the slave database cluster to simulate a failover.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl promote -D db-slave</span><br><span class="line"></span><br><span class="line">2019-10-30 11:10:16.951 PDT [16643] LOG:  received promote request</span><br><span class="line">2019-10-30 11:10:16.951 PDT [16651] FATAL:  terminating walreceiver process due to administrator <span class="built_in">command</span></span><br><span class="line">2019-10-30 11:10:16.966 PDT [16643] LOG:  redo <span class="keyword">done</span> at 0/3003B60</span><br><span class="line">2019-10-30 11:10:16.991 PDT [16643] LOG:  selected new timeline ID: 2</span><br><span class="line">2019-10-30 11:10:17.030 PDT [16643] LOG:  archive recovery complete</span><br><span class="line">2019-10-30 11:10:17.051 PDT [16642] LOG:  database system is ready to accept connections</span><br></pre></td></tr></table></figure><p>As seen above, After <code>slave</code> gets promoted, it switches to a new timeline for future data operations. At this point the <code>master</code> and <code>slave</code> are no longer streaming WAL files from each other and we essentialyl have two independent database clusters running. We will call them <code>old master</code> and <code>new master</code> in the following sections instead so it is clear.</p><h3 id="3-Insert-Some-Data-to-the-Old-Master"><a href="#3-Insert-Some-Data-to-the-Old-Master" class="headerlink" title="3. Insert Some Data to the Old Master"></a>3. Insert Some Data to the Old Master</h3><p>We would like to create a data out-of-sync case by inserting some more data to the <code>old master</code> cluster.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y);"</span> -p 5432</span><br><span class="line">INSERT 0 100</span><br></pre></td></tr></table></figure><p>Check that both the <code>old master</code> and <code>new master</code> are clearly out of sync:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## new master ##</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5433</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   200</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="comment">## old master ##</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5432</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   300</span><br><span class="line">(1 row)</span><br></pre></td></tr></table></figure><h3 id="4-Configure-the-Old-Master-as-Standby-Server-to-Sync-with-New-Master"><a href="#4-Configure-the-Old-Master-as-Standby-Server-to-Sync-with-New-Master" class="headerlink" title="4. Configure the Old Master as Standby Server to Sync with New Master"></a>4. Configure the Old Master as Standby Server to Sync with New Master</h3><p>Now we would like to attempt a failover to make the <code>old master</code> as a standy server to syncrhonize with the <code>new master</code>.</p><p>Let’s shutdown the <code>old master</code> cluster.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-master stop</span><br><span class="line">waiting <span class="keyword">for</span> server to shut down.... <span class="keyword">done</span></span><br><span class="line">server stopped</span><br></pre></td></tr></table></figure><p>Let’s update postgresql.conf in the <code>old master</code>:</p><figure class="highlight bash"><figcaption><span>db-master/postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">recovery_target_timeline = <span class="string">'latest'</span></span><br><span class="line">archive_cleanup_command = <span class="string">'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r'</span></span><br><span class="line">restore_command = <span class="string">'cp /home/caryh/streaming-replication/archivedir/%f %p'</span></span><br><span class="line">primary_slot_name = <span class="string">'main'</span></span><br><span class="line">primary_conninfo = <span class="string">'user=cary passfile='</span><span class="string">'/home/caryh/.pgpass'</span><span class="string">' host=127.0.0.1 port=5433 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any'</span></span><br></pre></td></tr></table></figure><p>the <code>primary_conninfo</code> tells the <code>old master</code> to stream WAL files from the <code>new master</code> located at 127.0.0.1:5433.</p><p>Also, do not forget to touch the <code>standby.signal</code> file to tell the cluster to run in standby mode:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch db-master/standby.signal</span><br></pre></td></tr></table></figure><p>We specified in the <code>old master</code> to connect to <code>primary_slot_name</code> = <code>main</code>. Let’s create the matching replication slot on the <code>new master</code>.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_create_physical_replication_slot('main');"</span> -p 5433</span><br><span class="line"> slot_name | lsn </span><br><span class="line">-----------+-----</span><br><span class="line"> main      | </span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"select * from pg_replication_slots;"</span> -p 5433 -x</span><br><span class="line">-[ RECORD 1 ]-------+---------</span><br><span class="line">slot_name           | main</span><br><span class="line">plugin              | </span><br><span class="line">slot_type           | physical</span><br><span class="line">datoid              | </span><br><span class="line">database            | </span><br><span class="line">temporary           | f</span><br><span class="line">active              | f</span><br><span class="line">active_pid          | </span><br><span class="line">xmin                | </span><br><span class="line">catalog_xmin        | </span><br><span class="line">restart_lsn         | </span><br><span class="line">confirmed_flush_lsn |</span><br></pre></td></tr></table></figure><p>Now the <code>new master</code> has a matching replication slot called <code>main</code> and it is not active at the moment. Now we are ready to start the <code>old master</code> as standby server</p><h3 id="5-Start-the-Old-Master-as-Standby-Server"><a href="#5-Start-the-Old-Master-as-Standby-Server" class="headerlink" title="5. Start the Old Master as Standby Server"></a>5. Start the Old Master as Standby Server</h3><p>Now, we are ready to start the <code>old master</code> as a standby:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-master start</span><br><span class="line"></span><br><span class="line">2019-10-30 11:30:04.071 PDT [1610] HINT:  Future <span class="built_in">log</span> output will go to <span class="built_in">log</span> destination <span class="string">"syslog"</span>.</span><br><span class="line">2019-10-30 11:30:04.075 PDT [1611] LOG:  database system was shut down at 2019-10-30 11:29:13 PDT</span><br><span class="line">2019-10-30 11:30:04.079 PDT [1611] LOG:  restored <span class="built_in">log</span> file <span class="string">"00000002.history"</span> from archive</span><br><span class="line">2019-10-30 11:30:04.082 PDT [1611] LOG:  entering standby mode</span><br><span class="line">2019-10-30 11:30:04.084 PDT [1611] LOG:  restored <span class="built_in">log</span> file <span class="string">"00000002.history"</span> from archive</span><br><span class="line">2019-10-30 11:30:04.095 PDT [1611] FATAL:  requested timeline 2 is not a child of this server<span class="string">'s history</span></span><br><span class="line"><span class="string">2019-10-30 11:30:04.095 PDT [1611] DETAIL:  Latest checkpoint is at 0/4000028 on timeline 1, but in the history of the requested timeline, the server forked off from that timeline at 0/3003B98.</span></span><br><span class="line"><span class="string">2019-10-30 11:30:04.096 PDT [1610] LOG:  startup process (PID 1611) exited with exit code 1</span></span><br><span class="line"><span class="string">2019-10-30 11:30:04.096 PDT [1610] LOG:  aborting startup due to startup process failure</span></span><br><span class="line"><span class="string">2019-10-30 11:30:04.098 PDT [1610] LOG:  database system is shut down</span></span><br></pre></td></tr></table></figure><p>As you can see above, the <code>old master</code> refuses to start because there is a timeline difference between the <code>old master</code> and the <code>new master</code>. This is caused by the additional data insertions that happens to the <code>old master</code> after the promotion event in step number 3. </p><p>This is where pg_rewind comes handy in situation like this, to synchronize the two clusters.</p><h3 id="6-Use-pg-rewind-to-Synchronize-the-two-Clusters"><a href="#6-Use-pg-rewind-to-Synchronize-the-two-Clusters" class="headerlink" title="6. Use pg_rewind to Synchronize the two Clusters"></a>6. Use pg_rewind to Synchronize the two Clusters</h3><p>Now, let’s synchronize the two database clusters with pg_rewind.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_rewind --target-pgdata=db-master --<span class="built_in">source</span>-server=<span class="string">"port=5433 user=cary dbname=clusterdb"</span> --progress</span><br><span class="line">pg_rewind: connected to server</span><br><span class="line">pg_rewind: servers diverged at WAL location 0/3003E58 on timeline 1</span><br><span class="line">pg_rewind: rewinding from last common checkpoint at 0/2000060 on timeline 1</span><br><span class="line">pg_rewind: reading <span class="built_in">source</span> file list</span><br><span class="line">pg_rewind: reading target file list</span><br><span class="line">pg_rewind: reading WAL <span class="keyword">in</span> target</span><br><span class="line">pg_rewind: need to copy 53 MB (total <span class="built_in">source</span> directory size is 78 MB)</span><br><span class="line">54363/54363 kB (100%) copied</span><br><span class="line">pg_rewind: creating backup label and updating control file</span><br><span class="line">pg_rewind: syncing target data directory</span><br><span class="line">pg_rewind: Done!</span><br></pre></td></tr></table></figure><p>After pg_rewind is finished, we will have to edit once more the configuration of the <code>old master</code> because the tool copies most of the configuration settings from the <code>new master</code> to the <code>old master</code> as a synchronization process. Let’s examine both <code>db-master/postgresql.auto.conf</code> and <code>db-master/postgresql.conf</code> and make sure of the followings again.</p><figure class="highlight bash"><figcaption><span>db-master/postgresql.conf</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">############# db-master/postgresql.conf #############</span></span><br><span class="line">primary_slot_name = <span class="string">'main'</span></span><br><span class="line">recovery_target_timeline = <span class="string">'latest'</span></span><br><span class="line">port = 5432</span><br><span class="line"></span><br><span class="line"><span class="comment">############# db-master/postgresql.auto.conf #############</span></span><br><span class="line">primary_conninfo = <span class="string">'user=cary passfile='</span><span class="string">'/home/caryh/.pgpass'</span><span class="string">' host=127.0.0.1 port=5433 sslmode=disable sslcompression=0 gssencmode=disable target_session_attrs=any'</span></span><br></pre></td></tr></table></figure><p>and also, don’t forget about this:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch db-master/standby.signal</span><br></pre></td></tr></table></figure><p>Now, we should be ready to start the <code>old master</code> again.</p><h3 id="7-Start-the-Old-Master-Agian-as-Standby-Server"><a href="#7-Start-the-Old-Master-Agian-as-Standby-Server" class="headerlink" title="7. Start the Old Master Agian as Standby Server"></a>7. Start the Old Master Agian as Standby Server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pg_ctl -D db-master start</span><br><span class="line"></span><br><span class="line">2019-10-30 12:27:28.140 PDT [5095] LOG:  restored <span class="built_in">log</span> file <span class="string">"000000010000000000000002"</span> from archive</span><br><span class="line">2019-10-30 12:27:28.167 PDT [5095] LOG:  redo starts at 0/2000028</span><br><span class="line">2019-10-30 12:27:28.182 PDT [5095] LOG:  consistent recovery state reached at 0/3027258</span><br><span class="line">2019-10-30 12:27:28.183 PDT [5095] LOG:  invalid record length at 0/3027258: wanted 24, got 0</span><br><span class="line">2019-10-30 12:27:28.183 PDT [5095] LOG:  redo <span class="keyword">done</span> at 0/3027230</span><br><span class="line">2019-10-30 12:27:28.183 PDT [5095] LOG:  last completed transaction was at <span class="built_in">log</span> time 2019-10-30 12:20:34.056723-07</span><br><span class="line">019-10-30 12:27:28.226 PDT [5094] LOG:  database system is ready to accept connections</span><br></pre></td></tr></table></figure><p>The <code>old master</code> can now start as a streaming replication to the <code>new master</code> and we can observe that after using pg_rewind the additional data that was inserted to <code>old master</code> in step number 3 is now removed, as it has been <code>rewound</code> from 300 entries to 200 entries to match up with the <code>new master</code>.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## new master ##</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5433</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   200</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="comment">## old master ##</span></span><br><span class="line">$ psql -d clusterdb -U cary -c <span class="string">"SELECT count(*) from test_table"</span> -p 5432</span><br><span class="line"> count </span><br><span class="line">-------</span><br><span class="line">   200</span><br><span class="line">(1 row)</span><br></pre></td></tr></table></figure><h3 id="8-Summary"><a href="#8-Summary" class="headerlink" title="8. Summary"></a>8. Summary</h3><p>In this blog, we have simulated a failover case and observe the effect of promoting a standby <code>slave</code> server while more data insertions happening to the original <code>master</code> server. We have demonstrated how to use <code>pg_rewind</code> tool to synchronize both <code>master</code> and <code>slave</code> after the <code>slave</code> promotion. Though it results some data deletion at the original <code>master</code>, in the end, we are able to resolve the timeline conflict with <code>pg_rewind</code> and complete the database failover scenario.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h3&gt;&lt;p&gt;In the previous blog, we have discussed how to correctly set up streaming replication clusters between one master and one slave in Postgres version 12. In this blog, we will simulate a failover scenario on the master database, which causes the replica (or slave) database cluster to be promoted as new master and continue the operation. We will also simulate a failback scenario to reuse the old master cluster after the failover scenario with the help of pg_rewind. &lt;/p&gt;
&lt;p&gt;Normally it is quite easy to do a failback to the old master after slave gets promoted to master but if there is data written to the old master after slave promotion, we will have an out-of-sync case between them both and we will have to use the pg_rewind tool to synchronize the two data directories in order to bring the old master to match the state of the new master. Please note that the pg_rewind tool will remove transactions from the old master in order to match up with the new, so certain pre-caution is needed to use this tool.&lt;/p&gt;
&lt;p&gt;Here’s a brief overview of list of actions we are going to perform:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simulate failover by promoting &lt;code&gt;slave&lt;/code&gt; cluster, so it becomes a &lt;code&gt;new master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;simulate data insertion to &lt;code&gt;master&lt;/code&gt; cluster, also referred as &lt;code&gt;old master&lt;/code&gt; after promotion&lt;/li&gt;
&lt;li&gt;shutdown the &lt;code&gt;old master&lt;/code&gt; cluster and set it up as a standby server&lt;/li&gt;
&lt;li&gt;run pg_rewind on &lt;code&gt;old master&lt;/code&gt; to synchronize transaction states with &lt;code&gt;new master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;bring up &lt;code&gt;old master&lt;/code&gt; as a standby server to synchronize with the &lt;code&gt;new master&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blog assumes you already have streaming replication setup between one master and one slave from previous blog. If you have not checked out the previous blog titled “Streaming Replication Setup in PG12 - How to Do it Right”, it is recommended to give that a read first.&lt;/p&gt;
&lt;p&gt;The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="replication" scheme="http://caryhuang.github.io/tags/replication/"/>
    
    <category term="pg_rewind" scheme="http://caryhuang.github.io/tags/pg-rewind/"/>
    
    <category term="failover" scheme="http://caryhuang.github.io/tags/failover/"/>
    
  </entry>
  
  <entry>
    <title>Trace-Postgres-query-processing-internals-with-debugger</title>
    <link href="http://caryhuang.github.io/2019/09/27/Trace-Postgres-query-processing-internals-with-debugger/"/>
    <id>http://caryhuang.github.io/2019/09/27/Trace-Postgres-query-processing-internals-with-debugger/</id>
    <published>2019-09-28T06:36:24.000Z</published>
    <updated>2020-01-13T23:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>In this article we will use GDB debugger to trace the internals of Postgres and observe how an input query passes through several levels of transformation (Parser -&gt; Analyzer -&gt; Rewriter -&gt; Planner -&gt; Executor) and eventually produces an output. </p><p>This article is based on PG12 running on Ubuntu 18.04,  and we will use a simple <code>SELECT</code> query with <code>ORDER BY</code> , <code>GROUP BY</code>, and <code>LIMIT</code> keywords to go through the entire query processing tracing.</p><a id="more"></a><h3 id="2-Preparation"><a href="#2-Preparation" class="headerlink" title="2. Preparation"></a>2. Preparation</h3><p>GDB debugger is required to be installed to trace the internals of Postgres. Most recent distribution of Linux already comes with gdb pre-installed. If you do not have it, please install.</p><h4 id="2-1-Enable-Debugging-and-Disable-Compiler-Optimization-on-PG-Build"><a href="#2-1-Enable-Debugging-and-Disable-Compiler-Optimization-on-PG-Build" class="headerlink" title="2.1 Enable Debugging and Disable Compiler Optimization on PG Build"></a>2.1 Enable Debugging and Disable Compiler Optimization on PG Build</h4><p>For GDB to be useful, the postgres binaries have to be compiled with debugging symbols enabled (-g). In addition, I would suggest to turn off compiler optimization (-O0) such that while tracing we will be able to examine all memory blocks and values, and observe the execution flow properly.</p><p>Enable debugging using the ./configure utility in the Postgres source code repository</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> [PG_SOURCE_DIR]</span><br><span class="line">./configure --<span class="built_in">enable</span>-debug</span><br></pre></td></tr></table></figure><p>This will add the (-g) parameter to the CFLAGS in the main Makefile to include debugging symbols.</p><p>Once finished, let’s disable compiler optimization by editing </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">src/Makefile.global</span><br></pre></td></tr></table></figure><p>Find the line where CFLAGS is defined and changed -O2 to -O0 like this:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CFLAGS &#x3D; -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror&#x3D;vla -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision&#x3D;standard -Wno-format-truncation -g -O0</span><br></pre></td></tr></table></figure><p>Then we need to build and install with the new Makefile</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><h4 id="2-2-Initialize-Postgres-Server"><a href="#2-2-Initialize-Postgres-Server" class="headerlink" title="2.2 Initialize Postgres Server"></a>2.2 Initialize Postgres Server</h4><p>For a new build, we will need to initialize a new database</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">initDb /home/caryh/postgresql</span><br><span class="line">create user caryh</span><br><span class="line">createdb carytest</span><br></pre></td></tr></table></figure><p>For referencing purposes, I would suggest enable debug log for the Postgres server by modifying postgres.conf located in database home directory. In this case it is located in </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/home/caryh/postgresql/postgres.conf</span><br></pre></td></tr></table></figure><p>Enable the following lines in postgres.conf</p><figure class="highlight bash"><figcaption><span>postgres.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">log_destination = <span class="string">'syslog'</span></span><br><span class="line">syslog_facility = <span class="string">'LOCAL0'</span></span><br><span class="line">syslog_ident = <span class="string">'postgres'</span></span><br><span class="line">syslog_sequence_numbers = on</span><br><span class="line">syslog_split_messages = on</span><br><span class="line"></span><br><span class="line">debug_print_parse = on</span><br><span class="line">debug_print_rewritten = on</span><br><span class="line">debug_print_plan = on</span><br><span class="line">debug_pretty_print = on</span><br><span class="line">log_checkpoints = on</span><br><span class="line">log_connections = on</span><br><span class="line">log_disconnections = on</span><br><span class="line">log_duration = on</span><br></pre></td></tr></table></figure><p>Why do we enable debug log when we will be tracing postgres with gdb? This is because the output at some of the stages of query processing is represented as a complex list of structures and it is not very straightforward to print this structure unless we have written a third party print script that can help us recursively print the content of the complex structure. Postgres already has this function built-in and presented in the form of a debugging log. </p><h4 id="2-3-Start-Postgres-Server-and-Connect-with-Client-Tool"><a href="#2-3-Start-Postgres-Server-and-Connect-with-Client-Tool" class="headerlink" title="2.3 Start Postgres Server and Connect with Client Tool"></a>2.3 Start Postgres Server and Connect with Client Tool</h4><p>Start the PG database</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pg_ctl -D /home/caryh/postgresql start</span><br></pre></td></tr></table></figure><p>Connect to PG database as user</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">psql -d carytest -U cary</span><br></pre></td></tr></table></figure><h4 id="2-4-Populate-Example-Tables-and-Values"><a href="#2-4-Populate-Example-Tables-and-Values" class="headerlink" title="2.4 Populate Example Tables and Values"></a>2.4 Populate Example Tables and Values</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> deviceinfo (</span><br><span class="line">  serial_number <span class="built_in">varchar</span>(<span class="number">45</span>) PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  manufacturer <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">  device_type <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">password</span> <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">  registration_time <span class="built_in">timestamp</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> devicedata (</span><br><span class="line">  serial_number <span class="built_in">varchar</span>(<span class="number">45</span>) <span class="keyword">REFERENCES</span> deviceinfo(serial_number),</span><br><span class="line">  record_time <span class="built_in">timestamp</span>,</span><br><span class="line">  uptime <span class="built_in">int</span>,</span><br><span class="line">  temperature <span class="built_in">numeric</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  voltage <span class="built_in">numeric</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  <span class="keyword">power</span> <span class="built_in">numeric</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  firmware_version <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">  configuration_file <span class="built_in">varchar</span>(<span class="number">45</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> deviceinfo <span class="keyword">VALUES</span>( <span class="string">'X00001'</span>, <span class="string">'Highgo Inc'</span>, <span class="number">1</span>, <span class="string">'password'</span>, <span class="string">'2019-09-18 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> deviceinfo <span class="keyword">VALUES</span>( <span class="string">'X00002'</span>, <span class="string">'Highgo Inc'</span>, <span class="number">2</span>, <span class="string">'password'</span>, <span class="string">'2019-09-18 17:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> deviceinfo <span class="keyword">VALUES</span>( <span class="string">'X00003'</span>, <span class="string">'Highgo Inc'</span>, <span class="number">1</span>, <span class="string">'password'</span>, <span class="string">'2019-09-18 18:00:00'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00001'</span>, <span class="string">'2019-09-20 16:00:00'</span>, <span class="number">2000</span>, <span class="number">38.23</span>, <span class="number">189.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00001'</span>, <span class="string">'2019-09-20 17:00:00'</span>, <span class="number">3000</span>, <span class="number">68.23</span>, <span class="number">221.00</span>, <span class="number">675.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00001'</span>, <span class="string">'2019-09-20 18:00:00'</span>, <span class="number">4000</span>, <span class="number">70.23</span>, <span class="number">220.00</span>, <span class="number">333.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00001'</span>, <span class="string">'2019-09-20 19:00:00'</span>, <span class="number">5000</span>, <span class="number">124.23</span>, <span class="number">88.00</span>, <span class="number">678.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 11:00:00'</span>, <span class="number">8000</span>, <span class="number">234.23</span>, <span class="number">567.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 12:00:00'</span>, <span class="number">9000</span>, <span class="number">56.23</span>, <span class="number">234.00</span>, <span class="number">345.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 13:00:00'</span>, <span class="number">3000</span>, <span class="number">12.23</span>, <span class="number">56.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 14:00:00'</span>, <span class="number">4000</span>, <span class="number">56.23</span>, <span class="number">77.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 11:00:00'</span>, <span class="number">8000</span>, <span class="number">234.23</span>, <span class="number">567.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 12:00:00'</span>, <span class="number">9000</span>, <span class="number">56.23</span>, <span class="number">234.00</span>, <span class="number">345.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 13:00:00'</span>, <span class="number">3000</span>, <span class="number">12.23</span>, <span class="number">56.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00002'</span>, <span class="string">'2019-09-20 14:00:00'</span>, <span class="number">4000</span>, <span class="number">56.23</span>, <span class="number">77.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 07:00:00'</span>, <span class="number">25000</span>, <span class="number">68.23</span>, <span class="number">99.00</span>, <span class="number">43.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 08:00:00'</span>, <span class="number">20600</span>, <span class="number">178.23</span>, <span class="number">333.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 09:00:00'</span>, <span class="number">20070</span>, <span class="number">5.23</span>, <span class="number">33.00</span>, <span class="number">123.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 10:00:00'</span>, <span class="number">200043</span>, <span class="number">45.23</span>, <span class="number">45.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 09:00:00'</span>, <span class="number">20070</span>, <span class="number">5.23</span>, <span class="number">33.00</span>, <span class="number">123.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> devicedata <span class="keyword">VALUES</span> (<span class="string">'X00003'</span>, <span class="string">'2019-09-20 10:00:00'</span>, <span class="number">200043</span>, <span class="number">45.23</span>, <span class="number">45.00</span>, <span class="number">456.1</span>, <span class="string">'version01'</span>, <span class="string">'config01'</span>);</span><br></pre></td></tr></table></figure><h3 id="3-Start-gdb-Debugger"><a href="#3-Start-gdb-Debugger" class="headerlink" title="3. Start gdb Debugger"></a>3. Start gdb Debugger</h3><p>Find the PID of the connecting client PG session</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ps -ef | grep postgres</span><br><span class="line">caryh     7072  1946  0 Sep26 ?        00:00:01 /usr/<span class="built_in">local</span>/pgsql/bin/postgres -D /home/caryh/postgresql</span><br><span class="line">caryh     7074  7072  0 Sep26 ?        00:00:00 postgres: checkpointer   </span><br><span class="line">caryh     7075  7072  0 Sep26 ?        00:00:01 postgres: background writer   </span><br><span class="line">caryh     7076  7072  0 Sep26 ?        00:00:01 postgres: walwriter   </span><br><span class="line">caryh     7077  7072  0 Sep26 ?        00:00:01 postgres: autovacuum launcher   </span><br><span class="line">caryh     7078  7072  0 Sep26 ?        00:00:03 postgres: stats collector   </span><br><span class="line">caryh     7079  7072  0 Sep26 ?        00:00:00 postgres: logical replication launcher   </span><br><span class="line">caryh     7082  7072  0 Sep26 ?        00:00:00 postgres: cary carytest [<span class="built_in">local</span>] idle</span><br></pre></td></tr></table></figure><p>In this case it is the last line of the ps output as both my client and server reside in the same machine. Yours may be different.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">caryh     7082  7072  0 Sep26 ?        00:00:00 postgres: cary carytest [<span class="built_in">local</span>] idle</span><br></pre></td></tr></table></figure><p>Now we can run gdb with the postgres binary </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gdb /usr/<span class="built_in">local</span>/pgsql/bin/postgres</span><br><span class="line">GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-git</span><br><span class="line">Copyright (C) 2018 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.  Type <span class="string">"show copying"</span></span><br><span class="line">and <span class="string">"show warranty"</span> <span class="keyword">for</span> details.</span><br><span class="line">This GDB was configured as <span class="string">"x86_64-linux-gnu"</span>.</span><br><span class="line">Type <span class="string">"show configuration"</span> <span class="keyword">for</span> configuration details.</span><br><span class="line">For bug reporting instructions, please see:</span><br><span class="line">&lt;http://www.gnu.org/software/gdb/bugs/&gt;.</span><br><span class="line">Find the GDB manual and other documentation resources online at:</span><br><span class="line">&lt;http://www.gnu.org/software/gdb/documentation/&gt;.</span><br><span class="line">For <span class="built_in">help</span>, <span class="built_in">type</span> <span class="string">"help"</span>.</span><br><span class="line">Type <span class="string">"apropos word"</span> to search <span class="keyword">for</span> commands related to <span class="string">"word"</span>...</span><br><span class="line">Reading symbols from /usr/<span class="built_in">local</span>/pgsql/bin/postgres...done.</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Now, we can attach gdb to the PID identified in previous step</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) attach 7082</span><br><span class="line">Attaching to program: &#x2F;usr&#x2F;local&#x2F;pgsql&#x2F;bin&#x2F;postgres, process 7082</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libpthread.so.0...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;.build-id&#x2F;28&#x2F;c6aade70b2d40d1f0f3d0a1a0cad1ab816448f.debug...done.</span><br><span class="line">done.</span><br><span class="line">[Thread debugging using libthread_db enabled]</span><br><span class="line">Using host libthread_db library &quot;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libthread_db.so.1&quot;.</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;librt.so.1...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;librt-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libdl.so.2...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libdl-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libm.so.6...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libm-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libc.so.6...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libc-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">Reading symbols from &#x2F;lib64&#x2F;ld-linux-x86-64.so.2...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;ld-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">Reading symbols from &#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnss_files.so.2...Reading symbols from &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libnss_files-2.27.so...done.</span><br><span class="line">done.</span><br><span class="line">0x00007fce71eafb77 in epoll_wait (epfd&#x3D;4, events&#x3D;0x5633194c87e0, maxevents&#x3D;1, timeout&#x3D;-1)</span><br><span class="line">    at ..&#x2F;sysdeps&#x2F;unix&#x2F;sysv&#x2F;linux&#x2F;epoll_wait.c:30</span><br><span class="line">30  ..&#x2F;sysdeps&#x2F;unix&#x2F;sysv&#x2F;linux&#x2F;epoll_wait.c: No such file or directory.</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Upon attach, Postgres process will be put on a break and we are able to issue breakpoints command from here</p><h3 id="4-Start-Tracing-with-gdb"><a href="#4-Start-Tracing-with-gdb" class="headerlink" title="4. Start Tracing with gdb"></a>4. Start Tracing with gdb</h3><p><code>exec_simple_query</code> is the function that will trigger all stages of query processing. Let’s put a breakpoint here.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b exec_simple_query</span><br><span class="line">Breakpoint 1 at 0x56331899a43b: file postgres.c, line 985.</span><br><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br></pre></td></tr></table></figure><p>Now, let’s type in a SELECT query with ORDER BY keywords on the postgres client connection terminal to trigger break point</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;</span><br></pre></td></tr></table></figure><p>Breakpoint should be triggered</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Breakpoint 1, exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Let’s do a backtrace <code>bt</code> command to see how the control got here.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) bt</span><br><span class="line">#0  exec_simple_query (query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985</span><br><span class="line">#1  0x000056331899f01c in PostgresMain (argc&#x3D;1, argv&#x3D;0x5633194c89c8, dbname&#x3D;0x5633194c8890 &quot;carytest&quot;, username&#x3D;0x5633194c8878 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#2  0x00005633188fba97 in BackendRun (port&#x3D;0x5633194c0f60) at postmaster.c:4431</span><br><span class="line">#3  0x00005633188fb1ba in BackendStartup (port&#x3D;0x5633194c0f60) at postmaster.c:4122</span><br><span class="line">#4  0x00005633188f753e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#5  0x00005633188f6cd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5633194974c0) at postmaster.c:1377</span><br><span class="line">#6  0x000056331881a10f in main (argc&#x3D;3, argv&#x3D;0x5633194974c0) at main.c:228</span><br></pre></td></tr></table></figure><p>As you can see, PostmasterMain process is one of the early process to be started and this is where it will spawn all the backend processes and initialize the ‘ServerLoop’ to listen for client connections. When a client connets and issues some queries, the handle will be passed from the backend to <code>PostgresMain</code> and this is where the query processing will begine.</p><h3 id="5-Parser-Stage"><a href="#5-Parser-Stage" class="headerlink" title="5. Parser Stage"></a>5. Parser Stage</h3><p>Parser Stage is the first stage in query processing, which will take an input query string and produce a raw un-analyzed parse tree. The control will eventually come to the raw_parser function, so let’s set a break point there and do a backtrace:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb)  b raw_parser</span><br><span class="line">Breakpoint 2 at 0x5633186b5bae: file parser.c, line 37.</span><br><span class="line">(gdb)  c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 2, raw_parser (str&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37</span><br><span class="line"></span><br><span class="line">(gdb) bt</span><br><span class="line">#0  raw_parser (str&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37</span><br><span class="line">#1  0x000056331899a03e in pg_parse_query ( query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:641</span><br><span class="line">#2  0x000056331899a4c9 in exec_simple_query ( query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1037</span><br><span class="line">#3  0x000056331899f01c in PostgresMain (argc&#x3D;1, argv&#x3D;0x5633194c89c8, dbname&#x3D;0x5633194c8890 &quot;carytest&quot;,  username&#x3D;0x5633194c8878 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#4  0x00005633188fba97 in BackendRun (port&#x3D;0x5633194c0f60) at postmaster.c:4431</span><br><span class="line">#5  0x00005633188fb1ba in BackendStartup (port&#x3D;0x5633194c0f60) at postmaster.c:4122</span><br><span class="line">#6  0x00005633188f753e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#7  0x00005633188f6cd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5633194974c0) at postmaster.c:1377</span><br><span class="line">#8  0x000056331881a10f in main (argc&#x3D;3, argv&#x3D;0x5633194974c0) at main.c:228</span><br></pre></td></tr></table></figure><p>In <code>raw_parser</code>, 2 things will happen, first to scan the query with flex-based scanner to check keyword validity and second to do the actual parsing with bison-based parser. In the end, it will return a parse tree for next stage.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) n</span><br><span class="line">43    yyscanner &#x3D; scanner_init(str, &amp;yyextra.core_yy_extra,</span><br><span class="line">(gdb) n</span><br><span class="line">47    yyextra.have_lookahead &#x3D; false;</span><br><span class="line">(gdb) n</span><br><span class="line">50    parser_init(&amp;yyextra);</span><br><span class="line">(gdb) </span><br><span class="line">53    yyresult &#x3D; base_yyparse(yyscanner);</span><br><span class="line">(gdb) </span><br><span class="line">56    scanner_finish(yyscanner);</span><br><span class="line">(gdb) </span><br><span class="line">58    if (yyresult)       &#x2F;* error *&#x2F;</span><br><span class="line">(gdb) </span><br><span class="line">61    return yyextra.parsetree;</span><br></pre></td></tr></table></figure><p>It is not very straight-forward to examine the content of the parse tree stored in <code>yyextra.parsetree</code> as above. This is why we enabled postgres debug log so that we can utilize it to recursively print the content of the parse tree. The parse tree illustrated by <code>yyextra.parsetree</code> can be visualized as this image below:</p><p><img src="/images/parse_tree.png" alt="extension"></p><h3 id="6-0-Analyzer-Stage"><a href="#6-0-Analyzer-Stage" class="headerlink" title="6.0 Analyzer Stage"></a>6.0 Analyzer Stage</h3><p>Now we have a list of parse trees, size 1 in this example, PG will need to feed each item in the list into anaylzer and rewriter functions. Let’s set a break point at <code>parse_analyze</code> function</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb)  b parse_analyze</span><br><span class="line">Breakpoint 3 at 0x56331867d608: file analyze.c, line 104.</span><br><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 3, parse_analyze (parseTree&#x3D;0x56331949dd50,  sourceText&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes&#x3D;0x0, numParams&#x3D;0, queryEnv&#x3D;0x0) at analyze.c:104</span><br><span class="line">104   ParseState *pstate &#x3D; make_parsestate(NULL);</span><br><span class="line">(gdb) bt</span><br><span class="line"></span><br><span class="line">#0  parse_analyze (parseTree&#x3D;0x56331949dd50, sourceText&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes&#x3D;0x0, numParams&#x3D;0, queryEnv&#x3D;0x0) at analyze.c:104</span><br><span class="line">#1  0x000056331899a0a8 in pg_analyze_and_rewrite (parsetree&#x3D;0x56331949dd50, </span><br><span class="line">    query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, </span><br><span class="line">    paramTypes&#x3D;0x0, numParams&#x3D;0, queryEnv&#x3D;0x0) at postgres.c:695</span><br><span class="line">#2  0x000056331899a702 in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;)</span><br><span class="line">    at postgres.c:1140</span><br><span class="line">#3  0x000056331899f01c in PostgresMain (argc&#x3D;1, argv&#x3D;0x5633194c89c8, </span><br><span class="line">    dbname&#x3D;0x5633194c8890 &quot;carytest&quot;, username&#x3D;0x5633194c8878 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#4  0x00005633188fba97 in BackendRun (port&#x3D;0x5633194c0f60) at postmaster.c:4431</span><br><span class="line">#5  0x00005633188fb1ba in BackendStartup (port&#x3D;0x5633194c0f60) at postmaster.c:4122</span><br><span class="line">#6  0x00005633188f753e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#7  0x00005633188f6cd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5633194974c0) at postmaster.c:1377</span><br><span class="line">#8  0x000056331881a10f in main (argc&#x3D;3, argv&#x3D;0x5633194974c0) at main.c:228</span><br></pre></td></tr></table></figure><p>The above backtrace shows how the control gets to parse_analyze function, and 2 vital imputs are <code>parseTree</code> (type RawStmt) and (const char) <code>sourceText</code></p><p>Let’s traverse to the end of parse_analyze</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) n</span><br><span class="line">109   pstate-&gt;p_sourcetext &#x3D; sourceText;</span><br><span class="line">(gdb) </span><br><span class="line">111   if (numParams &gt; 0)</span><br><span class="line">(gdb) </span><br><span class="line">114   pstate-&gt;p_queryEnv &#x3D; queryEnv;</span><br><span class="line">(gdb) </span><br><span class="line">116   query &#x3D; transformTopLevelStmt(pstate, parseTree);</span><br><span class="line">(gdb) </span><br><span class="line">118   if (post_parse_analyze_hook)</span><br><span class="line">(gdb) </span><br><span class="line">121   free_parsestate(pstate);</span><br><span class="line">(gdb) </span><br><span class="line">123   return query;</span><br></pre></td></tr></table></figure><p>At analyzer stage, it produces a result of type <code>Query</code> and it is in fact the data type return from the parser stage as a <code>List</code> of <code>Query</code>. This structure will be fed into the rewriter stage.</p><h3 id="7-0-Rewriter-Stage"><a href="#7-0-Rewriter-Stage" class="headerlink" title="7.0 Rewriter Stage"></a>7.0 Rewriter Stage</h3><p>Rewriter is the next stage following analyzer, let’s create a break point at <code>pg_rewrite_query</code> and do a backtrace:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b pg_rewrite_query</span><br><span class="line">Breakpoint 4 at 0x56331899a1c1: file postgres.c, line 773</span><br><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 4, pg_rewrite_query (query&#x3D;0x56331949dee0) at postgres.c:773</span><br><span class="line">773   if (Debug_print_parse)</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  pg_rewrite_query (query&#x3D;0x56331949dee0) at postgres.c:773</span><br><span class="line">#1  0x000056331899a0cf in pg_analyze_and_rewrite (parsetree&#x3D;0x56331949dd50, </span><br><span class="line">    query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, </span><br><span class="line">    paramTypes&#x3D;0x0, numParams&#x3D;0, queryEnv&#x3D;0x0) at postgres.c:704</span><br><span class="line">#2  0x000056331899a702 in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;)</span><br><span class="line">    at postgres.c:1140</span><br><span class="line">#3  0x000056331899f01c in PostgresMain (argc&#x3D;1, argv&#x3D;0x5633194c89c8, </span><br><span class="line">    dbname&#x3D;0x5633194c8890 &quot;carytest&quot;, username&#x3D;0x5633194c8878 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#4  0x00005633188fba97 in BackendRun (port&#x3D;0x5633194c0f60) at postmaster.c:4431</span><br><span class="line">#5  0x00005633188fb1ba in BackendStartup (port&#x3D;0x5633194c0f60) at postmaster.c:4122</span><br><span class="line">#6  0x00005633188f753e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#7  0x00005633188f6cd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5633194974c0) at postmaster.c:1377</span><br><span class="line">#8  0x000056331881a10f in main (argc&#x3D;3, argv&#x3D;0x5633194974c0) at main.c:228</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Rewriter takes the output of the previou stage and returns a querytree_list of type <code>List*</code>. Let’s trace the function to the end and print the output</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">773     if (Debug_print_parse)</span><br><span class="line">(gdb) n</span><br><span class="line">774     elog_node_display(LOG, &quot;parse tree&quot;, query,</span><br><span class="line">(gdb) </span><br><span class="line">777   if (log_parser_stats)</span><br><span class="line">(gdb) </span><br><span class="line">780   if (query-&gt;commandType &#x3D;&#x3D; CMD_UTILITY)</span><br><span class="line">(gdb) </span><br><span class="line">788     querytree_list &#x3D; QueryRewrite(query);</span><br><span class="line">(gdb) </span><br><span class="line">791   if (log_parser_stats)</span><br><span class="line">(gdb) </span><br><span class="line">848   if (Debug_print_rewritten)</span><br><span class="line">(gdb) </span><br><span class="line">849     elog_node_display(LOG, &quot;rewritten parse tree&quot;, querytree_list,</span><br><span class="line">(gdb) </span><br><span class="line">852   return querytree_list;</span><br></pre></td></tr></table></figure><p>the <code>line 774 elog_node_display</code> and <code>line 849 elog_node_display</code> are the debug print function provided by postgres to recursively print the content of <code>Query</code> before and after rewriter stage. After examining the output query tree, we found that in this example, the rewriter does not make much modification to the origianl query tree and it can be visualized as:</p><p><img src="/images/parse_tree.png" alt="extension"></p><h3 id="8-0-Planner-Stage"><a href="#8-0-Planner-Stage" class="headerlink" title="8.0 Planner Stage"></a>8.0 Planner Stage</h3><p>Planner is the next stage immediately following the previous. The main planner function entry point is <code>pg_plan_query</code> and it takes the output from previous stage as input. Let’s create a breakpoint and do a backtrace again</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b pg_plan_queries</span><br><span class="line">Breakpoint 5 at 0x56331899a32d: file postgres.c, line 948.</span><br><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 5, pg_plan_queries (querytrees&#x3D;0x563319558558, cursorOptions&#x3D;256, boundParams&#x3D;0x0)</span><br><span class="line">    at postgres.c:948</span><br><span class="line">948   List     *stmt_list &#x3D; NIL;</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  pg_plan_queries (querytrees&#x3D;0x563319558558, cursorOptions&#x3D;256, boundParams&#x3D;0x0) at postgres.c:948</span><br><span class="line">#1  0x000056331899a722 in exec_simple_query ( query_string&#x3D;0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1143</span><br><span class="line">#2  0x000056331899f01c in PostgresMain (argc&#x3D;1, argv&#x3D;0x5633194c89c8, dbname&#x3D;0x5633194c8890 &quot;carytest&quot;, username&#x3D;0x5633194c8878 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#3  0x00005633188fba97 in BackendRun (port&#x3D;0x5633194c0f60) at postmaster.c:4431</span><br><span class="line">#4  0x00005633188fb1ba in BackendStartup (port&#x3D;0x5633194c0f60) at postmaster.c:4122</span><br><span class="line">#5  0x00005633188f753e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#6  0x00005633188f6cd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5633194974c0) at postmaster.c:1377</span><br><span class="line">#7  0x000056331881a10f in main (argc&#x3D;3, argv&#x3D;0x5633194974c0) at main.c:228</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Now, we are here, let’s trace the function until the end. Please note that for each content block in the input querytree list, the function will call a helper plan function called <code>pg_plan_query</code> and it will  perform the real plan operation there and return the result in <code>plannedStmt</code> data type</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) n</span><br><span class="line">951   foreach(query_list, querytrees)</span><br><span class="line">(gdb) n</span><br><span class="line">953     Query    *query &#x3D; lfirst_node(Query, query_list);</span><br><span class="line">(gdb) n</span><br><span class="line">956     if (query-&gt;commandType &#x3D;&#x3D; CMD_UTILITY)</span><br><span class="line">(gdb) n</span><br><span class="line">968       stmt &#x3D; pg_plan_query(query, cursorOptions, boundParams);</span><br><span class="line">(gdb) s</span><br><span class="line">pg_plan_query (querytree&#x3D;0x56331949dee0, cursorOptions&#x3D;256, boundParams&#x3D;0x0) at postgres.c:866</span><br><span class="line">866   if (querytree-&gt;commandType &#x3D;&#x3D; CMD_UTILITY)</span><br><span class="line">(gdb) n</span><br><span class="line">874   if (log_planner_stats)</span><br><span class="line">(gdb) </span><br><span class="line">878   plan &#x3D; planner(querytree, cursorOptions, boundParams);</span><br><span class="line">(gdb) n</span><br><span class="line">880   if (log_planner_stats)</span><br><span class="line">(gdb) </span><br><span class="line">929   if (Debug_print_plan)</span><br><span class="line">(gdb) </span><br><span class="line">930     elog_node_display(LOG, &quot;plan&quot;, plan, Debug_pretty_print);</span><br><span class="line">(gdb) </span><br><span class="line">934   return plan;</span><br></pre></td></tr></table></figure><p>Line <code>930 elog_node_display</code> will print the content of <code>PlannedStmt</code> recursively to syslog and it can be visualized as:</p><p><img src="/images/plan-tree2.png" alt="extension"></p><p>The above plan tree corresponds to the output of <code>EXPLAIN ANALYZE</code> on the same query.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">carytest=&gt; EXPLAIN ANALYZE SELECT serial_number, COUNT(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;</span><br><span class="line">                                                       QUERY PLAN                             </span><br><span class="line">                          </span><br><span class="line"><span class="comment">------------------------------------------------------------------------------------</span></span><br><span class="line"> Limit  (cost=1.32..1.33 rows=2 width=15) (actual time=0.043..0.044 rows=2 loops=1)</span><br><span class="line">   -&gt;  Sort  (cost=1.32..1.33 rows=3 width=15) (actual time=0.042..0.042 rows=2 loops=1)</span><br><span class="line">         Sort Key: (count(serial_number)) DESC</span><br><span class="line">         Sort Method: quicksort  Memory: 25kB</span><br><span class="line">         -&gt;  HashAggregate  (cost=1.27..1.30 rows=3 width=15) (actual time=0.033..0.035 rows=3</span><br><span class="line"> loops=1)</span><br><span class="line">               Group Key: serial_number</span><br><span class="line">               -&gt;  Seq Scan on devicedata  (cost=0.00..1.18 rows=18 width=7) (actual time=0.01</span><br><span class="line">3..0.016 rows=18 loops=1)</span><br><span class="line"> Planning Time: 28.541 ms</span><br><span class="line"> Execution Time: 0.097 ms</span><br><span class="line">(9 rows)</span><br></pre></td></tr></table></figure><p>Line <code>878 plan = planner(querytree, cursorOptions, boundParams);</code> in the above trace is the real planner logic and it is a complex stage. Inside this function, it will compute the initial cost and run time cost of all possible queries and in the end, it will choose a plan that is the least expensive. </p><p>with the <code>plannedStmt</code> produced, we are ready to enter the next stage of query processing.</p><h3 id="9-0-Executor-Stage"><a href="#9-0-Executor-Stage" class="headerlink" title="9.0 Executor Stage"></a>9.0 Executor Stage</h3><p>In addition to planner, executor is also one of the complex stages of query processing. This module is responsible for executing the query plan produced from previous stage and sending the query results back to the connecting client. </p><p>Executor is invoked and managed with a wrapper called <code>portal</code> and portal is an object representing the execution state of a query and providing memory management services but it does not actually run the executor. In the end, the portal will invoke one of the four executor routines as below</p><p>-ExecutorStart()<br>-ExecutorRun()<br>-ExecutorFinish()<br>-ExecutorEnd()</p><p>Before we can use the above routines, the portal needs to be initialized first. In the previous stage, the control is left at <code>exec_simple_query</code> at line 1147, let’s continue tracing from here to enter portal initialization</p><p>Let’s create a break point for each executor routine and do a back trace on each as we continue</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b ExecutorStart</span><br><span class="line">Breakpoint 6 at 0x5633187ad797: file execMain.c, line 146.</span><br><span class="line">(gdb) b ExecutorRun</span><br><span class="line">Breakpoint 7 at 0x5633187ada1e: file execMain.c, line 306.</span><br><span class="line">(gdb) b ExecutorFinish</span><br><span class="line">Breakpoint 8 at 0x5633187adc35: file execMain.c, line 405.</span><br><span class="line">(gdb) b ExecutorEnd</span><br><span class="line">Breakpoint 9 at 0x5633187add1e: file execMain.c, line 465.</span><br></pre></td></tr></table></figure><h4 id="9-1-Executor-Start"><a href="#9-1-Executor-Start" class="headerlink" title="9.1 Executor Start"></a>9.1 Executor Start</h4><p>The main purpose of ExecutorStart routine is to prepare the query plan, allocate storage and prepare rule manager. Let’s continue the tracing and do a backtrace.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Breakpoint 6, ExecutorStart (queryDesc&#x3D;0x5633195712e0, eflags&#x3D;0) at execMain.c:146</span><br><span class="line">146   if (ExecutorStart_hook)</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  ExecutorStart (queryDesc&#x3D;0x564977500190, eflags&#x3D;0) at execMain.c:146</span><br><span class="line">#1  0x0000564975eb87e0 in PortalStart (portal&#x3D;0x5649774a18d0, params&#x3D;0x0, eflags&#x3D;0, snapshot&#x3D;0x0)</span><br><span class="line">    at pquery.c:518</span><br><span class="line">#2  0x0000564975eb27b5 in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1176</span><br><span class="line">#3  0x0000564975eb701c in PostgresMain (argc&#x3D;1, argv&#x3D;0x564977465a08, </span><br><span class="line">    dbname&#x3D;0x5649774658d0 &quot;carytest&quot;, username&#x3D;0x5649774658b8 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#4  0x0000564975e13a97 in BackendRun (port&#x3D;0x56497745dfa0) at postmaster.c:4431</span><br><span class="line">#5  0x0000564975e131ba in BackendStartup (port&#x3D;0x56497745dfa0) at postmaster.c:4122</span><br><span class="line">#6  0x0000564975e0f53e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#7  0x0000564975e0ecd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5649774344c0) at postmaster.c:1377</span><br><span class="line">#8  0x0000564975d3210f in main (argc&#x3D;3, argv&#x3D;0x5649774344c0) at main.c:228</span><br><span class="line"></span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><h4 id="9-2-Executor-Run"><a href="#9-2-Executor-Run" class="headerlink" title="9.2 Executor Run"></a>9.2 Executor Run</h4><p>ExecutorRun is the main routine of executor module, and its main task is to execute the query plan, this routing will call the <code>ExecutePlan</code> function to actually execute the plan. In the end, before return, the result of query will be stored in <code>Estate</code> structure called <code>estate</code> and inside there is a count of how many tutples have been processed by the executor</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 7, ExecutorRun (queryDesc&#x3D;0x5633195712e0, direction&#x3D;ForwardScanDirection, count&#x3D;0, </span><br><span class="line">    execute_once&#x3D;true) at execMain.c:306</span><br><span class="line">306   if (ExecutorRun_hook)</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  ExecutorRun (queryDesc&#x3D;0x564977500190, direction&#x3D;ForwardScanDirection, count&#x3D;0, </span><br><span class="line">    execute_once&#x3D;true) at execMain.c:306</span><br><span class="line">#1  0x0000564975eb915c in PortalRunSelect (portal&#x3D;0x5649774a18d0, forward&#x3D;true, count&#x3D;0, </span><br><span class="line">    dest&#x3D;0x564977539460) at pquery.c:929</span><br><span class="line">#2  0x0000564975eb8db6 in PortalRun (portal&#x3D;0x5649774a18d0, count&#x3D;9223372036854775807, </span><br><span class="line">    isTopLevel&#x3D;true, run_once&#x3D;true, dest&#x3D;0x564977539460, altdest&#x3D;0x564977539460, </span><br><span class="line">    completionTag&#x3D;0x7ffff0b937d0 &quot;&quot;) at pquery.c:770</span><br><span class="line">#3  0x0000564975eb28ad in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1215</span><br><span class="line">#4  0x0000564975eb701c in PostgresMain (argc&#x3D;1, argv&#x3D;0x564977465a08, </span><br><span class="line">    dbname&#x3D;0x5649774658d0 &quot;carytest&quot;, username&#x3D;0x5649774658b8 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#5  0x0000564975e13a97 in BackendRun (port&#x3D;0x56497745dfa0) at postmaster.c:4431</span><br><span class="line">#6  0x0000564975e131ba in BackendStartup (port&#x3D;0x56497745dfa0) at postmaster.c:4122</span><br><span class="line">#7  0x0000564975e0f53e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#8  0x0000564975e0ecd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5649774344c0) at postmaster.c:1377</span><br><span class="line">#9  0x0000564975d3210f in main (argc&#x3D;3, argv&#x3D;0x5649774344c0) at main.c:228</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Continue tracing the <code>ExecutorRun</code> to the end.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">306   if (ExecutorRun_hook)</span><br><span class="line">(gdb) n</span><br><span class="line">309     standard_ExecutorRun(queryDesc, direction, count, execute_once);</span><br><span class="line">(gdb) s</span><br><span class="line">standard_ExecutorRun (queryDesc&#x3D;0x564977500190, direction&#x3D;ForwardScanDirection, count&#x3D;0, </span><br><span class="line">    execute_once&#x3D;true) at execMain.c:325</span><br><span class="line">325   estate &#x3D; queryDesc-&gt;estate;</span><br><span class="line">(gdb) n</span><br><span class="line">333   oldcontext &#x3D; MemoryContextSwitchTo(estate-&gt;es_query_cxt);</span><br><span class="line">(gdb) n</span><br><span class="line">336   if (queryDesc-&gt;totaltime)</span><br><span class="line">(gdb) n</span><br><span class="line">342   operation &#x3D; queryDesc-&gt;operation;</span><br><span class="line">(gdb) </span><br><span class="line">343   dest &#x3D; queryDesc-&gt;dest;</span><br><span class="line">(gdb) </span><br><span class="line">348   estate-&gt;es_processed &#x3D; 0;</span><br><span class="line">(gdb) </span><br><span class="line">350   sendTuples &#x3D; (operation &#x3D;&#x3D; CMD_SELECT ||</span><br><span class="line">(gdb) </span><br><span class="line">353   if (sendTuples)</span><br><span class="line">(gdb) </span><br><span class="line">354     dest-&gt;rStartup(dest, operation, queryDesc-&gt;tupDesc);</span><br><span class="line">(gdb) </span><br><span class="line">359   if (!ScanDirectionIsNoMovement(direction))</span><br><span class="line">(gdb) </span><br><span class="line">361     if (execute_once &amp;&amp; queryDesc-&gt;already_executed)</span><br><span class="line">(gdb) </span><br><span class="line">363     queryDesc-&gt;already_executed &#x3D; true;</span><br><span class="line">(gdb) </span><br><span class="line">365     ExecutePlan(estate,</span><br><span class="line">(gdb) </span><br><span class="line">367           queryDesc-&gt;plannedstmt-&gt;parallelModeNeeded,</span><br><span class="line">(gdb) </span><br><span class="line">365     ExecutePlan(estate,</span><br><span class="line">(gdb) </span><br><span class="line">379   if (sendTuples)</span><br><span class="line">(gdb) </span><br><span class="line">380     dest-&gt;rShutdown(dest);</span><br><span class="line">(gdb) </span><br><span class="line">382   if (queryDesc-&gt;totaltime)</span><br><span class="line">(gdb) </span><br><span class="line">385   MemoryContextSwitchTo(oldcontext);</span><br><span class="line">(gdb) p estate</span><br><span class="line">$6 &#x3D; (EState *) 0x56497751fbb0</span><br><span class="line">(gdb) p estate-&gt;es_processed</span><br><span class="line">$7 &#x3D; 2</span><br></pre></td></tr></table></figure><h4 id="9-3-Executor-Finish"><a href="#9-3-Executor-Finish" class="headerlink" title="9.3 Executor Finish"></a>9.3 Executor Finish</h4><p>ExecutorFinish must be called after the last ExecutorRun, its main task is to perform necessary clearn up actions and also fire up after Triggers. Let’s trace a little further.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 8, ExecutorFinish (queryDesc&#x3D;0x5633195712e0) at execMain.c:405</span><br><span class="line">405   if (ExecutorFinish_hook)</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  ExecutorFinish (queryDesc&#x3D;0x564977500190) at execMain.c:405</span><br><span class="line">#1  0x0000564975c5b52c in PortalCleanup (portal&#x3D;0x5649774a18d0) at portalcmds.c:300</span><br><span class="line">#2  0x0000564976071ba4 in PortalDrop (portal&#x3D;0x5649774a18d0, isTopCommit&#x3D;false) at portalmem.c:499</span><br><span class="line">#3  0x0000564975eb28d3 in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225</span><br><span class="line">#4  0x0000564975eb701c in PostgresMain (argc&#x3D;1, argv&#x3D;0x564977465a08, </span><br><span class="line">    dbname&#x3D;0x5649774658d0 &quot;carytest&quot;, username&#x3D;0x5649774658b8 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#5  0x0000564975e13a97 in BackendRun (port&#x3D;0x56497745dfa0) at postmaster.c:4431</span><br><span class="line">#6  0x0000564975e131ba in BackendStartup (port&#x3D;0x56497745dfa0) at postmaster.c:4122</span><br><span class="line">#7  0x0000564975e0f53e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#8  0x0000564975e0ecd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5649774344c0) at postmaster.c:1377</span><br><span class="line">#9  0x0000564975d3210f in main (argc&#x3D;3, argv&#x3D;0x5649774344c0) at main.c:228</span><br></pre></td></tr></table></figure><p>Continue tracing the <code>ExecutorFinish</code> to the end.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">405   if (ExecutorFinish_hook)</span><br><span class="line">(gdb) n</span><br><span class="line">408     standard_ExecutorFinish(queryDesc);</span><br><span class="line">(gdb) s</span><br><span class="line">standard_ExecutorFinish (queryDesc&#x3D;0x564977500190) at execMain.c:420</span><br><span class="line">420   estate &#x3D; queryDesc-&gt;estate;</span><br><span class="line">(gdb) n</span><br><span class="line">429   oldcontext &#x3D; MemoryContextSwitchTo(estate-&gt;es_query_cxt);</span><br><span class="line">(gdb) </span><br><span class="line">432   if (queryDesc-&gt;totaltime)</span><br><span class="line">(gdb) </span><br><span class="line">436   ExecPostprocessPlan(estate);</span><br><span class="line">(gdb) </span><br><span class="line">439   if (!(estate-&gt;es_top_eflags &amp; EXEC_FLAG_SKIP_TRIGGERS))</span><br><span class="line">(gdb) </span><br><span class="line">442   if (queryDesc-&gt;totaltime)</span><br><span class="line">(gdb) </span><br><span class="line">445   MemoryContextSwitchTo(oldcontext);</span><br><span class="line">(gdb) </span><br><span class="line">447   estate-&gt;es_finished &#x3D; true;</span><br><span class="line">(gdb) </span><br><span class="line">448 &#125;</span><br></pre></td></tr></table></figure><h4 id="9-4-Executor-End"><a href="#9-4-Executor-End" class="headerlink" title="9.4 Executor End"></a>9.4 Executor End</h4><p>This routing basically resets and releases some of the state variables in <code>QueryDesc</code> used during execution. ExecutorEnd is the last routine to be called and before entry, the  <code>PortalCleanup</code> and  <code>PortalDrop</code> are invoked first. So as we are in this routine the outer <code>Portal</code> object is also performing the cleanup process.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Breakpoint 9, ExecutorEnd (queryDesc&#x3D;0x5633195712e0) at execMain.c:465</span><br><span class="line">465   if (ExecutorEnd_hook)</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  ExecutorEnd (queryDesc&#x3D;0x564977500190) at execMain.c:465</span><br><span class="line">#1  0x0000564975c5b538 in PortalCleanup (portal&#x3D;0x5649774a18d0) at portalcmds.c:301</span><br><span class="line">#2  0x0000564976071ba4 in PortalDrop (portal&#x3D;0x5649774a18d0, isTopCommit&#x3D;false) at portalmem.c:499</span><br><span class="line">#3  0x0000564975eb28d3 in exec_simple_query (</span><br><span class="line">    query_string&#x3D;0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225</span><br><span class="line">#4  0x0000564975eb701c in PostgresMain (argc&#x3D;1, argv&#x3D;0x564977465a08, </span><br><span class="line">    dbname&#x3D;0x5649774658d0 &quot;carytest&quot;, username&#x3D;0x5649774658b8 &quot;cary&quot;) at postgres.c:4249</span><br><span class="line">#5  0x0000564975e13a97 in BackendRun (port&#x3D;0x56497745dfa0) at postmaster.c:4431</span><br><span class="line">#6  0x0000564975e131ba in BackendStartup (port&#x3D;0x56497745dfa0) at postmaster.c:4122</span><br><span class="line">#7  0x0000564975e0f53e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#8  0x0000564975e0ecd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5649774344c0) at postmaster.c:1377</span><br><span class="line">#9  0x0000564975d3210f in main (argc&#x3D;3, argv&#x3D;0x5649774344c0) at main.c:228</span><br><span class="line"></span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>Let’s Continue tracing <code>ExecutorEnd</code> to the end.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">465   if (ExecutorEnd_hook)</span><br><span class="line">(gdb) n</span><br><span class="line">468     standard_ExecutorEnd(queryDesc);</span><br><span class="line">(gdb) s</span><br><span class="line">standard_ExecutorEnd (queryDesc&#x3D;0x564977500190) at execMain.c:480</span><br><span class="line">480   estate &#x3D; queryDesc-&gt;estate;</span><br><span class="line">(gdb) n</span><br><span class="line">495   oldcontext &#x3D; MemoryContextSwitchTo(estate-&gt;es_query_cxt);</span><br><span class="line">(gdb) </span><br><span class="line">497   ExecEndPlan(queryDesc-&gt;planstate, estate);</span><br><span class="line">(gdb) </span><br><span class="line">500   UnregisterSnapshot(estate-&gt;es_snapshot);</span><br><span class="line">(gdb) </span><br><span class="line">501   UnregisterSnapshot(estate-&gt;es_crosscheck_snapshot);</span><br><span class="line">(gdb) </span><br><span class="line">506   MemoryContextSwitchTo(oldcontext);</span><br><span class="line">(gdb) </span><br><span class="line">512   FreeExecutorState(estate);</span><br><span class="line">(gdb) </span><br><span class="line">515   queryDesc-&gt;tupDesc &#x3D; NULL;</span><br><span class="line">(gdb) </span><br><span class="line">516   queryDesc-&gt;estate &#x3D; NULL;</span><br><span class="line">(gdb) </span><br><span class="line">517   queryDesc-&gt;planstate &#x3D; NULL;</span><br><span class="line">(gdb) </span><br><span class="line">518   queryDesc-&gt;totaltime &#x3D; NULL;</span><br><span class="line">(gdb) </span><br><span class="line">519 &#125;</span><br></pre></td></tr></table></figure><p>This routine marks the end of the query processing stages, the control will be passed back to <code>exec_simple_query</code> to finish the transaction and present result back to the client.</p><h3 id="10-0-Presenting-the-Result-Back-to-Client"><a href="#10-0-Presenting-the-Result-Back-to-Client" class="headerlink" title="10.0 Presenting the Result Back to Client"></a>10.0 Presenting the Result Back to Client</h3><p>With the transaction ended, the  <code>send_ready_for_query</code> flag will be set, and the control is now able to enter <code>ReadyForQuery</code> to present the result to client.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b ReadyForQuery</span><br><span class="line">Breakpoint 10 at 0x56331899811d: file dest.c, line 251.</span><br><span class="line">(gdb) c</span><br><span class="line">Continuing.</span><br><span class="line"></span><br><span class="line">Breakpoint 10, ReadyForQuery (dest&#x3D;DestRemote) at dest.c:251</span><br><span class="line">251 &#123;</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  ReadyForQuery (dest&#x3D;DestRemote) at dest.c:251</span><br><span class="line">#1  0x0000564975eb6eee in PostgresMain (argc&#x3D;1, argv&#x3D;0x564977465a08, </span><br><span class="line">    dbname&#x3D;0x5649774658d0 &quot;carytest&quot;, username&#x3D;0x5649774658b8 &quot;cary&quot;) at postgres.c:4176</span><br><span class="line">#2  0x0000564975e13a97 in BackendRun (port&#x3D;0x56497745dfa0) at postmaster.c:4431</span><br><span class="line">#3  0x0000564975e131ba in BackendStartup (port&#x3D;0x56497745dfa0) at postmaster.c:4122</span><br><span class="line">#4  0x0000564975e0f53e in ServerLoop () at postmaster.c:1704</span><br><span class="line">#5  0x0000564975e0ecd4 in PostmasterMain (argc&#x3D;3, argv&#x3D;0x5649774344c0) at postmaster.c:1377</span><br><span class="line">#6  0x0000564975d3210f in main (argc&#x3D;3, argv&#x3D;0x5649774344c0) at main.c:228</span><br><span class="line"></span><br><span class="line">(gdb) n</span><br><span class="line">252   switch (dest)</span><br><span class="line">(gdb) </span><br><span class="line">257       if (PG_PROTOCOL_MAJOR(FrontendProtocol) &gt;&#x3D; 3)</span><br><span class="line">(gdb) </span><br><span class="line">261         pq_beginmessage(&amp;buf, &#39;Z&#39;);</span><br><span class="line">(gdb) </span><br><span class="line">262         pq_sendbyte(&amp;buf, TransactionBlockStatusCode());</span><br><span class="line">(gdb) </span><br><span class="line">263         pq_endmessage(&amp;buf);</span><br><span class="line">(gdb) p dest</span><br><span class="line">$90 &#x3D; DestRemote</span><br><span class="line">(gdb)  n</span><br><span class="line">268       pq_flush();</span><br><span class="line">(gdb) </span><br><span class="line">269       break;</span><br><span class="line">(gdb) </span><br><span class="line">282 &#125;</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>as <code>pq_flush()</code> is called, the result of the query will be returned back to the client at remote destination.</p><h4 id="10-1-Client-Results"><a href="#10-1-Client-Results" class="headerlink" title="10.1 Client Results"></a>10.1 Client Results</h4><p>Client will now see the output below as a result of the query</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;</span><br><span class="line"> serial_number | count </span><br><span class="line"><span class="comment">---------------+-------</span></span><br><span class="line"> X00002        |     8</span><br><span class="line"> X00003        |     6</span><br><span class="line">(2 rows)</span><br></pre></td></tr></table></figure><h3 id="11-Summary"><a href="#11-Summary" class="headerlink" title="11 Summary"></a>11 Summary</h3><p>So far, we have traced through severl stags of query processing. Namely</p><ul><li>Parser</li><li>Analyzer</li><li>Rewritter</li><li>Planner</li><li>Executor</li></ul><p>To summarize all the above, I have created a simple call hierarchy ( or a list of breakpoints) below that outlines the important core functions that will be called while stepping through the above stages. The ‘b’ in front of each function name corresponds to the break point command of gdb.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Main Entry ##</span><br><span class="line">b exec_simple_query</span><br><span class="line"></span><br><span class="line">  ## Parser ##</span><br><span class="line">  b pg_parse_query            -&gt; returns (List* of Query)</span><br><span class="line">    b raw_parser              -&gt; returns (List* of Query)</span><br><span class="line">      b base_yyparse          -&gt; returns (List* of Query)</span><br><span class="line">  </span><br><span class="line">  ## Analzyer and Rewritter ##</span><br><span class="line">  b pg_analyze_and_rewrite    -&gt; returns (List*)</span><br><span class="line">    b parse_analyze           -&gt; returns (Query*)</span><br><span class="line">    b pg_rewrite_query        -&gt; returns (List* of Query)   </span><br><span class="line">      b QueryRewrite          -&gt; returns (List* of Query)   </span><br><span class="line"></span><br><span class="line">  ## Planner ##</span><br><span class="line">  b pg_plan_queries           -&gt; returns (List* of plannedStmt)</span><br><span class="line">    b pg_plan_query           -&gt; returns (PlannedStmt*)</span><br><span class="line">      b planner               -&gt; returns (PlannedStmt*)</span><br><span class="line"></span><br><span class="line">  ## Executor ##</span><br><span class="line">  b PortalStart               -&gt; returns void</span><br><span class="line">    b ExecutorStart           -&gt; returns void</span><br><span class="line"></span><br><span class="line">  b PortalRun                 -&gt; returns bool</span><br><span class="line">    b PortalRunSelect         -&gt; returns uint64 </span><br><span class="line">      b ExecutorRun           -&gt; returns void</span><br><span class="line"></span><br><span class="line">  b PortalDrop                -&gt; returns void</span><br><span class="line">    b PortalCleanup           -&gt; returns void</span><br><span class="line">      b ExecutorFinish        -&gt; returns void</span><br><span class="line">      b ExecutorEnd           -&gt; returns void</span><br><span class="line"></span><br><span class="line">  ## Present Result ##</span><br><span class="line">b ReadyForQuery               -&gt; returns void</span><br><span class="line">  b pq_flush                  -&gt; returns void</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h3&gt;&lt;p&gt;In this article we will use GDB debugger to trace the internals of Postgres and observe how an input query passes through several levels of transformation (Parser -&amp;gt; Analyzer -&amp;gt; Rewriter -&amp;gt; Planner -&amp;gt; Executor) and eventually produces an output. &lt;/p&gt;
&lt;p&gt;This article is based on PG12 running on Ubuntu 18.04,  and we will use a simple &lt;code&gt;SELECT&lt;/code&gt; query with &lt;code&gt;ORDER BY&lt;/code&gt; , &lt;code&gt;GROUP BY&lt;/code&gt;, and &lt;code&gt;LIMIT&lt;/code&gt; keywords to go through the entire query processing tracing.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="gdb" scheme="http://caryhuang.github.io/tags/gdb/"/>
    
    <category term="trace" scheme="http://caryhuang.github.io/tags/trace/"/>
    
    <category term="query processing" scheme="http://caryhuang.github.io/tags/query-processing/"/>
    
  </entry>
  
  <entry>
    <title>A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres</title>
    <link href="http://caryhuang.github.io/2019/09/25/A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres/"/>
    <id>http://caryhuang.github.io/2019/09/25/A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres/</id>
    <published>2019-09-26T06:36:03.000Z</published>
    <updated>2020-01-13T23:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Postgres is a huge database system consisting of a wide range of built-in data types, functions, features and operators that can be utilized to solve many common to complex problems. However, in the world full of complex problems, sometimes these are just not enough depending on the use case complexities.</p><p>Worry not, since Postgres version 9, it is possible to extend Postgres’s existing functionalities with the use of “extensions”</p><p>In this article, I will show you how to create your own extensions and add to Postgres.</p><p>Please note that this article is based on Postgres version 12 running on Ubuntu 18.04 and before you can create your own extensions, PG must have been built and installed first</p><a id="more"></a><h3 id="2-Built-in-Extensions"><a href="#2-Built-in-Extensions" class="headerlink" title="2. Built-in Extensions"></a>2. Built-in Extensions</h3><p>Before we jump into creating your own extensions, it is important to know that there is already a list of  extensions available from the PG community included in the Postgres software distribution. </p><p>The detailed information of community supplied extensions can be found in this link: <a href="https://www.postgresql.org/docs/9.1/contrib.html">https://www.postgresql.org/docs/9.1/contrib.html</a></p><h3 id="3-Build-and-Install-PG-Default-Extensions"><a href="#3-Build-and-Install-PG-Default-Extensions" class="headerlink" title="3. Build and Install PG Default Extensions"></a>3. Build and Install PG Default Extensions</h3><p>All the PG community extensions are located in the directory below. This is also where we will be adding our own extensions</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$PG_SOURCE_DIR</span>/postgres/contrib</span><br></pre></td></tr></table></figure><p>where [PG SOURCE DIR] is the directory to your PG source code</p><p>These modules are not built automatically unless you build the ‘world’ target. To Manually build and install them, use these commands.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> contrib</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>The above command will install the extensions to </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$SHAREDIR</span>/extension</span><br></pre></td></tr></table></figure><p>and required C shared libraries to </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$LIBDIR</span></span><br></pre></td></tr></table></figure><p>where $SHAREDIR and $LIBDIR are the values returned by pg_config</p><p>For the extensions that utilize the C language as implementation, there will be a C shared libraries (.so) being produced by the make command. This C shared library contains all the methods supported by the extension. </p><p>With default extensions and libraries installed, we can then see the installed extensions by the following queries</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> pg_available_extensions();</span><br><span class="line"><span class="keyword">SELECT</span> pg_available_extension_versions();</span><br></pre></td></tr></table></figure><h3 id="4-Create-Extension-Using-plpqsql-Language"><a href="#4-Create-Extension-Using-plpqsql-Language" class="headerlink" title="4. Create Extension Using plpqsql Language"></a>4. Create Extension Using plpqsql Language</h3><p>For this example, we will create a very simple extension that will count the number of specified character of a given string. This extension takes 2 input arguments, first being the string, and second being the desired character. It will return an integer indicating the number of occurance of the desired characters presented in the string</p><p>first, let’s navigate to the contrib directory to add our extension</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> [PG SOURCE DIR]/contrib</span><br></pre></td></tr></table></figure><p>let’s create a new directory called char_count. This will be the name of the extension</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir char_count</span><br><span class="line"><span class="built_in">cd</span> char_count</span><br></pre></td></tr></table></figure><p>create the folders for defining testcases later</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir sql</span><br><span class="line">mkdir expected</span><br></pre></td></tr></table></figure><p>create and an extension control file using this naming convention:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Extension name].control</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>char_count.control</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># char_count extension</span></span><br><span class="line">comment = <span class="string">'function to count number of specified characters'</span></span><br><span class="line">default_version = <span class="string">'1.0'</span></span><br><span class="line">module_pathname = <span class="string">'$libdir/char_count'</span></span><br><span class="line">relocatable = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>create a data sql file using this naming convention:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Extension name]--[Extension version].sql</span><br></pre></td></tr></table></figure><figure class="highlight sql"><figcaption><span>char_count--1.0.sql</span></figcaption><table><tr><td class="code"><pre><span class="line">\echo <span class="keyword">Use</span> <span class="string">"CREATE EXTENSION char_count"</span> <span class="keyword">to</span> <span class="keyword">load</span> this file. \quit</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> char_count(<span class="built_in">TEXT</span>, <span class="built_in">CHAR</span>)</span><br><span class="line"><span class="keyword">RETURNS</span> <span class="built_in">INTEGER</span></span><br><span class="line"><span class="keyword">LANGUAGE</span> plpgsql IMMUTABLE <span class="keyword">STRICT</span></span><br><span class="line">  <span class="keyword">AS</span> $$</span><br><span class="line">    <span class="keyword">DECLARE</span></span><br><span class="line">      charCount <span class="built_in">INTEGER</span> := <span class="number">0</span>;</span><br><span class="line">      i INTEGER := 0;</span><br><span class="line">      inputText TEXT := $1;</span><br><span class="line">      targetChar CHAR := $2;</span><br><span class="line">    <span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">WHILE</span> i &lt;= <span class="keyword">length</span>(inputText) <span class="keyword">LOOP</span></span><br><span class="line">      <span class="keyword">IF</span> <span class="keyword">substring</span>( inputText <span class="keyword">from</span> i <span class="keyword">for</span> <span class="number">1</span>) = targetChar <span class="keyword">THEN</span></span><br><span class="line">        charCount := charCount + <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">END</span> <span class="keyword">IF</span>;</span><br><span class="line">        i := i + 1;</span><br><span class="line">      <span class="keyword">END</span> <span class="keyword">LOOP</span>;</span><br><span class="line"></span><br><span class="line">    RETURN(charCount);</span><br><span class="line">    <span class="keyword">END</span>;</span><br><span class="line">  $$;</span><br></pre></td></tr></table></figure><p>Please note that the first echo line enforces the function to be loaded as extension</p><p>Create a Makefile</p><figure class="highlight bash"><figcaption><span>Makefile</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># contrib/char_count/Makefile</span></span><br><span class="line"></span><br><span class="line">EXTENSION = char_count</span><br><span class="line">DATA = char_count--1.0.sql</span><br><span class="line">PGFILEDESC = <span class="string">"char_count - count number of specified character"</span></span><br><span class="line">REGRESS = char_count</span><br><span class="line"></span><br><span class="line">ifdef USE_PGXS</span><br><span class="line">PG_CONFIG = pg_config</span><br><span class="line">PGXS := $(shell $(PG_CONFIG) --pgxs)</span><br><span class="line">include $(PGXS)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">subdir = contrib/char_count</span><br><span class="line">top_builddir = ../..</span><br><span class="line">include $(top_builddir)/src/Makefile.global</span><br><span class="line">include $(top_srcdir)/contrib/contrib-global.mk</span><br><span class="line">endif</span><br></pre></td></tr></table></figure><p>With the files in place ,we can go ahread and run within the char_count extension folder</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>This will install char_count extension to  $SHAREDIR</p><p>Now we can connect to the PG server and make use of the new extension that we have just added:</p><p><img src="/images/char_count-extension.png" alt="extension"></p><h3 id="5-Create-a-Test-Case-for-the-New-Extension"><a href="#5-Create-a-Test-Case-for-the-New-Extension" class="headerlink" title="5. Create a Test Case for the New Extension"></a>5. Create a Test Case for the New Extension</h3><p>We have already created a sql folder from previous steps, let’s create a new .sql file for our test case</p><figure class="highlight sql"><figcaption><span>char_count.sql</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> EXTENSION char_count;</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'a'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'b'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'c'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'x'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'c'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'b'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'5'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'0'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'asd'</span>);</span><br></pre></td></tr></table></figure><p>Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">REGRESS = char_count</span><br></pre></td></tr></table></figure><p>Run the testcase and Obtain Results</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make installcheck</span><br></pre></td></tr></table></figure><p>For the first time, the regression test will fail, because we have not provided the expected output file (.out file) for the test case. A new folder “results” is created upon running the regression test, and there is a (.out) file inside containing all the output from the test case</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> EXTENSION char_count;</span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'a'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          4</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'b'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          7</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'c'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          2</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'x'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          0</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'c'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          2</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'b'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          7</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'5'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          5</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'3'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          7</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'2'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          7</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'1'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          4</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'0'</span>);</span><br><span class="line"> char_count </span><br><span class="line"><span class="comment">------------</span></span><br><span class="line">          1</span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> char_count(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'asd'</span>);</span><br><span class="line">ERROR:  value too long for type character(1)</span><br><span class="line">CONTEXT:  PL/pgSQL function char_count(text,character) line 7 during statement block local variable initialization</span><br></pre></td></tr></table></figure><p>We should examine this .out file and made sure the outputs are all correct and we will copy it over to the expected folder</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp char_count/results/char_count.out char_count/expected</span><br></pre></td></tr></table></figure><h3 id="6-Create-your-Own-Extension-Using-C-Language"><a href="#6-Create-your-Own-Extension-Using-C-Language" class="headerlink" title="6. Create your Own Extension Using C Language"></a>6. Create your Own Extension Using C Language</h3><p>In the previous section, we created a extension using plpgsql function language. This is in many ways very similar to the ‘CREATE FUNCTION’ commands except that in the above example, we specifically states that the function can only be loaded through the CREATE EXTENSION command.</p><p>In most cases, the custom extensions are mostly built in C codes because of its flexibility and performance benefits.</p><p>To demonstrate this, we will create a new extension called char_count_c. Let’s repeat some of the process above:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> [PG_SOURCE_DIR]/contrib</span><br><span class="line">mkdir char_count_c</span><br><span class="line"><span class="built_in">cd</span> char_count_c</span><br><span class="line">mkdir expected</span><br><span class="line">mkdir sql</span><br></pre></td></tr></table></figure><p>create a control file:</p><figure class="highlight bash"><figcaption><span>char_count_c.control</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># char_count_c extension</span></span><br><span class="line">comment = <span class="string">'c function to count number of specified characters'</span></span><br><span class="line">default_version = <span class="string">'1.0'</span></span><br><span class="line">module_pathname = <span class="string">'$libdir/char_count_c'</span></span><br><span class="line">relocatable = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>create a data sql file </p><figure class="highlight bash"><figcaption><span>char_count_c--1.0.sql</span></figcaption><table><tr><td class="code"><pre><span class="line">\<span class="built_in">echo</span> Use <span class="string">"CREATE EXTENSION char_count"</span> to load this file. \quit</span><br><span class="line">CREATE FUNCTION char_count_c(TEXT, TEXT) RETURNS INTEGER</span><br><span class="line">AS <span class="string">'$libdir/char_count_c'</span></span><br><span class="line">LANGUAGE C IMMUTABLE STRICT</span><br></pre></td></tr></table></figure><p>This is where it differs from the previous method to add extension. In here we specifically set the LANGUAGE to be C as oppose to plpgsql.</p><p>$libdir/char_count_c is important as this is the path in which the PG will try to find a corresponding C share library when char_count_c extension is loaded.</p><p>Now, create a Makefile</p><figure class="highlight bash"><figcaption><span>Makefile</span></figcaption><table><tr><td class="code"><pre><span class="line">MODULES = char_count_c</span><br><span class="line">EXTENSION = char_count_c</span><br><span class="line">DATA = char_count_c--1.0.sql</span><br><span class="line">PGFILEDESC = <span class="string">"char_count_c - count number of specified character"</span></span><br><span class="line">REGRESS = char_count_c</span><br><span class="line"></span><br><span class="line">ifdef USE_PGXS</span><br><span class="line">PG_CONFIG = pg_config</span><br><span class="line">PGXS := $(shell $(PG_CONFIG) --pgxs)</span><br><span class="line">include $(PGXS)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">subdir = contrib/char_count_c</span><br><span class="line">top_builddir = ../..</span><br><span class="line">include $(top_builddir)/src/Makefile.global</span><br><span class="line">include $(top_srcdir)/contrib/contrib-global.mk</span><br><span class="line">endif</span><br></pre></td></tr></table></figure><p>Here we added a new line called MODULES = char_count_c. This line will actually compile your C code into a shared library (.so) file which will be used by PG when char_count_c extension is loaded.</p><p>Create a new C source file</p><figure class="highlight c"><figcaption><span>char_count_c.c</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"postgres.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"fmgr.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"utils/builtins.h"</span></span></span><br><span class="line"></span><br><span class="line">PG_MODULE_MAGIC;</span><br><span class="line"></span><br><span class="line">PG_FUNCTION_INFO_V1(char_count_c);</span><br><span class="line"></span><br><span class="line">Datum</span><br><span class="line">char_count_c(PG_FUNCTION_ARGS)</span><br><span class="line">&#123;</span><br><span class="line">        <span class="keyword">int</span> charCount = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">text</span> * inputText = PG_GETARG_TEXT_PP(<span class="number">0</span>);</span><br><span class="line">        <span class="built_in">text</span> * targetChar = PG_GETARG_TEXT_PP(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> inputText_sz = VARSIZE(inputText)-VARHDRSZ;</span><br><span class="line">        <span class="keyword">int</span> targetChar_sz = VARSIZE(targetChar)-VARHDRSZ;</span><br><span class="line">        <span class="keyword">char</span> * cp_inputText = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">char</span> * cp_targetChar = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ( targetChar_sz &gt; <span class="number">1</span> )</span><br><span class="line">        &#123;</span><br><span class="line">                elog(ERROR, <span class="string">"arg1 must be 1 char long"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        cp_inputText = (<span class="keyword">char</span> *) palloc ( inputText_sz + <span class="number">1</span>);</span><br><span class="line">        cp_targetChar = (<span class="keyword">char</span> *) palloc ( targetChar_sz + <span class="number">1</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(cp_inputText, VARDATA(inputText), inputText_sz);</span><br><span class="line">        <span class="built_in">memcpy</span>(cp_targetChar, VARDATA(targetChar), targetChar_sz);</span><br><span class="line"></span><br><span class="line">        elog(INFO, <span class="string">"arg0 length is %d, value %s"</span>, (<span class="keyword">int</span>)<span class="built_in">strlen</span>(cp_inputText), cp_inputText );</span><br><span class="line">        elog(INFO, <span class="string">"arg1 length is %d, value %s"</span>, (<span class="keyword">int</span>)<span class="built_in">strlen</span>(cp_targetChar), cp_targetChar );</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> ( i &lt; <span class="built_in">strlen</span>(cp_inputText) )</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="keyword">if</span>( cp_inputText[i] == cp_targetChar[<span class="number">0</span>] )</span><br><span class="line">                        charCount++;</span><br><span class="line">                i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        pfree(cp_inputText);</span><br><span class="line">        pfree(cp_targetChar);</span><br><span class="line">        PG_RETURN_INT32(charCount);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now we can compile the extension</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure><p>If make is successful, there should be a new C shared library created</p><p><img src="/images/c-make-shared.png" alt="extension"></p><p>Let’s go ahread and install</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>This will copy the<br>char_count_c–1.0.sql and char_count_c.control to $SHAREDIR/extension<br>and char_count_c.so to $LIBDIR</p><p>Make sure char_count_c.so is indeed installed to the $LIBDIR, otherwise, PG will not be able to find it when the extension is loaded.</p><p>With the extension installed, we can connect to the PG server and use the new extension</p><p><img src="/images/char_count_c_load.png" alt="extension"></p><p>Create a new test case in char_count_c/sql</p><p>let’s make a copy of the test case from previous “char_count” example and change the names to “char_count_c”</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> EXTENSION char_count_c;</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'a'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'b'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc'</span>,<span class="string">'c'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'x'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'c'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'b'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'5'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'0'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> char_count_c(<span class="string">'aaaabbbbbbbcc1111222222233333335555590'</span>,<span class="string">'asd'</span>);</span><br></pre></td></tr></table></figure><p>Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">REGRESS = char_count_c</span><br></pre></td></tr></table></figure><p>Run the test case</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make installcheck</span><br></pre></td></tr></table></figure><p>copy the .out file to expected folder</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp char_count_c/results/char_count_c.out char_count_c/expected</span><br></pre></td></tr></table></figure><h3 id="7-Add-the-new-extensions-to-global-Makefile"><a href="#7-Add-the-new-extensions-to-global-Makefile" class="headerlink" title="7. Add the new extensions to global Makefile"></a>7. Add the new extensions to global Makefile</h3><p>If you would like to have your extensions built along with the community ones, instead of building individually, you will need to modify the global extension Makefile located in [PG SOURCE DIR]/contrib/Makefile, and add:</p><p>char_count and char_count_c in SUBDIRS parameter</p><h3 id="8-Summary"><a href="#8-Summary" class="headerlink" title="8. Summary"></a>8. Summary</h3><p>Postgres is a very flexibile and powerful database system that provides different ways for the end users to extend existing functionalities to fulfill his or her business needs.</p><p>From the examples above, we have learned that since Postgres version 9, we are able to create new extensions using either plpgsql or C language and be able to create regression tests as part of the extension build to ensure the extensions will work as intended.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h3&gt;&lt;p&gt;Postgres is a huge database system consisting of a wide range of built-in data types, functions, features and operators that can be utilized to solve many common to complex problems. However, in the world full of complex problems, sometimes these are just not enough depending on the use case complexities.&lt;/p&gt;
&lt;p&gt;Worry not, since Postgres version 9, it is possible to extend Postgres’s existing functionalities with the use of “extensions”&lt;/p&gt;
&lt;p&gt;In this article, I will show you how to create your own extensions and add to Postgres.&lt;/p&gt;
&lt;p&gt;Please note that this article is based on Postgres version 12 running on Ubuntu 18.04 and before you can create your own extensions, PG must have been built and installed first&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="extension" scheme="http://caryhuang.github.io/tags/extension/"/>
    
  </entry>
  
  <entry>
    <title>A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function</title>
    <link href="http://caryhuang.github.io/2019/09/25/A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function/"/>
    <id>http://caryhuang.github.io/2019/09/25/A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function/</id>
    <published>2019-09-26T05:32:25.000Z</published>
    <updated>2020-01-13T23:40:06.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h3><p>Table partitioning is introduced after Postgres version 9.4 that provides several performance improvement under extreme loads. Partitioning refers to splitting one logically large table into smaller pieces, which in turn distribute heavy loads across smaller pieces (also known as partitions).</p><p>There are several ways to define a partition table, such as declarative partitioning and partitioning by inheritance. In this article we will focus on a simple form of declarative partitioning by value range.</p><p>Later in this article, we will discuss how we can define a <code>TRIGGER</code> to work with a <code>FUNCTION</code> to make table updates more dynamic.</p><a id="more"></a><h3 id="2-Creating-a-Table-Partition-by-Range"><a href="#2-Creating-a-Table-Partition-by-Range" class="headerlink" title="2. Creating a Table Partition by Range"></a>2. Creating a Table Partition by Range</h3><p>Let’s define a use case. Say we are a world famous IT consulting company and there is a database table called <code>salesman_performance</code>, which contains all the sales personnel world wide and their lifetime revenue of sales. Technically it is possible to have one table containing all sales personnel in the world but as entries get much larger, the query performance may be greatly reduced.</p><p>Here, we would like to create 7 partitions, representing 7 different levels of sales (or ranks) like so:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance (</span><br><span class="line">        salesman_id <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">NULL</span>,</span><br><span class="line">        first_name <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">        last_name <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">        revenue <span class="built_in">numeric</span>(<span class="number">11</span>,<span class="number">2</span>),</span><br><span class="line">        last_updated <span class="built_in">timestamp</span></span><br><span class="line">) <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">RANGE</span> (revenue);</span><br></pre></td></tr></table></figure><p>Please note that, we have to specify that it is a partition table by using keyword “PARTITION BY RANGE”. It is not possible to alter a already created table and make it a partition table.</p><p>Now, let’s create 7 partitions based on revenue performance:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_chief <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">100000000.00</span>) <span class="keyword">TO</span> (<span class="number">999999999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_elite <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">10000000.00</span>) <span class="keyword">TO</span> (<span class="number">99999999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_above_average <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">1000000.00</span>) <span class="keyword">TO</span> (<span class="number">9999999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_average <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">100000.00</span>) <span class="keyword">TO</span> (<span class="number">999999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_below_average <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">10000.00</span>) <span class="keyword">TO</span> (<span class="number">99999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_need_work <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">1000.00</span>) <span class="keyword">TO</span> (<span class="number">9999.99</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> salesman_performance_poor <span class="keyword">PARTITION</span> <span class="keyword">OF</span> salesman_performance</span><br><span class="line">        <span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">0.00</span>) <span class="keyword">TO</span> (<span class="number">999.99</span>);</span><br></pre></td></tr></table></figure><p>Let’s insert some values into “salesman_performace” table with different users having different revenue performance:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">1</span>, <span class="string">'Cary'</span>, <span class="string">'Huang'</span>, <span class="number">4458375.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">2</span>, <span class="string">'Nick'</span>, <span class="string">'Wahlberg'</span>, <span class="number">340.2</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">3</span>, <span class="string">'Ed'</span>, <span class="string">'Chase'</span>, <span class="number">764.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">4</span>, <span class="string">'Jennifer'</span>, <span class="string">'Davis'</span>, <span class="number">33750.12</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">5</span>, <span class="string">'Johnny'</span>, <span class="string">'Lollobrigida'</span>, <span class="number">4465.23</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">6</span>, <span class="string">'Bette'</span>, <span class="string">'Nicholson'</span>, <span class="number">600.44</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">7</span>, <span class="string">'Joe'</span>, <span class="string">'Swank'</span>, <span class="number">445237.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">8</span>, <span class="string">'Fred'</span>, <span class="string">'Costner'</span>, <span class="number">2456789.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">9</span>, <span class="string">'Karl'</span>, <span class="string">'Berry'</span>, <span class="number">4483758.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">10</span>, <span class="string">'Zero'</span>, <span class="string">'Cage'</span>, <span class="number">74638930.64</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">11</span>, <span class="string">'Matt'</span>, <span class="string">'Johansson'</span>, <span class="number">655837.34</span>, <span class="string">'2019-09-20 16:00:00'</span>);</span><br></pre></td></tr></table></figure><p>Postgres will automatically distribute queries to the respective partition based on revenue range. </p><p>You may run the \d+ command to see the table and its partitions</p><p><img src="/images/partition-table.png" alt="partition-table"></p><p>or examine just salesman_performance, which shows partition key and range</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">\d+ salesman-performance</span><br></pre></td></tr></table></figure><p><img src="/images/partition-table-info.png" alt="partition-table"></p><p>we can also use EXPLAIN ANALYZE query to see the query plan PG system makes to scan each partition. In the plan, it indicates how many rows of records exist in each partition</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">ANALYZE</span> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> salesman_performance;</span><br></pre></td></tr></table></figure><p><img src="/images/partition-table-query-plan.png" alt="partition-table"></p><p>There you have it. This ia a very basic partition table that distributes data by value range.</p><p>One of the advantages of using partition table is that bulk loads and deletes can be done simply by adding or removing partitions (DROP TABLE). This is much faster and can entirely avoid VACUUM overhead caused by DELETE</p><p>When you make a update to an entry. Say salesman_id 1 has reached the “Chief” level of sales rank from “Above Average” rank</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> salesman_performance <span class="keyword">SET</span> revenue = <span class="number">445837555.34</span> <span class="keyword">where</span> salesman_id=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>You will see that Postgres automatically put salesman_id 1 into the “salesman_performance_chief” partition and removes from “salesman_performance_above_average”</p><h3 id="3-Delete-and-Detach-Partition"><a href="#3-Delete-and-Detach-Partition" class="headerlink" title="3. Delete and Detach Partition"></a>3. Delete and Detach Partition</h3><p>A partition can be deleted completely simply by the “DROP TABLE [partition name]” command. This may not be desirable in some use cases.</p><p>The more recommended approach is to use “DETACH PARTITION” queries, which removes the partition relationship but preserves the data.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> salesman_performance DETACH <span class="keyword">PARTITION</span> salesman_performance_chief;</span><br></pre></td></tr></table></figure><p>If a partition range is missing, and the subsequent insertion has a range that no other partitions contain, the insertion will fail.</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> salesman_performance <span class="keyword">VALUES</span>( <span class="number">12</span>, <span class="string">'New'</span>, <span class="string">'User'</span>, <span class="number">755837555.34</span>, <span class="keyword">current_timestamp</span>);</span><br><span class="line"></span><br><span class="line">=&gt; should result in failure because no partitions contain a range for this revenue =  755837555.34</span><br></pre></td></tr></table></figure><p>If we add back the partition for the missing range, then the above insertion will work:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> salesman_performance ATTACH <span class="keyword">PARTITION</span> salesman_performance_chief</span><br><span class="line"><span class="keyword">FOR</span> <span class="keyword">VALUES</span> <span class="keyword">FROM</span> (<span class="number">100000000.00</span>) <span class="keyword">TO</span> (<span class="number">999999999.99</span>);</span><br></pre></td></tr></table></figure><h3 id="4-Create-Function-Using-Plpgsql-and-Define-a-Trigger"><a href="#4-Create-Function-Using-Plpgsql-and-Define-a-Trigger" class="headerlink" title="4. Create Function Using Plpgsql and Define a Trigger"></a>4. Create Function Using Plpgsql and Define a Trigger</h3><p>In this section, we will use an example of subscriber and coupon code redemption to illustrate the use of Plpgsql function and a trigger to correctly manage the distribution of available coupon codes.</p><p>First we will have a table called “subscriber”, which store a list of users and a table called “coupon”, which stores a list of available coupons.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> subscriber (</span><br><span class="line">    sub_id <span class="built_in">int</span> <span class="keyword">not</span> <span class="literal">NULL</span>,</span><br><span class="line">    first_name <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">    last_name <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">    coupon_code_redeemed <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">    last_updated <span class="built_in">timestamp</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> coupon (</span><br><span class="line">    coupon_code <span class="built_in">varchar</span>(<span class="number">45</span>),</span><br><span class="line">    percent_off <span class="built_in">int</span> <span class="keyword">CHECK</span> (percent_off &gt;= <span class="number">0</span> <span class="keyword">AND</span> percent_off&lt;=<span class="number">100</span>),</span><br><span class="line">    redeemed_by <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    time_redeemed <span class="built_in">timestamp</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>Let’s insert some records to the above tables:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> subscriber (sub_id, first_name, last_name, last_updated) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'Cary'</span>,<span class="string">'Huang'</span>,<span class="keyword">current_timestamp</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> subscriber  (sub_id, first_name, last_name, last_updated) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'Nick'</span>,<span class="string">'Wahlberg'</span>,<span class="keyword">current_timestamp</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> subscriber  (sub_id, first_name, last_name, last_updated) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'Johnny'</span>,<span class="string">'Lollobrigida'</span>,<span class="keyword">current_timestamp</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> subscriber  (sub_id, first_name, last_name, last_updated) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'Joe'</span>,<span class="string">'Swank'</span>,<span class="keyword">current_timestamp</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> subscriber  (sub_id, first_name, last_name, last_updated) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'Matt'</span>,<span class="string">'Johansson'</span>,<span class="keyword">current_timestamp</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> coupon (coupon_code, percent_off) <span class="keyword">VALUES</span>(<span class="string">'CXNEHD-746353'</span>,<span class="number">20</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> coupon (coupon_code, percent_off)  <span class="keyword">VALUES</span>(<span class="string">'CXNEHD-653834'</span>,<span class="number">30</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> coupon (coupon_code, percent_off)  <span class="keyword">VALUES</span>(<span class="string">'CXNEHD-538463'</span>,<span class="number">40</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> coupon (coupon_code, percent_off)  <span class="keyword">VALUES</span>(<span class="string">'CXNEHD-493567'</span>,<span class="number">50</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> coupon (coupon_code, percent_off)  <span class="keyword">VALUES</span>(<span class="string">'CXNEHD-384756'</span>,<span class="number">95</span>);</span><br></pre></td></tr></table></figure><p>The tables now look like:<br><img src="/images/coupon-and-subscriber.png" alt="trigger-function"></p><p>Say one subscriber redeems a coupon code, we would need a <code>FUNCTION</code> to check if the redeemed coupon code is valid (ie. Exists in coupon table). If valid, we will update the subscriber table with the coupon code redeemed and at the same time update the coupon table to indicate which subscriber redeemed the coupon and at what time.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> <span class="keyword">REPLACE</span> <span class="keyword">FUNCTION</span> redeem_coupon() <span class="keyword">RETURNS</span> <span class="keyword">trigger</span> <span class="keyword">AS</span> $redeem_coupon$</span><br><span class="line">    <span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">IF</span> <span class="keyword">EXISTS</span> ( <span class="keyword">SELECT</span> <span class="number">1</span> <span class="keyword">FROM</span> coupon c <span class="keyword">where</span> c.coupon_code = NEW.coupon_code_redeemed ) <span class="keyword">THEN</span></span><br><span class="line">        <span class="keyword">UPDATE</span> coupon <span class="keyword">SET</span> redeemed_by=OLD.first_name, time_redeemed=<span class="string">'2019-09-20 16:00:00'</span> <span class="keyword">where</span>  coupon_code = NEW.coupon_code_redeemed;</span><br><span class="line">    ELSE</span><br><span class="line">        RAISE EXCEPTION 'coupon code does not exist';</span><br><span class="line">    <span class="keyword">END</span> <span class="keyword">IF</span>;</span><br><span class="line">        RETURN NEW;</span><br><span class="line">    <span class="keyword">END</span>;</span><br><span class="line">$redeem_coupon$ LANGUAGE plpgsql;</span><br></pre></td></tr></table></figure><p>we need to define a <code>TRIGGER</code>, which is invoked BEFORE UPDATE, to check the validity of a given coupon code.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TRIGGER</span> redeem_coupon_trigger</span><br><span class="line">  <span class="keyword">BEFORE</span> <span class="keyword">UPDATE</span></span><br><span class="line">  <span class="keyword">ON</span> subscriber</span><br><span class="line">  <span class="keyword">FOR</span> <span class="keyword">EACH</span> <span class="keyword">ROW</span></span><br><span class="line">  <span class="keyword">EXECUTE</span> <span class="keyword">PROCEDURE</span> redeem_coupon();</span><br></pre></td></tr></table></figure><p>\d+ subscriber should look like this:</p><p><img src="/images/trigger_table.png" alt="trigger-function"></p><p>Let’s have some users redeem invalid coupon codes and as expected, an exception will be raised if coupon code is not valid.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'12345678'</span> <span class="keyword">where</span> first_name=<span class="string">'Cary'</span>;</span><br><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'87654321'</span> <span class="keyword">where</span> first_name=<span class="string">'Nick'</span>;</span><br><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'55555555'</span> <span class="keyword">where</span> first_name=<span class="string">'Joe'</span>;</span><br></pre></td></tr></table></figure><p><img src="/images/invalid-coupon.png" alt="trigger-function"></p><p>Let’s correct the above and redeem only the valid coupon codes and there should not be any error.</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'CXNEHD-493567'</span> <span class="keyword">where</span> first_name=<span class="string">'Cary'</span>;</span><br><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'CXNEHD-653834'</span> <span class="keyword">where</span> first_name=<span class="string">'Nick'</span>;</span><br><span class="line"><span class="keyword">UPDATE</span> subscriber <span class="keyword">set</span> coupon_code_redeemed=<span class="string">'CXNEHD-384756'</span> <span class="keyword">where</span> first_name=<span class="string">'Joe'</span>;</span><br></pre></td></tr></table></figure><p><img src="/images/valid-coupon.png" alt="trigger-function"></p><p>Now both table should look like this, and now both table have information cross-related.</p><p><img src="/images/coupon-redeemed.png" alt="trigger-function"></p><p>And there you have it, a basic trigger function executed before each update.</p><h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><p>With the support of partitioned table defined by value range, we are able to define a condition for postgres to automatically split the load of a very large table across many smaller partitions. This has a lot of benefits in terms of performance boost and more efficient data management.</p><p>Having postgres <code>FUNCTION</code>  and <code>TRIGGER</code> working together as a duo, we are able to make general queries and updates more dynamic and automatic to achieve more complex operations. As some of the complex logics can be defined and handled as <code>FUNCTION</code>, which is then invoked at appropriate moment defined by <code>TRIGGER</code>, the application integrated to Postgres will have much less logics to implement. </p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h3&gt;&lt;p&gt;Table partitioning is introduced after Postgres version 9.4 that provides several performance improvement under extreme loads. Partitioning refers to splitting one logically large table into smaller pieces, which in turn distribute heavy loads across smaller pieces (also known as partitions).&lt;/p&gt;
&lt;p&gt;There are several ways to define a partition table, such as declarative partitioning and partitioning by inheritance. In this article we will focus on a simple form of declarative partitioning by value range.&lt;/p&gt;
&lt;p&gt;Later in this article, we will discuss how we can define a &lt;code&gt;TRIGGER&lt;/code&gt; to work with a &lt;code&gt;FUNCTION&lt;/code&gt; to make table updates more dynamic.&lt;/p&gt;</summary>
    
    
    
    <category term="PostgreSQL" scheme="http://caryhuang.github.io/categories/PostgreSQL/"/>
    
    
    <category term="partition" scheme="http://caryhuang.github.io/tags/partition/"/>
    
    <category term="trigger" scheme="http://caryhuang.github.io/tags/trigger/"/>
    
  </entry>
  
</feed>
