{"pages":[{"title":"About Cary","text":"Cary is a multi-discpline software developer with 8 years of industrial experience developing innovative software solutions in C/C++ in the field of smart grid &amp; metering. He holds a bachelor degree in Electrical Engineering from University of British Columnbia (UBC) in Vancouver in 2012 and has been employed by Corinex Communication Corp. since then. He has extensive hands-on experience in technologies such as: Advanced Networking, Network &amp; Data security, Smart Metering Innovations, deployment management with Docker, Software Engineering Lifecycle, scalability, authentication, cryptology, relational &amp; non-relational database, web services, firewalls, embedded systems, RTOS, ARM, PKI, Cisco equipment, functional and Architecture Design with UML. Cary is a new member to the PostgreSQL community and has been contributing in terms of publishing technical blogs and performing patch reviews. He is actively involved in the development of Transparent Data Encryption (TDE) and internal Key Management System (KMS) features that are targeted to be released in the future PostgreSQL version 13 and 14. Putting the technical backgrounds aside. Cary is also a Forex trader, who automates most of his trading techniques and criteria with software. His trading criteria is rather simple. He trades mostly according to the price action and candle stick patterns as these reflect the true intent of the market. Cary is also a e-commerce entrepreneur who operates 2 online stores, “Shokunin Design Co” on Etsy.com and “Simplicity Design Inc” on Amazon.com. Check them out at the “links”. He is seasoned wood crafter, who designs, constructs and sells wooden hexagon shelves for home decoration under the “Shokunin Design Co” brand and the owner of baby product brand “Mini4More” under “Simplicity Design Inc” store front on Amazon. Cary is a believer that Your mind is a very powerful weapon; it can be your best friend, or your worst enemy. Having a positive mindset is absolutely crucial to your own success.","link":"/about/me.html"}],"posts":[{"title":"Types of SNMP OIDs Explained","text":"1. OverviewSNMP stands for Simple Network Management Protocol and it is commonly used to access and manage network equipment such as switches and routers. It has been available for a long time and has evolved quite a lot with better functionality and security. Each SNMP object can be addressed by its Object IDs (OIDs) and I find that many new SNMP users are confused about the SNMP OIDs and how they should be used correctly. There are 3 major kinds of SNMP OIDs, scalar, table and dynamic and each has its own unique way of accessing. This blog will explain each type of OID and show how they can be accessed correctly. 2. ScalarScalar SNMP OID represents one single value that is available for GET/SET on a SNMP agent. This type of SNMP OID is represented by a ‘leaf icon’ (if read only) or a “paper and pen” icon (if read and write) on MIB browser. To access a scalar value SNMP OID, you need to append a zero (0) in the end of the OID number. For example:cxSysBplTopologyTree has OID number = .1.3.6.1.4.1.6232.8.1.1.1 To access it, you much append a zero (0) in the end to indicate that you want to access this OID as a scalar value. So it becomes = .1.3.6.1.4.1.6232.8.1.1.1.0. without this zero, you will get “OID does not exist error” 3. TableTable OID represents one or more sets of values that are available for GET/SET on a SNMP agent. This type of SNMP OID is represented by a ‘Table icon’ on MIB browser. Each Table OID must have an index value that can be used by the client to select which table entry to retrieve value from. The index value is represented by the “key” icon on MIB browser and normally it is defined as integer. To access a value from a Table OID, you need to append an index number that is larger than zero at the end of the OID number For example:plSysFWVersion has OID = .1.3.6.1.4.1.6232.8.3.1.1.5 To access it, you must append an index number in the end of the OID number to select one of many table entries. In most cases, (also our case), you need to put the index number = 1 to indicate you would like the value from table entry number 1. So the OID becomes .1.3.6.1.4.1.6232.8.3.1.1.5.1. Without this index number 1, you will get “OID does not exist error”. Why do we need a table OID?SNMP is mostly used to manage network equipment and it is common that this network equipment can have multiple Network Interface Card (NIC), for example (eth0, eth1, eth4…etc). If table is not used, you will need to define multiple separate sets of SNMP OIDS, each corresponding to one NIC. This is extremely not efficient and could get messy. Easier way is to define the table OIDs containing only one set of OID common for all NICs, and manager software can access each NIC’s OID values simply by varying the index number 4. Dynamic OIDDynamic OID is a more advanced usage of Table OID, which involves using more than one value to index table entries. In table OID, we talked about using one integer value to select a table entry by appending a number in the end of OID value. In addition to the integer number acting as primary index, we can define a second index value to further refine the result… like a 2 dimensional table, and we call this dynamic OID. For dynamic OID, the table is defined like the picture below… with 2 keys defined to look up a table value: plPhyByMACCard and plPhyByMACMAC This means in order to retrieve a value from this table, you have to supply 2 key values • plPhyByMACCardThis is an integer value index, which we will append “1” in our case just like the previous chapter • plPhyByMACMACThis is the second index, and it has a type = PHYSADDRESS instead of integer, this means you need to append the 6 byte MAC address (in decimal form) also to the end of the OID in order to retrieve this value For example:• plPhyByMACTxPhySpeed has OID = .1.3.6.1.4.1.6232.8.3.3.1.1.11 We need to append a “1” in the end of the OID due to first index (plPhyByMACCard)So it becomes .1.3.6.1.4.1.6232.8.3.3.1.1.11.1 • Now we need to tell the agent which MAC address we would like to get the PHY speed from, let’s say MAC = 00:0B:C2:12:CD:E3. In our case, this MAC address is sent to the transport in cxSysPeerMacAddr attribute field • We need to convert the MAC to 6 decimal numbersSo it becomes 0.11.194.18.205.227 • Now we append the six numbers at the end of the OID that has been appended 1 from previous step:So it becomes .1.3.6.1.4.1.6232.8.3.3.1.1.11.1.0.11.194.18.205.227 Why do we need a second key value defined as MAC address?Let’s say a network switch has 5 devices connected to it having 5 different MAC addresses and there is one dynamic table OID using Network ID and MAC as keys defined to record the PHY speed to each connected equipment. In order for a SNMP manager to read the speed to one MAC equipment, it has to supply both network id and MAC address as index to the SNMP table so the agent can return the speed for respective device. Without dynamic table OID, the client simply cannot retrieve the speed value for any of the devices. SNMP walk can be used to GET all the available table entries, which represent all the devices connected. In the screenshot above, I first did a snmpwalk on OID .1.3.6.1.4.1.6232.8.3.3.1.1.11 without any index added, then I receive 2 entries, each entry corresponding to the Tx speed of one MAC. As you can see, even the returned value has the OID + integer index + MAC index. Second, I did a snmpget on OID .1.3.6.1.4.1.6232.8.3.3.1.1.11.1.0.11.194.18.205.227 and I only get one entry in the result as I have selected.","link":"/2019/07/04/Types-of-SNMP-OIDs-Explained/"},{"title":"A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function","text":"1. OverviewTable partitioning is introduced after Postgres version 9.4 that provides several performance improvement under extreme loads. Partitioning refers to splitting one logically large table into smaller pieces, which in turn distribute heavy loads across smaller pieces (also known as partitions). There are several ways to define a partition table, such as declarative partitioning and partitioning by inheritance. In this article we will focus on a simple form of declarative partitioning by value range. Later in this article, we will discuss how we can define a TRIGGER to work with a FUNCTION to make table updates more dynamic. 2. Creating a Table Partition by RangeLet’s define a use case. Say we are a world famous IT consulting company and there is a database table called salesman_performance, which contains all the sales personnel world wide and their lifetime revenue of sales. Technically it is possible to have one table containing all sales personnel in the world but as entries get much larger, the query performance may be greatly reduced. Here, we would like to create 7 partitions, representing 7 different levels of sales (or ranks) like so: CREATE TABLE salesman_performance ( salesman_id int not NULL, first_name varchar(45), last_name varchar(45), revenue numeric(11,2), last_updated timestamp) PARTITION BY RANGE (revenue); Please note that, we have to specify that it is a partition table by using keyword “PARTITION BY RANGE”. It is not possible to alter a already created table and make it a partition table. Now, let’s create 7 partitions based on revenue performance: CREATE TABLE salesman_performance_chief PARTITION OF salesman_performance FOR VALUES FROM (100000000.00) TO (999999999.99);CREATE TABLE salesman_performance_elite PARTITION OF salesman_performance FOR VALUES FROM (10000000.00) TO (99999999.99);CREATE TABLE salesman_performance_above_average PARTITION OF salesman_performance FOR VALUES FROM (1000000.00) TO (9999999.99);CREATE TABLE salesman_performance_average PARTITION OF salesman_performance FOR VALUES FROM (100000.00) TO (999999.99);CREATE TABLE salesman_performance_below_average PARTITION OF salesman_performance FOR VALUES FROM (10000.00) TO (99999.99);CREATE TABLE salesman_performance_need_work PARTITION OF salesman_performance FOR VALUES FROM (1000.00) TO (9999.99);CREATE TABLE salesman_performance_poor PARTITION OF salesman_performance FOR VALUES FROM (0.00) TO (999.99); Let’s insert some values into “salesman_performace” table with different users having different revenue performance: INSERT INTO salesman_performance VALUES( 1, 'Cary', 'Huang', 4458375.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 2, 'Nick', 'Wahlberg', 340.2, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 3, 'Ed', 'Chase', 764.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 4, 'Jennifer', 'Davis', 33750.12, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 5, 'Johnny', 'Lollobrigida', 4465.23, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 6, 'Bette', 'Nicholson', 600.44, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 7, 'Joe', 'Swank', 445237.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 8, 'Fred', 'Costner', 2456789.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 9, 'Karl', 'Berry', 4483758.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 10, 'Zero', 'Cage', 74638930.64, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 11, 'Matt', 'Johansson', 655837.34, '2019-09-20 16:00:00'); Postgres will automatically distribute queries to the respective partition based on revenue range. You may run the \\d+ command to see the table and its partitions or examine just salesman_performance, which shows partition key and range \\d+ salesman-performance we can also use EXPLAIN ANALYZE query to see the query plan PG system makes to scan each partition. In the plan, it indicates how many rows of records exist in each partition EXPLAIN ANALYZE SELECT * FROM salesman_performance; There you have it. This ia a very basic partition table that distributes data by value range. One of the advantages of using partition table is that bulk loads and deletes can be done simply by adding or removing partitions (DROP TABLE). This is much faster and can entirely avoid VACUUM overhead caused by DELETE When you make a update to an entry. Say salesman_id 1 has reached the “Chief” level of sales rank from “Above Average” rank UPDATE salesman_performance SET revenue = 445837555.34 where salesman_id=1; You will see that Postgres automatically put salesman_id 1 into the “salesman_performance_chief” partition and removes from “salesman_performance_above_average” 3. Delete and Detach PartitionA partition can be deleted completely simply by the “DROP TABLE [partition name]” command. This may not be desirable in some use cases. The more recommended approach is to use “DETACH PARTITION” queries, which removes the partition relationship but preserves the data. ALTER TABLE salesman_performance DETACH PARTITION salesman_performance_chief; If a partition range is missing, and the subsequent insertion has a range that no other partitions contain, the insertion will fail. INSERT INTO salesman_performance VALUES( 12, 'New', 'User', 755837555.34, current_timestamp);=&gt; should result in failure because no partitions contain a range for this revenue = 755837555.34 If we add back the partition for the missing range, then the above insertion will work: ALTER TABLE salesman_performance ATTACH PARTITION salesman_performance_chiefFOR VALUES FROM (100000000.00) TO (999999999.99); 4. Create Function Using Plpgsql and Define a TriggerIn this section, we will use an example of subscriber and coupon code redemption to illustrate the use of Plpgsql function and a trigger to correctly manage the distribution of available coupon codes. First we will have a table called “subscriber”, which store a list of users and a table called “coupon”, which stores a list of available coupons. CREATE TABLE subscriber ( sub_id int not NULL, first_name varchar(45), last_name varchar(45), coupon_code_redeemed varchar(200), last_updated timestamp);CREATE TABLE coupon ( coupon_code varchar(45), percent_off int CHECK (percent_off &gt;= 0 AND percent_off&lt;=100), redeemed_by varchar(100), time_redeemed timestamp); Let’s insert some records to the above tables: INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Cary','Huang',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Nick','Wahlberg',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Johnny','Lollobrigida',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Joe','Swank',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Matt','Johansson',current_timestamp);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-746353',20);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-653834',30);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-538463',40);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-493567',50);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-384756',95); The tables now look like: Say one subscriber redeems a coupon code, we would need a FUNCTION to check if the redeemed coupon code is valid (ie. Exists in coupon table). If valid, we will update the subscriber table with the coupon code redeemed and at the same time update the coupon table to indicate which subscriber redeemed the coupon and at what time. CREATE OR REPLACE FUNCTION redeem_coupon() RETURNS trigger AS $redeem_coupon$ BEGIN IF EXISTS ( SELECT 1 FROM coupon c where c.coupon_code = NEW.coupon_code_redeemed ) THEN UPDATE coupon SET redeemed_by=OLD.first_name, time_redeemed='2019-09-20 16:00:00' where coupon_code = NEW.coupon_code_redeemed; ELSE RAISE EXCEPTION 'coupon code does not exist'; END IF; RETURN NEW; END;$redeem_coupon$ LANGUAGE plpgsql; we need to define a TRIGGER, which is invoked BEFORE UPDATE, to check the validity of a given coupon code. CREATE TRIGGER redeem_coupon_trigger BEFORE UPDATE ON subscriber FOR EACH ROW EXECUTE PROCEDURE redeem_coupon(); \\d+ subscriber should look like this: Let’s have some users redeem invalid coupon codes and as expected, an exception will be raised if coupon code is not valid. UPDATE subscriber set coupon_code_redeemed='12345678' where first_name='Cary';UPDATE subscriber set coupon_code_redeemed='87654321' where first_name='Nick';UPDATE subscriber set coupon_code_redeemed='55555555' where first_name='Joe'; Let’s correct the above and redeem only the valid coupon codes and there should not be any error. UPDATE subscriber set coupon_code_redeemed='CXNEHD-493567' where first_name='Cary';UPDATE subscriber set coupon_code_redeemed='CXNEHD-653834' where first_name='Nick';UPDATE subscriber set coupon_code_redeemed='CXNEHD-384756' where first_name='Joe'; Now both table should look like this, and now both table have information cross-related. And there you have it, a basic trigger function executed before each update. 5. SummaryWith the support of partitioned table defined by value range, we are able to define a condition for postgres to automatically split the load of a very large table across many smaller partitions. This has a lot of benefits in terms of performance boost and more efficient data management. Having postgres FUNCTION and TRIGGER working together as a duo, we are able to make general queries and updates more dynamic and automatic to achieve more complex operations. As some of the complex logics can be defined and handled as FUNCTION, which is then invoked at appropriate moment defined by TRIGGER, the application integrated to Postgres will have much less logics to implement.","link":"/2019/09/25/A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function/"},{"title":"Replication-Failover-with-pg_rewind-in-PG12","text":"1. OverviewIn the previous blog, we have discussed how to correctly set up streaming replication clusters between one master and one slave in Postgres version 12. In this blog, we will simulate a failover scenario on the master database, which causes the replica (or slave) database cluster to be promoted as new master and continue the operation. We will also simulate a failback scenario to reuse the old master cluster after the failover scenario with the help of pg_rewind. Normally it is quite easy to do a failback to the old master after slave gets promoted to master but if there is data written to the old master after slave promotion, we will have an out-of-sync case between them both and we will have to use the pg_rewind tool to synchronize the two data directories in order to bring the old master to match the state of the new master. Please note that the pg_rewind tool will remove transactions from the old master in order to match up with the new, so certain pre-caution is needed to use this tool. Here’s a brief overview of list of actions we are going to perform: simulate failover by promoting slave cluster, so it becomes a new master simulate data insertion to master cluster, also referred as old master after promotion shutdown the old master cluster and set it up as a standby server run pg_rewind on old master to synchronize transaction states with new master bring up old master as a standby server to synchronize with the new master This blog assumes you already have streaming replication setup between one master and one slave from previous blog. If you have not checked out the previous blog titled “Streaming Replication Setup in PG12 - How to Do it Right”, it is recommended to give that a read first. The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04 2. Simulate a Failover CaseWe will simply promote the slave database cluster to simulate a failover. $ pg_ctl promote -D db-slave2019-10-30 11:10:16.951 PDT [16643] LOG: received promote request2019-10-30 11:10:16.951 PDT [16651] FATAL: terminating walreceiver process due to administrator command2019-10-30 11:10:16.966 PDT [16643] LOG: redo done at 0/3003B602019-10-30 11:10:16.991 PDT [16643] LOG: selected new timeline ID: 22019-10-30 11:10:17.030 PDT [16643] LOG: archive recovery complete2019-10-30 11:10:17.051 PDT [16642] LOG: database system is ready to accept connections As seen above, After slave gets promoted, it switches to a new timeline for future data operations. At this point the master and slave are no longer streaming WAL files from each other and we essentialyl have two independent database clusters running. We will call them old master and new master in the following sections instead so it is clear. 3. Insert Some Data to the Old MasterWe would like to create a data out-of-sync case by inserting some more data to the old master cluster. $ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y);\" -p 5432INSERT 0 100 Check that both the old master and new master are clearly out of sync: ## new master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row)## old master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 300(1 row) 4. Configure the Old Master as Standby Server to Sync with New MasterNow we would like to attempt a failover to make the old master as a standy server to syncrhonize with the new master. Let’s shutdown the old master cluster. $ pg_ctl -D db-master stopwaiting for server to shut down.... doneserver stopped Let’s update postgresql.conf in the old master: db-master/postgresql.confrecovery_target_timeline = 'latest'archive_cleanup_command = 'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r'restore_command = 'cp /home/caryh/streaming-replication/archivedir/%f %p'primary_slot_name = 'main'primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5433 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any' the primary_conninfo tells the old master to stream WAL files from the new master located at 127.0.0.1:5433. Also, do not forget to touch the standby.signal file to tell the cluster to run in standby mode: touch db-master/standby.signal We specified in the old master to connect to primary_slot_name = main. Let’s create the matching replication slot on the new master. $ psql -d clusterdb -U cary -c \"select * from pg_create_physical_replication_slot('main');\" -p 5433 slot_name | lsn -----------+----- main | (1 row)$ psql -d clusterdb -U cary -c \"select * from pg_replication_slots;\" -p 5433 -x-[ RECORD 1 ]-------+---------slot_name | mainplugin | slot_type | physicaldatoid | database | temporary | factive | factive_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | Now the new master has a matching replication slot called main and it is not active at the moment. Now we are ready to start the old master as standby server 5. Start the Old Master as Standby ServerNow, we are ready to start the old master as a standby: $ pg_ctl -D db-master start2019-10-30 11:30:04.071 PDT [1610] HINT: Future log output will go to log destination \"syslog\".2019-10-30 11:30:04.075 PDT [1611] LOG: database system was shut down at 2019-10-30 11:29:13 PDT2019-10-30 11:30:04.079 PDT [1611] LOG: restored log file \"00000002.history\" from archive2019-10-30 11:30:04.082 PDT [1611] LOG: entering standby mode2019-10-30 11:30:04.084 PDT [1611] LOG: restored log file \"00000002.history\" from archive2019-10-30 11:30:04.095 PDT [1611] FATAL: requested timeline 2 is not a child of this server's history2019-10-30 11:30:04.095 PDT [1611] DETAIL: Latest checkpoint is at 0/4000028 on timeline 1, but in the history of the requested timeline, the server forked off from that timeline at 0/3003B98.2019-10-30 11:30:04.096 PDT [1610] LOG: startup process (PID 1611) exited with exit code 12019-10-30 11:30:04.096 PDT [1610] LOG: aborting startup due to startup process failure2019-10-30 11:30:04.098 PDT [1610] LOG: database system is shut down As you can see above, the old master refuses to start because there is a timeline difference between the old master and the new master. This is caused by the additional data insertions that happens to the old master after the promotion event in step number 3. This is where pg_rewind comes handy in situation like this, to synchronize the two clusters. 6. Use pg_rewind to Synchronize the two ClustersNow, let’s synchronize the two database clusters with pg_rewind. $ pg_rewind --target-pgdata=db-master --source-server=\"port=5433 user=cary dbname=clusterdb\" --progresspg_rewind: connected to serverpg_rewind: servers diverged at WAL location 0/3003E58 on timeline 1pg_rewind: rewinding from last common checkpoint at 0/2000060 on timeline 1pg_rewind: reading source file listpg_rewind: reading target file listpg_rewind: reading WAL in targetpg_rewind: need to copy 53 MB (total source directory size is 78 MB)54363/54363 kB (100%) copiedpg_rewind: creating backup label and updating control filepg_rewind: syncing target data directorypg_rewind: Done! After pg_rewind is finished, we will have to edit once more the configuration of the old master because the tool copies most of the configuration settings from the new master to the old master as a synchronization process. Let’s examine both db-master/postgresql.auto.conf and db-master/postgresql.conf and make sure of the followings again. db-master/postgresql.conf############# db-master/postgresql.conf #############primary_slot_name = 'main'recovery_target_timeline = 'latest'port = 5432############# db-master/postgresql.auto.conf #############primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5433 sslmode=disable sslcompression=0 gssencmode=disable target_session_attrs=any' and also, don’t forget about this: touch db-master/standby.signal Now, we should be ready to start the old master again. 7. Start the Old Master Agian as Standby Server$ pg_ctl -D db-master start2019-10-30 12:27:28.140 PDT [5095] LOG: restored log file \"000000010000000000000002\" from archive2019-10-30 12:27:28.167 PDT [5095] LOG: redo starts at 0/20000282019-10-30 12:27:28.182 PDT [5095] LOG: consistent recovery state reached at 0/30272582019-10-30 12:27:28.183 PDT [5095] LOG: invalid record length at 0/3027258: wanted 24, got 02019-10-30 12:27:28.183 PDT [5095] LOG: redo done at 0/30272302019-10-30 12:27:28.183 PDT [5095] LOG: last completed transaction was at log time 2019-10-30 12:20:34.056723-07019-10-30 12:27:28.226 PDT [5094] LOG: database system is ready to accept connections The old master can now start as a streaming replication to the new master and we can observe that after using pg_rewind the additional data that was inserted to old master in step number 3 is now removed, as it has been rewound from 300 entries to 200 entries to match up with the new master. ## new master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row)## old master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 200(1 row) 8. SummaryIn this blog, we have simulated a failover case and observe the effect of promoting a standby slave server while more data insertions happening to the original master server. We have demonstrated how to use pg_rewind tool to synchronize both master and slave after the slave promotion. Though it results some data deletion at the original master, in the end, we are able to resolve the timeline conflict with pg_rewind and complete the database failover scenario.","link":"/2019/09/27/Replication-Failover-with-pg-rewind-in-PG12/"},{"title":"Understanding Security Features in PostgreSQL - Part 1","text":"1. IntroductionPostgreSQL is packed with several security features for a database administrator to utilize according to his or her organizational security needs. The word Security is a very broad concept and could refer to completely different procedures and methodology to achieve in different PostgreSQL components. This blog is divided into part 1, 2 and 3 and I will explain the word Security with regards to PostgreSQL version 12.1 and how it is practiced in different areas within the system. In Part 1 of the blog, I will be discussing the basic security features that exist in PostgreSQL with emphasis on Host-based authentication methods as well as user-based access control with the concept of roles. If done right, we could have a much more robust database server and potentially reduce the attack surface on the server, protecting it from attacks like SQL injections. I will also briefly discuss a few of the advanced authentication methods such as LDAP and PAM authentication. There are many more advanced authentication methods supported and we will be producing more articles in the near future to cover more of these methods. In Part 2 of the blog, I will be discussing TLS in greater detail, which I believe is crucial for a database administrator to understand first before enabling TLS in the PostgreSQL server. TLS is a fairly large and one of the least understood protocol today, which contains a lot of security components and methodology related to cryptography that could be quite confusing. In Part 3 of the blog, I will be discussing how to apply TLS configurations to both PostgreSQL server and client following the TLS principles that have been discussed in Part 2. I will also briefly discuss Transparent Data Encryption (TDE) that the PG community is currently working on that introduces another layer of secured database environment. Below is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. PostgreSQL Server Listen AddressPostgreSQL server is a TCP server that by default listens on localhost at port 5432. The server listen address may seem very trivial at first in terms of security but it is actually very important because understanding how the PostgreSQL is serving the incoming connections is fundamental to building a more secured network environment. Connection settings are located in postgresql.conf The listen_addresses parameter tells PostgreSQL which addresses to listen on. This value should match the IP address of the network interface cards in the host machine. ifconfig on Unix-based systems (or ipconfig for Windows) is a handy command that lists all the network interfaces and their IP addresses. listen_address supports the less secured * configuration, which will listen to all the network interfaces available #listen_addresses = 'localhost' # what IP address(es) to listen on; # comma-separated list of addresses; # defaults to 'localhost'; use '*' for all # (change requires restart)#port = 5432 # (change requires restart) Another important connection configuration is the maximum connections allowed. By default PostgreSQL allows 100 simultaneous connections to be active at a time with 3 connections reserved for super user. That is 97 connections for regular database users. These numbers should be configured accordingly depending on the usage case of the database server and we definitely don’t want too many unintentional connections to access the database max_connections = 100 # (change requires restart)superuser_reserved_connections = 3 # (change requires restart) 3. Host-Based AuthenticationHost-based authentication refers to the process of verifying the identity of a user connection based on the IP addresses of the connecting host. PostgreSQL supports host-based authentication by adding and removing desired entries in the pg_hba.conf file. This file works in a similar way as defining firewall rules. The official documentation on pg_hba.conf can be found here: https://www.postgresql.org/docs/current/auth-pg-hba-conf.html A simple example below defines the following rules: Allows connections from subnet 192.168.3.0/24 to access the database named “software_team” Allows connections from subnet 192.168.4.0/24 to access the database named “marketing_team” Allows connections from subnet 192.168.5.0/24 to access the database named “sales_team” Allows connections from subnet 192.168.6.0/24 to access the database named “management” The admin user has permission to access all the database given that the admin is connecting from localhost (both IPv4 and IPv6) or from a UNIX domain socket. Allows all user connections coming from subnet 192.168.7.0/24 to access the database named “production_team” and is able to do replication connection. Please note that the word “replication” is a special term reserved to allow replication connections rather than database name. Allows user from a unsecured network “172.16.30.0/24” to access the “sales_team” database only if the connection uses SSL (a.k.a TLS) Rejects all connections from 172.16.50.5 # TYPE DATABASE USER ADDRESS METHODlocal all admin trustlocal all admin 127.0.0.1/32 trustlocal all admin ::1/128 trusthost software_team all 192.168.3.0/24 trusthost marketing_team all 192.168.4.0/24 trusthost sales_team all 192.168.5.0/24 trusthost management all 192.168.6.0/24 trusthost production_team all 192.168.7.0/24 trusthost replication all 192.168.7.0/24 trusthost all all 172.16.50.5/32 rejecthostssl sales_team all 172.16.30.0/24 trust The simple example above uses 2 basic methods to control the access, trust and reject. This will suffice in a small database server environment. However, depending on the infrastructure, the application’s nature and data security, stronger authentication methods are encourged, such as LDAP, GSSPI with Kerberos, SSPI, RADIUS SCRAM-SHA-256…etc. Generally speaking, most of these stronger authentication methods require PostgreSQL to communicate with foreign authentication servers to complete the authentication process in a more secured way and provide automatic “single-sign-on” authentications through means of shared secrets, token exchange, or user name mappings. I will briefly introduces LDAP and PAM authentication in this blog. 4. Authentication with LDAP ServerLDAP stands for Light-weight Directory Access Protocol, which is commonly deployed as centralized authentication system for medium to large organizations. This authentication server provides user credential authentication and stores related user details like distinguished name, domain names and business units..etc. Every entry in an LDAP directory server has a distinguished name (DN). It is the name that uniquely identifies an entry in the directory and made up of attribute=value pairs. As a LDAP client on the PostgreSQL side, attribute=value pairs are required to be supplied in pg_hba.conf file separated by commas. For example: # TYPE DATABASE USER ADDRESS METHODhost production_team production_user 0.0.0.0/0 ldap dapserver=192.168.7.100 ldapport=389 ldapprefix=\"cn=\" ldapsuffix=\", dc=organization, dc=com\" Please note that LDAP by default is not encrypted and communicating user credential unecrypted is never a good idea. LDAP over TLS is supported by appending ldaptls=1 to the ldap attributes in pg_hba.conf file. Please also note that ldaptls=1 only provides secured connection between PostgreSQL server and LDAP server. The connection between PostgreSQL server and client is not using TLS by default, so it needs to be enabled as well. TLS is discussed in details in part 2 of this blog. 5. Authentication with PAMPAM stands for Pluggable Authentication Module and it operates similarly to password. The default service name is posgresql. First we need to create a linux user (Example based on Ubuntu 18.04). $ useradd production_user$ passwd production_userChanging password for user production_user.New UNIX password:Retype new UNIX password:passwd: all authentication tokens updated successfully. Create /etc/pam.d/postgresql with the content: #%PAM-1.0auth include system-authccount include system-authpassword include system-authsession include system-auth Create production_user in PostgreSQL server $ CREATE USER production_user; Then finally update the pg_hba.conf with pam authentication method. # TYPE DATABASE USER ADDRESS METHODhost production_team production_user 0.0.0.0/0 pam 6. Authentication with CertificateAuthentication with certificate can be applied to all the authentication methods by appending clientcert=1 in method parameters. This is only useful with hostssl type records in pg_hba.conf file and requires that the PostgreSQL server has TLS enabled in postgresql.conf with path to CA certificate specified. We will discuss TLS and certificates in part 2 of the blog in more details. With clientcert=1 in place, the server will require that the client to send its TLS certificate for verification. The connection will abort if client fails to provide a certiticate. The server will verify the common name (CN) in the certificate against the server’s hostname. Both should match. In addition, certificate validity dates will be verified and most importantly, the server will try to determine the chain of trust from the client certificate against the CA certificate configured in the server to determine if the client can be trusted. # TYPE DATABASE USER ADDRESS METHODhostssl production_team production_user 0.0.0.0/0 pam clientcert=1 7. Role-Based Access ControlRole-based access control refers to the process of verifying database access permissions based on the pre-defined roles and user privileges. PostgreSQL supports role-based access in several levels, such as table, function, procedural language and user levels. I will explain the concept in table and user level access control that follow the general guidelines below: A user with super user privilege can do any activities in the database A user who creates a table owns the table and can set its permission A user needs to belong to a proper role to perform administrative operations such as create another user or role Other users need proper permissions to view or operate on a table created by another user. When a PostgreSQL database cluster has been initialized, a super user will be created by default that equals to the system user that initializes the cluster. This super user is the starting point to define other role and other users and privileges to ensure proper database access. 8. Assign Table and Column Level Privileges to UsersThe GRANT clause supported in PostgreSQL is used to configure the access privileges (official documentation here: https://www.postgresql.org/docs/current/sql-grant.html). GRANT is a very universal clause that can be used to add access privileges to tables, databases, roles, table spaces…etc. The opposite of GRANT is REVOKE, which removes privileges (official documentation here: https://www.postgresql.org/docs/current/sql-revoke.html). In this blog, I will use GRANT on table and role level. When a table is created, it is assigned an owner. The owner is normally the user that executed the creation statement. The initial state is that only the owner (or a superuser) can do anything with the table. To allow other users to use it, privileges must be granted. There are many types of privileges that can be granted to a table or a table column. The image below is taken directly from the official PostgreSQL documentation that lists all the available privileges and their applicable objects. Consider a simple SQL command example below that assigns table and column access privileges to other users $ GRANT SELECT ON table1 TO userA;$ GRANT SELECT ON table2 TO userA;$ GRANT SELECT ON table3 TO userA;$ GRANT UPDATE ON table1 TO userB;$ GRANT INSERT ON table2 TO userB;$ GRANT INSERT ON table3 TO userB;$ GRANT UPDATE ON table2 TO userC;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO userC; The above SQL commands can be illustrated as: 9. Assign User Level Privileges as RolesA ROLE is an entity that can own database objects and have database privileges; a role can be considered a “user”, a “group”, or both depending on how it is used.(official documentation here: https://www.postgresql.org/docs/current/sql-createrole.html). Similar to a table, a created role can be altered with the ALTER clause or deleted with the DROP clause. Please note that when a role is created initially, the permission to LOGIN is not allowed by default and has to be manually set such that the users belonging to this role can log in to the database server. The same can be done with CREATE USER clause, which allows LOGIN by default. So the following 2 commands are essentially the same $ CREATE ROLE username LOGIN;$ CREATE USER username; The following image is taken directly from the official PostgreSQL documentation that lists all the privileg keywords that can be associated to a role. Consider the following simple example that creates 3 users and 4 different roles having different user level access privileges. /* Create 3 users */$ CREATE USER userA;$ CREATE USER userB;$ CREATE USER userC;/* Create 4 roles */$ CREATE ROLE role1 LOGIN CREATEDB CREATEROLE;$ CREATE ROLE role2 WITH PASSWORD '12345678' LOGIN REPLICATION;$ CREATE ROLE role3 LOGIN CREATEDB INHERIT;$ CREATE ROLE role4 LOGIN CONNECTION LIMIT 5 ;$ GRANT role1 TO userA;$ GRANT role1 TO userB;$ GRANT role3 TO userC;$ GRANT role4 TO userC;postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+----------------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {role1} | userB | | {role1} | userC | | {role3, role4} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | $ GRANT SELECT ON table1 TO role1;$ GRANT SELECT ON table2 TO role1;$ GRANT SELECT ON table3 TO role1;$ GRANT UPDATE ON table1 TO role1;$ GRANT INSERT ON table2 TO role1;$ GRANT INSERT ON table3 TO role1;$ GRANT UPDATE ON table2 TO role3;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO role3; Use the \\du+ meta command to see all the roles that have been created with summary of the attributes associated with each role. To see the full list of attributes per role, use the SQL command SELECT * FROM pg_roles;. postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+-----------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {} | userB | | {} | userC | | {} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | 10. Assign Table Level Privileges via RolesSection 3.1 illustrates privilege assignments directly to each individual users, which is desirable in smaller database servers. Imagine a larger database server where there could potentially be hundreds of users exist in the entire database cluster. Managing the table level privileges would get quite complicated and tedious. Luckily, PostgreSQL supports assigning users to roles for better privilege management Following the examples in section 3.2, we can use the GRANT command again to assign users to roles. Note that the Member of will display the relationship between users and roles after we have related them with GRANT clause. $ GRANT role1 TO userA;$ GRANT role1 TO userB;$ GRANT role3 TO userC;$ GRANT role4 TO userC;postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+----------------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {role1} | userB | | {role1} | userC | | {role3, role4} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | Following the examples in section 3.1, we can use the GRANT command again to assign table level privileges to roles that we have created instead of to users directly $ GRANT SELECT ON table1 TO role1;$ GRANT SELECT ON table2 TO role1;$ GRANT SELECT ON table3 TO role1;$ GRANT UPDATE ON table1 TO role1;$ GRANT INSERT ON table2 TO role1;$ GRANT INSERT ON table3 TO role1;$ GRANT UPDATE ON table2 TO role3;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO role3; The above SQL commands can be illustrated as: 11. Role InheritanceINHERIT and NOINHERIT are one of the special attributes that can be assigned to a role. When a role (say role 1) contains INHERIT attribute and is a member of another role (say role 2). All the attributes existing in both role 1 and role 2 will be applied to the user. Consider a simple example below: $ CREATE ROLE role1 LOGIN CREATEDB REPLICATION;$ CREATE ROLE role2 LOGIN CREATEROLE INHERIT;$ GRANT role1 TO role2;$ GRANT role2 TO userA; Which can be visualized as: In this case, role2 is created with INHERIT, userA will be assigned the privileges defined in both role1 and role2. 12. SummaryIn this blog, we went over several mechanisms in postgreSQL that allows a database administrator to configure the authentication of incoming user connections and the privilege configuration in table, column and user level via the concept of roles. PostgreSQL provides pg_hba.conf file that configures simple authentication and supports stronger authentication methods against remote authentication services such as GSSAPI, kerberos, RADIUS, PAM and LDAP..etc. So far we have only talked about authentication and authorization (AA) in PostgreSQL terms, in part 2 of this blog, I will explain the general concept of data encryption, how to secure data communication between server and client with TLS and how to achieve encryption on storage devices.","link":"/2020/01/10/Understanding-Security-Features-in-PostgreSQL-Part1/"},{"title":"A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres","text":"1. OverviewPostgres is a huge database system consisting of a wide range of built-in data types, functions, features and operators that can be utilized to solve many common to complex problems. However, in the world full of complex problems, sometimes these are just not enough depending on the use case complexities. Worry not, since Postgres version 9, it is possible to extend Postgres’s existing functionalities with the use of “extensions” In this article, I will show you how to create your own extensions and add to Postgres. Please note that this article is based on Postgres version 12 running on Ubuntu 18.04 and before you can create your own extensions, PG must have been built and installed first 2. Built-in ExtensionsBefore we jump into creating your own extensions, it is important to know that there is already a list of extensions available from the PG community included in the Postgres software distribution. The detailed information of community supplied extensions can be found in this link: https://www.postgresql.org/docs/9.1/contrib.html 3. Build and Install PG Default ExtensionsAll the PG community extensions are located in the directory below. This is also where we will be adding our own extensions $PG_SOURCE_DIR/postgres/contrib where [PG SOURCE DIR] is the directory to your PG source code These modules are not built automatically unless you build the ‘world’ target. To Manually build and install them, use these commands. cd contribmakesudo make install The above command will install the extensions to $SHAREDIR/extension and required C shared libraries to $LIBDIR where $SHAREDIR and $LIBDIR are the values returned by pg_config For the extensions that utilize the C language as implementation, there will be a C shared libraries (.so) being produced by the make command. This C shared library contains all the methods supported by the extension. With default extensions and libraries installed, we can then see the installed extensions by the following queries SELECT pg_available_extensions();SELECT pg_available_extension_versions(); 4. Create Extension Using plpqsql LanguageFor this example, we will create a very simple extension that will count the number of specified character of a given string. This extension takes 2 input arguments, first being the string, and second being the desired character. It will return an integer indicating the number of occurance of the desired characters presented in the string first, let’s navigate to the contrib directory to add our extension cd [PG SOURCE DIR]/contrib let’s create a new directory called char_count. This will be the name of the extension mkdir char_countcd char_count create the folders for defining testcases later mkdir sqlmkdir expected create and an extension control file using this naming convention: [Extension name].control char_count.control# char_count extensioncomment = 'function to count number of specified characters'default_version = '1.0'module_pathname = '$libdir/char_count'relocatable = true create a data sql file using this naming convention: [Extension name]--[Extension version].sql char_count--1.0.sql\\echo Use \"CREATE EXTENSION char_count\" to load this file. \\quitCREATE FUNCTION char_count(TEXT, CHAR)RETURNS INTEGERLANGUAGE plpgsql IMMUTABLE STRICT AS $$ DECLARE charCount INTEGER := 0; i INTEGER := 0; inputText TEXT := $1; targetChar CHAR := $2; BEGIN WHILE i &lt;= length(inputText) LOOP IF substring( inputText from i for 1) = targetChar THEN charCount := charCount + 1; END IF; i := i + 1; END LOOP; RETURN(charCount); END; $$; Please note that the first echo line enforces the function to be loaded as extension Create a Makefile Makefile# contrib/char_count/MakefileEXTENSION = char_countDATA = char_count--1.0.sqlPGFILEDESC = \"char_count - count number of specified character\"REGRESS = char_countifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/char_counttop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif With the files in place ,we can go ahread and run within the char_count extension folder sudo make install This will install char_count extension to $SHAREDIR Now we can connect to the PG server and make use of the new extension that we have just added: 5. Create a Test Case for the New ExtensionWe have already created a sql folder from previous steps, let’s create a new .sql file for our test case char_count.sqlCREATE EXTENSION char_count;SELECT char_count('aaaabbbbbbbcc','a');SELECT char_count('aaaabbbbbbbcc','b');SELECT char_count('aaaabbbbbbbcc','c');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','x');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','c');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','b');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','5');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','3');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','2');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','1');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','0');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','asd'); Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line: REGRESS = char_count Run the testcase and Obtain Results make installcheck For the first time, the regression test will fail, because we have not provided the expected output file (.out file) for the test case. A new folder “results” is created upon running the regression test, and there is a (.out) file inside containing all the output from the test case CREATE EXTENSION char_count;SELECT char_count('aaaabbbbbbbcc','a'); char_count ------------ 4(1 row)SELECT char_count('aaaabbbbbbbcc','b'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc','c'); char_count ------------ 2(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','x'); char_count ------------ 0(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','c'); char_count ------------ 2(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','b'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','5'); char_count ------------ 5(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','3'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','2'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','1'); char_count ------------ 4(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','0'); char_count ------------ 1(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','asd');ERROR: value too long for type character(1)CONTEXT: PL/pgSQL function char_count(text,character) line 7 during statement block local variable initialization We should examine this .out file and made sure the outputs are all correct and we will copy it over to the expected folder cp char_count/results/char_count.out char_count/expected 6. Create your Own Extension Using C LanguageIn the previous section, we created a extension using plpgsql function language. This is in many ways very similar to the ‘CREATE FUNCTION’ commands except that in the above example, we specifically states that the function can only be loaded through the CREATE EXTENSION command. In most cases, the custom extensions are mostly built in C codes because of its flexibility and performance benefits. To demonstrate this, we will create a new extension called char_count_c. Let’s repeat some of the process above: cd [PG_SOURCE_DIR]/contribmkdir char_count_ccd char_count_cmkdir expectedmkdir sql create a control file: char_count_c.control# char_count_c extensioncomment = 'c function to count number of specified characters'default_version = '1.0'module_pathname = '$libdir/char_count_c'relocatable = true create a data sql file char_count_c--1.0.sql\\echo Use \"CREATE EXTENSION char_count\" to load this file. \\quitCREATE FUNCTION char_count_c(TEXT, TEXT) RETURNS INTEGERAS '$libdir/char_count_c'LANGUAGE C IMMUTABLE STRICT This is where it differs from the previous method to add extension. In here we specifically set the LANGUAGE to be C as oppose to plpgsql. $libdir/char_count_c is important as this is the path in which the PG will try to find a corresponding C share library when char_count_c extension is loaded. Now, create a Makefile MakefileMODULES = char_count_cEXTENSION = char_count_cDATA = char_count_c--1.0.sqlPGFILEDESC = \"char_count_c - count number of specified character\"REGRESS = char_count_cifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/char_count_ctop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif Here we added a new line called MODULES = char_count_c. This line will actually compile your C code into a shared library (.so) file which will be used by PG when char_count_c extension is loaded. Create a new C source file char_count_c.c#include \"postgres.h\"#include \"fmgr.h\"#include \"utils/builtins.h\"PG_MODULE_MAGIC;PG_FUNCTION_INFO_V1(char_count_c);Datumchar_count_c(PG_FUNCTION_ARGS){ int charCount = 0; int i = 0; text * inputText = PG_GETARG_TEXT_PP(0); text * targetChar = PG_GETARG_TEXT_PP(1); int inputText_sz = VARSIZE(inputText)-VARHDRSZ; int targetChar_sz = VARSIZE(targetChar)-VARHDRSZ; char * cp_inputText = NULL; char * cp_targetChar = NULL; if ( targetChar_sz &gt; 1 ) { elog(ERROR, \"arg1 must be 1 char long\"); } cp_inputText = (char *) palloc ( inputText_sz + 1); cp_targetChar = (char *) palloc ( targetChar_sz + 1); memcpy(cp_inputText, VARDATA(inputText), inputText_sz); memcpy(cp_targetChar, VARDATA(targetChar), targetChar_sz); elog(INFO, \"arg0 length is %d, value %s\", (int)strlen(cp_inputText), cp_inputText ); elog(INFO, \"arg1 length is %d, value %s\", (int)strlen(cp_targetChar), cp_targetChar ); while ( i &lt; strlen(cp_inputText) ) { if( cp_inputText[i] == cp_targetChar[0] ) charCount++; i++; } pfree(cp_inputText); pfree(cp_targetChar); PG_RETURN_INT32(charCount);} Now we can compile the extension make If make is successful, there should be a new C shared library created Let’s go ahread and install sudo make install This will copy thechar_count_c–1.0.sql and char_count_c.control to $SHAREDIR/extensionand char_count_c.so to $LIBDIR Make sure char_count_c.so is indeed installed to the $LIBDIR, otherwise, PG will not be able to find it when the extension is loaded. With the extension installed, we can connect to the PG server and use the new extension Create a new test case in char_count_c/sql let’s make a copy of the test case from previous “char_count” example and change the names to “char_count_c” CREATE EXTENSION char_count_c;SELECT char_count_c('aaaabbbbbbbcc','a');SELECT char_count_c('aaaabbbbbbbcc','b');SELECT char_count_c('aaaabbbbbbbcc','c');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','x');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','c');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','b');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','5');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','3');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','2');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','1');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','0');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','asd'); Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line: REGRESS = char_count_c Run the test case make installcheck copy the .out file to expected folder cp char_count_c/results/char_count_c.out char_count_c/expected 7. Add the new extensions to global MakefileIf you would like to have your extensions built along with the community ones, instead of building individually, you will need to modify the global extension Makefile located in [PG SOURCE DIR]/contrib/Makefile, and add: char_count and char_count_c in SUBDIRS parameter 8. SummaryPostgres is a very flexibile and powerful database system that provides different ways for the end users to extend existing functionalities to fulfill his or her business needs. From the examples above, we have learned that since Postgres version 9, we are able to create new extensions using either plpgsql or C language and be able to create regression tests as part of the extension build to ensure the extensions will work as intended.","link":"/2019/09/25/A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres/"},{"title":"Steaming-Replication-Setup-in-PG12-How-to-do-it-right","text":"1. OverviewPostgreSQL 12 has been considered as a major update consisting of major performance boost with partitioning enhancements, indexing improvements, optimized planner logics and several others. One of the major changes is noticeably the removal of recovery.conf in a standby cluster. For this reason, the procedure to set up a streaming replication clusters has changed, and in this blog, I will demonstrate how to properly setup a streaming replication setup in PG12. Streaming replication setup requires a master cluster and one or more slave clusters that will replicate the data inserted to the master by streaming the archived WAL files generated by master. The master and slaves can reside on different machines connected via network but in this blog, we will use one master and one slave setup and both will be run on the same machine with different port number. The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04 2. Master Database Cluster SetupCreate a master database cluster using initdb tool: $ initdb /home/caryh/streaming-replication/db-master$ cd /home/caryh/streaming-replication /home/caryh/streaming-replication is the root folder to all the database clusters that we will be creating in this blog and db-master directory will be created here as a result of above commands. Let’s modify the default postgreql.conf and enable several important configuration options as shown below for streaming replication setup. db-master/postgresql.confwal_level = replicaarchive_mode = onmax_wal_senders = 10 wal_keep_segments = 10hot_standby = onarchive_command = 'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'port = 5432wal_log_hints = on The configuration above enables Postgres to archive the WAL files in the directory /home/caryh/streaming-replication/archivedir/ when it has completed writing to a full block of WAL file or when pg_basebackup command has been issued. The %f and %p used within archive_command are internal to Postgres and %f will be replaced with the filename of the target WAL file and %p replaced with path to the targeted WAL file. It is very important when setting the archive_command to ensure the WAL files are archived to a location where the slave cluster can access. Please note that wal_log_hints must be enabled for pg_rewind tool to work properly. We will discuss more about pg_rewind in the next blog post. Examine the client authentication file db-master/pg_hba.conf and make sure the master cluster allows replication connections from a slave cluster remotely. In my case, both my master and slave will be run on the same host, so I will leave the loopback IP address as it is. If your slave cluster is located in another machine, make sure to replace the loopback address with the right one. db-master/pg_hba.conf # Allow replication connections from 127.0.0.1, by a user with the replication privilege.# TYPE DATABASE USER ADDRESS METHODhost replication all 127.0.0.1/32 trust Let’s go ahead and start the master database cluster with the above configuration files, create a super user with permission to do replication, and a database called clusterdb $ pg_ctl -D db-master start$ createuser cary -s --replication$ createdb clusterdb Insert some test data to the master cluster. For simplicity, we will insert 100 integers to test_table. $ psql -d clusterdb -U cary -c \"CREATE TABLE test_table(x integer)\"CREATE TABLE$ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y)\"INSERT 0 100$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" count ------- 100(1 row) 3. Slave Database Cluster SetupThe goal of setting up the slave cluster is to make a backup of the current master and set it up as a standby server, meaning it will stream the WAL file updates from the master and perform replication of the data. Postgres provides several tools and methods to perform physical database backup. Exclusive methods such as pg_start_backup('label') and pg_stop_backup() are quite common in earlier Postgres versions. In this blog, we will use the newer, and simpler non-exclusive pg_basebackup fronend tool to execute the backup. There are advantages and disadvantaged for both methods and this discussion is not within the scope of this blog. This article here provides very good explaination on both methods: https://www.cybertec-postgresql.com/en/exclusive-backup-deprecated-what-now/ Let’s use pg_basebackup to create the slave cluster. $ pg_basebackup -h 127.0.0.1 -U cary -p 5432 -D db-slave -P -Xs -R31373/31373 kB (100%), 1/1 tablespace where:-h is the IP of the master cluster-U is the username that is permitted to do replication-p is the port number of the running master cluster-D is the directory where we want to set up the slave database cluster-P to show the progress-Xs to select WAL streaming method-R to write a recovery.conf file. This step is where it would differ from the previous PG versions. The -R command will no longer output a recovery.conf file in the db-slave directory. $ ls db-slavebackup_label pg_dynshmem pg_multixact pg_snapshots pg_tblspc pg_xactbase pg_hba.conf pg_notify pg_stat pg_twophase postgresql.auto.confglobal pg_ident.conf pg_replslot pg_stat_tmp PG_VERSION postgresql.confpg_commit_ts pg_logical pg_serial pg_subtrans pg_wal standby.signal The contents of the old recovery.conf file are moved to postgresql.conf and postgresql.auto.conf instead. Let’s examine db-slave/postgresql.auto.conf first, and we will see that pg_basebackup already created the primary_conninfo for us. This line used to be located in recovery.conf and it tells where and how a slave cluster should stream from the master cluster. Make sure this line is present in the postgresql.auto.conf. db-slave/postgresql.auto.conf# Do not edit this file manually!# It will be overwritten by the ALTER SYSTEM command.primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5432 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any' Let’s examine db-slave/postgresql.conf and update some of the parameters. db-slave/postgresql.confwal_level = replicaarchive_mode = onmax_wal_senders = 10 wal_keep_segments = 10hot_standby = onarchive_command = 'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'wal_log_hints = onport = 5433restore_command = 'cp /home/caryh/streaming-replication/archivedir/%f %p'archive_cleanup_command = 'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r' Since db-slave/postgresql.conf is directly copied from master cluster via pg_basebackup, we will need to change the port to some port different (5433 in this case) from the master since both are running on the same machine. We will need to fill the restore_command and archive_cleanup_command so the slave cluster knows how to get the archived WAL files for streaming purposes. These two parameters used to be defined in recovery.conf and are moved to postgresql.conf in PG12. In the db-slave directory, please note that a new standby.signal file is created automatically by pg_basebackup to indicate that this slave cluster will be run in standby mode. The standby.signal file is a new addition in PG12 to replace standby_mode = 'on' that used to be defined in recovery.conf. If this file is not present, make sure it is created by: $ touch db-slave/standby.signal Now, let’s start the slave cluster: $ pg_ctl -D db-slave start 4. Verify the Streaming Replication SetupOnce both master and slave clusters are setup and running, we should see from the ps -ef command that some of the backend processes are started to achieve the replication, namely, walsender and walreceiver. $ ps -ef | grep postgrescaryh 12782 2921 0 16:12 ? 00:00:00 /usr/local/pgsql/bin/postgres -D db-mastercaryh 12784 12782 0 16:12 ? 00:00:00 postgres: checkpointer caryh 12785 12782 0 16:12 ? 00:00:00 postgres: background writer caryh 12786 12782 0 16:12 ? 00:00:00 postgres: walwriter caryh 12787 12782 0 16:12 ? 00:00:00 postgres: autovacuum launcher caryh 12788 12782 0 16:12 ? 00:00:00 postgres: archiver last was 000000010000000000000002.00000028.backupcaryh 12789 12782 0 16:12 ? 00:00:00 postgres: stats collector caryh 12790 12782 0 16:12 ? 00:00:00 postgres: logical replication launcher caryh 15702 2921 0 17:06 ? 00:00:00 /usr/local/pgsql/bin/postgres -D db-slavecaryh 15703 15702 0 17:06 ? 00:00:00 postgres: startup recovering 000000010000000000000003caryh 15708 15702 0 17:06 ? 00:00:00 postgres: checkpointer caryh 15709 15702 0 17:06 ? 00:00:00 postgres: background writer caryh 15711 15702 0 17:06 ? 00:00:00 postgres: stats collector caryh 15713 15702 0 17:06 ? 00:00:00 postgres: walreceiver streaming 0/3000148caryh 15714 12782 0 17:06 ? 00:00:00 postgres: walsender cary 127.0.0.1(59088) streaming 0/3000148caryh 15728 10962 0 17:06 pts/5 00:00:00 grep --color=auto post We can also check the replication status in details by issuing a query to the master cluster: $ psql -d clusterdb -U cary -c \"select * from pg_stat_replication;\" -x -p 5432-[ RECORD 1 ]----+------------------------------pid | 15714usesysid | 16384usename | caryapplication_name | walreceiverclient_addr | 127.0.0.1client_hostname | client_port | 59088backend_start | 2019-10-29 17:06:49.072082-07backend_xmin | state | streamingsent_lsn | 0/3000148write_lsn | 0/3000148flush_lsn | 0/3000148replay_lsn | 0/3000148write_lag | flush_lag | replay_lag | sync_priority | 0sync_state | asyncreply_time | 2019-10-29 17:10:09.515563-07 Lastly, we can insert additional data to the master cluster and verify that slave also has the data updated. # Query slave cluster$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 100(1 row)# Query master cluster$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 100(1 row)# Insert more data to master cluster$ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y)\" -p 5432INSERT 0 100# Query slave cluster againpsql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row) Both master and slave clusters are now in sync. 5. Setup Replication SlotsThe previous steps illustrate how to correctly setup streaming replication between a master and slave cluster. However, there may be a case where the slave can be disconnected for some reason for extended period of time and may fail to replicate with the master when some of the un-replicated WAL files are recycled or deleted from the master cluster controlled by wal_keep_segments parameter. Replication slots ensure master can retain enough WAL segments for all slaves to receive them and prevent the master from removing rows that could cause a recovery conflict on the slaves. Let’s create a replication slot on the master cluster called slave: $ psql -d clusterdb -U cary -c \"select * from pg_create_physical_replication_slot('slave')\" -p 5432 slot_name | lsn -----------+----- slave | (1 row)$ psql -d clusterdb -U cary -c \"select * from pg_replication_slots\" -x -p 5432-[ RECORD 1 ]-------+---------slot_name | slaveplugin | slot_type | physicaldatoid | database | temporary | factive | factive_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | We have just created replication slot on master called slave and it is currently not active (active = f). Let’s modify slave’s postgresql.conf and make it connect to the master’s replication slot db-slave/postgresql.confprimary_slot_name = 'slave' Please note that this argument primary_slot_name us also used to be defined in recovery.conf and moved to postgresql.conf in PG12. After the change, we are required to restart the slave. $ pg_ctl -D db-slave stop$ pg_ctl -D db-slave start If all is good, checking the replication slots on master should have the slot status as active. $ psql -d clusterdb -U cary -c \"select * from pg_replication_slots\" -x -p 5432-[ RECORD 1 ]-------+----------slot_name | slaveplugin | slot_type | physicaldatoid | database | temporary | factive | tactive_pid | 16652xmin | catalog_xmin | restart_lsn | 0/3003B98confirmed_flush_lsn | 6. SummaryIn this blog, we have discussed the updated procedures to setup streaming replication clusters in PG12, in which several steps have been changed from the older versions, particularly the removal of recovery.conf. Here is a short list of changes related to replication setup that have been moved from recovery.conf restore_command =&gt; moved to postgresql.conf recovery_target_timeline =&gt; moved to postgresql.conf standby_mode =&gt; replaced by standby.signal primary_conninfo =&gt; moved to postgresql.conf or postgresql.auto.conf archive_cleanup_command =&gt; moved to postgresql.conf primary_slot_name =&gt; moved to postgresql.conf","link":"/2019/10/29/Steaming-Replication-Setup-in-PG12-How-to-do-it-right/"},{"title":"Trace-Postgres-query-processing-internals-with-debugger","text":"1. OverviewIn this article we will use GDB debugger to trace the internals of Postgres and observe how an input query passes through several levels of transformation (Parser -&gt; Analyzer -&gt; Rewriter -&gt; Planner -&gt; Executor) and eventually produces an output. This article is based on PG12 running on Ubuntu 18.04, and we will use a simple SELECT query with ORDER BY , GROUP BY, and LIMIT keywords to go through the entire query processing tracing. 2. PreparationGDB debugger is required to be installed to trace the internals of Postgres. Most recent distribution of Linux already comes with gdb pre-installed. If you do not have it, please install. 2.1 Enable Debugging and Disable Compiler Optimization on PG BuildFor GDB to be useful, the postgres binaries have to be compiled with debugging symbols enabled (-g). In addition, I would suggest to turn off compiler optimization (-O0) such that while tracing we will be able to examine all memory blocks and values, and observe the execution flow properly. Enable debugging using the ./configure utility in the Postgres source code repository cd [PG_SOURCE_DIR]./configure --enable-debug This will add the (-g) parameter to the CFLAGS in the main Makefile to include debugging symbols. Once finished, let’s disable compiler optimization by editing src/Makefile.global Find the line where CFLAGS is defined and changed -O2 to -O0 like this: CFLAGS = -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -g -O0 Then we need to build and install with the new Makefile makesudo make install 2.2 Initialize Postgres ServerFor a new build, we will need to initialize a new database initDb /home/caryh/postgresqlcreate user caryhcreatedb carytest For referencing purposes, I would suggest enable debug log for the Postgres server by modifying postgres.conf located in database home directory. In this case it is located in /home/caryh/postgresql/postgres.conf Enable the following lines in postgres.conf postgres.conflog_destination = 'syslog'syslog_facility = 'LOCAL0'syslog_ident = 'postgres'syslog_sequence_numbers = onsyslog_split_messages = ondebug_print_parse = ondebug_print_rewritten = ondebug_print_plan = ondebug_pretty_print = onlog_checkpoints = onlog_connections = onlog_disconnections = onlog_duration = on Why do we enable debug log when we will be tracing postgres with gdb? This is because the output at some of the stages of query processing is represented as a complex list of structures and it is not very straightforward to print this structure unless we have written a third party print script that can help us recursively print the content of the complex structure. Postgres already has this function built-in and presented in the form of a debugging log. 2.3 Start Postgres Server and Connect with Client ToolStart the PG database pg_ctl -D /home/caryh/postgresql start Connect to PG database as user psql -d carytest -U cary 2.4 Populate Example Tables and ValuesCREATE TABLE deviceinfo ( serial_number varchar(45) PRIMARY KEY, manufacturer varchar(45), device_type int, password varchar(45), registration_time timestamp);CREATE TABLE devicedata ( serial_number varchar(45) REFERENCES deviceinfo(serial_number), record_time timestamp, uptime int, temperature numeric(10,2), voltage numeric(10,2), power numeric(10,2), firmware_version varchar(45), configuration_file varchar(45));INSERT INTO deviceinfo VALUES( 'X00001', 'Highgo Inc', 1, 'password', '2019-09-18 16:00:00');INSERT INTO deviceinfo VALUES( 'X00002', 'Highgo Inc', 2, 'password', '2019-09-18 17:00:00');INSERT INTO deviceinfo VALUES( 'X00003', 'Highgo Inc', 1, 'password', '2019-09-18 18:00:00');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 16:00:00', 2000, 38.23, 189.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 17:00:00', 3000, 68.23, 221.00, 675.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 18:00:00', 4000, 70.23, 220.00, 333.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 19:00:00', 5000, 124.23, 88.00, 678.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 11:00:00', 8000, 234.23, 567.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 12:00:00', 9000, 56.23, 234.00, 345.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 13:00:00', 3000, 12.23, 56.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 14:00:00', 4000, 56.23, 77.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 11:00:00', 8000, 234.23, 567.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 12:00:00', 9000, 56.23, 234.00, 345.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 13:00:00', 3000, 12.23, 56.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 14:00:00', 4000, 56.23, 77.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 07:00:00', 25000, 68.23, 99.00, 43.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 08:00:00', 20600, 178.23, 333.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 09:00:00', 20070, 5.23, 33.00, 123.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 10:00:00', 200043, 45.23, 45.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 09:00:00', 20070, 5.23, 33.00, 123.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 10:00:00', 200043, 45.23, 45.00, 456.1, 'version01', 'config01'); 3. Start gdb DebuggerFind the PID of the connecting client PG session $ ps -ef | grep postgrescaryh 7072 1946 0 Sep26 ? 00:00:01 /usr/local/pgsql/bin/postgres -D /home/caryh/postgresqlcaryh 7074 7072 0 Sep26 ? 00:00:00 postgres: checkpointer caryh 7075 7072 0 Sep26 ? 00:00:01 postgres: background writer caryh 7076 7072 0 Sep26 ? 00:00:01 postgres: walwriter caryh 7077 7072 0 Sep26 ? 00:00:01 postgres: autovacuum launcher caryh 7078 7072 0 Sep26 ? 00:00:03 postgres: stats collector caryh 7079 7072 0 Sep26 ? 00:00:00 postgres: logical replication launcher caryh 7082 7072 0 Sep26 ? 00:00:00 postgres: cary carytest [local] idle In this case it is the last line of the ps output as both my client and server reside in the same machine. Yours may be different. caryh 7082 7072 0 Sep26 ? 00:00:00 postgres: cary carytest [local] idle Now we can run gdb with the postgres binary sudo gdb /usr/local/pgsql/bin/postgresGNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-gitCopyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type \"show copying\"and \"show warranty\" for details.This GDB was configured as \"x86_64-linux-gnu\".Type \"show configuration\" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type \"help\".Type \"apropos word\" to search for commands related to \"word\"...Reading symbols from /usr/local/pgsql/bin/postgres...done.(gdb) Now, we can attach gdb to the PID identified in previous step (gdb) attach 7082Attaching to program: /usr/local/pgsql/bin/postgres, process 7082Reading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug/.build-id/28/c6aade70b2d40d1f0f3d0a1a0cad1ab816448f.debug...done.done.[Thread debugging using libthread_db enabled]Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.Reading symbols from /lib/x86_64-linux-gnu/librt.so.1...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/librt-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libdl.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libdl-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libm.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libm-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.27.so...done.done.Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libnss_files.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libnss_files-2.27.so...done.done.0x00007fce71eafb77 in epoll_wait (epfd=4, events=0x5633194c87e0, maxevents=1, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:3030 ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.(gdb) Upon attach, Postgres process will be put on a break and we are able to issue breakpoints command from here 4. Start Tracing with gdbexec_simple_query is the function that will trigger all stages of query processing. Let’s put a breakpoint here. (gdb) b exec_simple_queryBreakpoint 1 at 0x56331899a43b: file postgres.c, line 985.(gdb) cContinuing. Now, let’s type in a SELECT query with ORDER BY keywords on the postgres client connection terminal to trigger break point carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; Breakpoint should be triggered Breakpoint 1, exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985(gdb) Let’s do a backtrace bt command to see how the control got here. (gdb) bt#0 exec_simple_query (query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985#1 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#2 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#3 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#4 0x00005633188f753e in ServerLoop () at postmaster.c:1704#5 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#6 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 As you can see, PostmasterMain process is one of the early process to be started and this is where it will spawn all the backend processes and initialize the ‘ServerLoop’ to listen for client connections. When a client connets and issues some queries, the handle will be passed from the backend to PostgresMain and this is where the query processing will begine. 5. Parser StageParser Stage is the first stage in query processing, which will take an input query string and produce a raw un-analyzed parse tree. The control will eventually come to the raw_parser function, so let’s set a break point there and do a backtrace: (gdb) b raw_parserBreakpoint 2 at 0x5633186b5bae: file parser.c, line 37.(gdb) cContinuing.Breakpoint 2, raw_parser (str=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37(gdb) bt#0 raw_parser (str=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37#1 0x000056331899a03e in pg_parse_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:641#2 0x000056331899a4c9 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1037#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 In raw_parser, 2 things will happen, first to scan the query with flex-based scanner to check keyword validity and second to do the actual parsing with bison-based parser. In the end, it will return a parse tree for next stage. (gdb) n43 yyscanner = scanner_init(str, &amp;yyextra.core_yy_extra,(gdb) n47 yyextra.have_lookahead = false;(gdb) n50 parser_init(&amp;yyextra);(gdb) 53 yyresult = base_yyparse(yyscanner);(gdb) 56 scanner_finish(yyscanner);(gdb) 58 if (yyresult) /* error */(gdb) 61 return yyextra.parsetree; It is not very straight-forward to examine the content of the parse tree stored in yyextra.parsetree as above. This is why we enabled postgres debug log so that we can utilize it to recursively print the content of the parse tree. The parse tree illustrated by yyextra.parsetree can be visualized as this image below: 6.0 Analyzer StageNow we have a list of parse trees, size 1 in this example, PG will need to feed each item in the list into anaylzer and rewriter functions. Let’s set a break point at parse_analyze function (gdb) b parse_analyzeBreakpoint 3 at 0x56331867d608: file analyze.c, line 104.(gdb) cContinuing.Breakpoint 3, parse_analyze (parseTree=0x56331949dd50, sourceText=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at analyze.c:104104 ParseState *pstate = make_parsestate(NULL);(gdb) bt#0 parse_analyze (parseTree=0x56331949dd50, sourceText=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at analyze.c:104#1 0x000056331899a0a8 in pg_analyze_and_rewrite (parsetree=0x56331949dd50, query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at postgres.c:695#2 0x000056331899a702 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1140#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 The above backtrace shows how the control gets to parse_analyze function, and 2 vital imputs are parseTree (type RawStmt) and (const char) sourceText Let’s traverse to the end of parse_analyze (gdb) n109 pstate-&gt;p_sourcetext = sourceText;(gdb) 111 if (numParams &gt; 0)(gdb) 114 pstate-&gt;p_queryEnv = queryEnv;(gdb) 116 query = transformTopLevelStmt(pstate, parseTree);(gdb) 118 if (post_parse_analyze_hook)(gdb) 121 free_parsestate(pstate);(gdb) 123 return query; At analyzer stage, it produces a result of type Query and it is in fact the data type return from the parser stage as a List of Query. This structure will be fed into the rewriter stage. 7.0 Rewriter StageRewriter is the next stage following analyzer, let’s create a break point at pg_rewrite_query and do a backtrace: (gdb) b pg_rewrite_queryBreakpoint 4 at 0x56331899a1c1: file postgres.c, line 773(gdb) cContinuing.Breakpoint 4, pg_rewrite_query (query=0x56331949dee0) at postgres.c:773773 if (Debug_print_parse)(gdb) bt#0 pg_rewrite_query (query=0x56331949dee0) at postgres.c:773#1 0x000056331899a0cf in pg_analyze_and_rewrite (parsetree=0x56331949dd50, query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at postgres.c:704#2 0x000056331899a702 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1140#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228(gdb) Rewriter takes the output of the previou stage and returns a querytree_list of type List*. Let’s trace the function to the end and print the output 773 if (Debug_print_parse)(gdb) n774 elog_node_display(LOG, &quot;parse tree&quot;, query,(gdb) 777 if (log_parser_stats)(gdb) 780 if (query-&gt;commandType == CMD_UTILITY)(gdb) 788 querytree_list = QueryRewrite(query);(gdb) 791 if (log_parser_stats)(gdb) 848 if (Debug_print_rewritten)(gdb) 849 elog_node_display(LOG, &quot;rewritten parse tree&quot;, querytree_list,(gdb) 852 return querytree_list; the line 774 elog_node_display and line 849 elog_node_display are the debug print function provided by postgres to recursively print the content of Query before and after rewriter stage. After examining the output query tree, we found that in this example, the rewriter does not make much modification to the origianl query tree and it can be visualized as: 8.0 Planner StagePlanner is the next stage immediately following the previous. The main planner function entry point is pg_plan_query and it takes the output from previous stage as input. Let’s create a breakpoint and do a backtrace again (gdb) b pg_plan_queriesBreakpoint 5 at 0x56331899a32d: file postgres.c, line 948.(gdb) cContinuing.Breakpoint 5, pg_plan_queries (querytrees=0x563319558558, cursorOptions=256, boundParams=0x0) at postgres.c:948948 List *stmt_list = NIL;(gdb) bt#0 pg_plan_queries (querytrees=0x563319558558, cursorOptions=256, boundParams=0x0) at postgres.c:948#1 0x000056331899a722 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1143#2 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#3 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#4 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#5 0x00005633188f753e in ServerLoop () at postmaster.c:1704#6 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#7 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228(gdb) Now, we are here, let’s trace the function until the end. Please note that for each content block in the input querytree list, the function will call a helper plan function called pg_plan_query and it will perform the real plan operation there and return the result in plannedStmt data type (gdb) n951 foreach(query_list, querytrees)(gdb) n953 Query *query = lfirst_node(Query, query_list);(gdb) n956 if (query-&gt;commandType == CMD_UTILITY)(gdb) n968 stmt = pg_plan_query(query, cursorOptions, boundParams);(gdb) spg_plan_query (querytree=0x56331949dee0, cursorOptions=256, boundParams=0x0) at postgres.c:866866 if (querytree-&gt;commandType == CMD_UTILITY)(gdb) n874 if (log_planner_stats)(gdb) 878 plan = planner(querytree, cursorOptions, boundParams);(gdb) n880 if (log_planner_stats)(gdb) 929 if (Debug_print_plan)(gdb) 930 elog_node_display(LOG, &quot;plan&quot;, plan, Debug_pretty_print);(gdb) 934 return plan; Line 930 elog_node_display will print the content of PlannedStmt recursively to syslog and it can be visualized as: The above plan tree corresponds to the output of EXPLAIN ANALYZE on the same query. carytest=&gt; EXPLAIN ANALYZE SELECT serial_number, COUNT(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; QUERY PLAN ------------------------------------------------------------------------------------ Limit (cost=1.32..1.33 rows=2 width=15) (actual time=0.043..0.044 rows=2 loops=1) -&gt; Sort (cost=1.32..1.33 rows=3 width=15) (actual time=0.042..0.042 rows=2 loops=1) Sort Key: (count(serial_number)) DESC Sort Method: quicksort Memory: 25kB -&gt; HashAggregate (cost=1.27..1.30 rows=3 width=15) (actual time=0.033..0.035 rows=3 loops=1) Group Key: serial_number -&gt; Seq Scan on devicedata (cost=0.00..1.18 rows=18 width=7) (actual time=0.013..0.016 rows=18 loops=1) Planning Time: 28.541 ms Execution Time: 0.097 ms(9 rows) Line 878 plan = planner(querytree, cursorOptions, boundParams); in the above trace is the real planner logic and it is a complex stage. Inside this function, it will compute the initial cost and run time cost of all possible queries and in the end, it will choose a plan that is the least expensive. with the plannedStmt produced, we are ready to enter the next stage of query processing. 9.0 Executor StageIn addition to planner, executor is also one of the complex stages of query processing. This module is responsible for executing the query plan produced from previous stage and sending the query results back to the connecting client. Executor is invoked and managed with a wrapper called portal and portal is an object representing the execution state of a query and providing memory management services but it does not actually run the executor. In the end, the portal will invoke one of the four executor routines as below -ExecutorStart()-ExecutorRun()-ExecutorFinish()-ExecutorEnd() Before we can use the above routines, the portal needs to be initialized first. In the previous stage, the control is left at exec_simple_query at line 1147, let’s continue tracing from here to enter portal initialization Let’s create a break point for each executor routine and do a back trace on each as we continue (gdb) b ExecutorStartBreakpoint 6 at 0x5633187ad797: file execMain.c, line 146.(gdb) b ExecutorRunBreakpoint 7 at 0x5633187ada1e: file execMain.c, line 306.(gdb) b ExecutorFinishBreakpoint 8 at 0x5633187adc35: file execMain.c, line 405.(gdb) b ExecutorEndBreakpoint 9 at 0x5633187add1e: file execMain.c, line 465. 9.1 Executor StartThe main purpose of ExecutorStart routine is to prepare the query plan, allocate storage and prepare rule manager. Let’s continue the tracing and do a backtrace. Breakpoint 6, ExecutorStart (queryDesc=0x5633195712e0, eflags=0) at execMain.c:146146 if (ExecutorStart_hook)(gdb) bt#0 ExecutorStart (queryDesc=0x564977500190, eflags=0) at execMain.c:146#1 0x0000564975eb87e0 in PortalStart (portal=0x5649774a18d0, params=0x0, eflags=0, snapshot=0x0) at pquery.c:518#2 0x0000564975eb27b5 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1176#3 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#4 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#5 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#6 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#7 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#8 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) 9.2 Executor RunExecutorRun is the main routine of executor module, and its main task is to execute the query plan, this routing will call the ExecutePlan function to actually execute the plan. In the end, before return, the result of query will be stored in Estate structure called estate and inside there is a count of how many tutples have been processed by the executor (gdb) cContinuing.Breakpoint 7, ExecutorRun (queryDesc=0x5633195712e0, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:306306 if (ExecutorRun_hook)(gdb) bt#0 ExecutorRun (queryDesc=0x564977500190, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:306#1 0x0000564975eb915c in PortalRunSelect (portal=0x5649774a18d0, forward=true, count=0, dest=0x564977539460) at pquery.c:929#2 0x0000564975eb8db6 in PortalRun (portal=0x5649774a18d0, count=9223372036854775807, isTopLevel=true, run_once=true, dest=0x564977539460, altdest=0x564977539460, completionTag=0x7ffff0b937d0 &quot;&quot;) at pquery.c:770#3 0x0000564975eb28ad in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1215#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) Continue tracing the ExecutorRun to the end. 306 if (ExecutorRun_hook)(gdb) n309 standard_ExecutorRun(queryDesc, direction, count, execute_once);(gdb) sstandard_ExecutorRun (queryDesc=0x564977500190, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:325325 estate = queryDesc-&gt;estate;(gdb) n333 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) n336 if (queryDesc-&gt;totaltime)(gdb) n342 operation = queryDesc-&gt;operation;(gdb) 343 dest = queryDesc-&gt;dest;(gdb) 348 estate-&gt;es_processed = 0;(gdb) 350 sendTuples = (operation == CMD_SELECT ||(gdb) 353 if (sendTuples)(gdb) 354 dest-&gt;rStartup(dest, operation, queryDesc-&gt;tupDesc);(gdb) 359 if (!ScanDirectionIsNoMovement(direction))(gdb) 361 if (execute_once &amp;&amp; queryDesc-&gt;already_executed)(gdb) 363 queryDesc-&gt;already_executed = true;(gdb) 365 ExecutePlan(estate,(gdb) 367 queryDesc-&gt;plannedstmt-&gt;parallelModeNeeded,(gdb) 365 ExecutePlan(estate,(gdb) 379 if (sendTuples)(gdb) 380 dest-&gt;rShutdown(dest);(gdb) 382 if (queryDesc-&gt;totaltime)(gdb) 385 MemoryContextSwitchTo(oldcontext);(gdb) p estate$6 = (EState *) 0x56497751fbb0(gdb) p estate-&gt;es_processed$7 = 2 9.3 Executor FinishExecutorFinish must be called after the last ExecutorRun, its main task is to perform necessary clearn up actions and also fire up after Triggers. Let’s trace a little further. (gdb) cContinuing.Breakpoint 8, ExecutorFinish (queryDesc=0x5633195712e0) at execMain.c:405405 if (ExecutorFinish_hook)(gdb) bt#0 ExecutorFinish (queryDesc=0x564977500190) at execMain.c:405#1 0x0000564975c5b52c in PortalCleanup (portal=0x5649774a18d0) at portalcmds.c:300#2 0x0000564976071ba4 in PortalDrop (portal=0x5649774a18d0, isTopCommit=false) at portalmem.c:499#3 0x0000564975eb28d3 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228 Continue tracing the ExecutorFinish to the end. 405 if (ExecutorFinish_hook)(gdb) n408 standard_ExecutorFinish(queryDesc);(gdb) sstandard_ExecutorFinish (queryDesc=0x564977500190) at execMain.c:420420 estate = queryDesc-&gt;estate;(gdb) n429 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) 432 if (queryDesc-&gt;totaltime)(gdb) 436 ExecPostprocessPlan(estate);(gdb) 439 if (!(estate-&gt;es_top_eflags &amp; EXEC_FLAG_SKIP_TRIGGERS))(gdb) 442 if (queryDesc-&gt;totaltime)(gdb) 445 MemoryContextSwitchTo(oldcontext);(gdb) 447 estate-&gt;es_finished = true;(gdb) 448 } 9.4 Executor EndThis routing basically resets and releases some of the state variables in QueryDesc used during execution. ExecutorEnd is the last routine to be called and before entry, the PortalCleanup and PortalDrop are invoked first. So as we are in this routine the outer Portal object is also performing the cleanup process. Breakpoint 9, ExecutorEnd (queryDesc=0x5633195712e0) at execMain.c:465465 if (ExecutorEnd_hook)(gdb) bt#0 ExecutorEnd (queryDesc=0x564977500190) at execMain.c:465#1 0x0000564975c5b538 in PortalCleanup (portal=0x5649774a18d0) at portalcmds.c:301#2 0x0000564976071ba4 in PortalDrop (portal=0x5649774a18d0, isTopCommit=false) at portalmem.c:499#3 0x0000564975eb28d3 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) Let’s Continue tracing ExecutorEnd to the end. 465 if (ExecutorEnd_hook)(gdb) n468 standard_ExecutorEnd(queryDesc);(gdb) sstandard_ExecutorEnd (queryDesc=0x564977500190) at execMain.c:480480 estate = queryDesc-&gt;estate;(gdb) n495 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) 497 ExecEndPlan(queryDesc-&gt;planstate, estate);(gdb) 500 UnregisterSnapshot(estate-&gt;es_snapshot);(gdb) 501 UnregisterSnapshot(estate-&gt;es_crosscheck_snapshot);(gdb) 506 MemoryContextSwitchTo(oldcontext);(gdb) 512 FreeExecutorState(estate);(gdb) 515 queryDesc-&gt;tupDesc = NULL;(gdb) 516 queryDesc-&gt;estate = NULL;(gdb) 517 queryDesc-&gt;planstate = NULL;(gdb) 518 queryDesc-&gt;totaltime = NULL;(gdb) 519 } This routine marks the end of the query processing stages, the control will be passed back to exec_simple_query to finish the transaction and present result back to the client. 10.0 Presenting the Result Back to ClientWith the transaction ended, the send_ready_for_query flag will be set, and the control is now able to enter ReadyForQuery to present the result to client. (gdb) b ReadyForQueryBreakpoint 10 at 0x56331899811d: file dest.c, line 251.(gdb) cContinuing.Breakpoint 10, ReadyForQuery (dest=DestRemote) at dest.c:251251 {(gdb) bt#0 ReadyForQuery (dest=DestRemote) at dest.c:251#1 0x0000564975eb6eee in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4176#2 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#3 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#4 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#5 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#6 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) n252 switch (dest)(gdb) 257 if (PG_PROTOCOL_MAJOR(FrontendProtocol) &gt;= 3)(gdb) 261 pq_beginmessage(&amp;buf, 'Z');(gdb) 262 pq_sendbyte(&amp;buf, TransactionBlockStatusCode());(gdb) 263 pq_endmessage(&amp;buf);(gdb) p dest$90 = DestRemote(gdb) n268 pq_flush();(gdb) 269 break;(gdb) 282 }(gdb) as pq_flush() is called, the result of the query will be returned back to the client at remote destination. 10.1 Client ResultsClient will now see the output below as a result of the query carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; serial_number | count ---------------+------- X00002 | 8 X00003 | 6(2 rows) 11 SummarySo far, we have traced through severl stags of query processing. Namely Parser Analyzer Rewritter Planner Executor To summarize all the above, I have created a simple call hierarchy ( or a list of breakpoints) below that outlines the important core functions that will be called while stepping through the above stages. The ‘b’ in front of each function name corresponds to the break point command of gdb. ## Main Entry ##b exec_simple_query ## Parser ## b pg_parse_query -&gt; returns (List* of Query) b raw_parser -&gt; returns (List* of Query) b base_yyparse -&gt; returns (List* of Query) ## Analzyer and Rewritter ## b pg_analyze_and_rewrite -&gt; returns (List*) b parse_analyze -&gt; returns (Query*) b pg_rewrite_query -&gt; returns (List* of Query) b QueryRewrite -&gt; returns (List* of Query) ## Planner ## b pg_plan_queries -&gt; returns (List* of plannedStmt) b pg_plan_query -&gt; returns (PlannedStmt*) b planner -&gt; returns (PlannedStmt*) ## Executor ## b PortalStart -&gt; returns void b ExecutorStart -&gt; returns void b PortalRun -&gt; returns bool b PortalRunSelect -&gt; returns uint64 b ExecutorRun -&gt; returns void b PortalDrop -&gt; returns void b PortalCleanup -&gt; returns void b ExecutorFinish -&gt; returns void b ExecutorEnd -&gt; returns void ## Present Result ##b ReadyForQuery -&gt; returns void b pq_flush -&gt; returns void","link":"/2019/09/27/Trace-Postgres-query-processing-internals-with-debugger/"},{"title":"Logical Replication Between PostgreSQL and MongoDB","text":"1. IntroductionPostgreSQL and MongoDB are two popular open source relational (SQL) and non-relational (NoSQL) databases available today. Both are maintained by groups of very experienced development teams globally and are widely used in many popular industries for adminitration and analytical purposes. MongoDB is a NoSQL Document-oriented Database which stores the data in form of key-value pairs expressed in JSON or BSON; it provides high performance and scalability along with data modelling and data management of huge sets of data in an enterprise application. PostgreSQL is a SQL database designed to handle a range of workloads in many applications supporting many concurrent users; it is a feature-rich database with high extensibility, which allows users to create custom plugins, extensions, data types, common table expressions to expand existing features I have recently been involved in the development of a MongoDB Decoder Plugin for PostgreSQL, which can be paired with a logical replication slot to publish WAL changes to a subscriber in a format that MongoDB can understand. Basically, we would like to enable logical replication between MongoDB (as subscriber) and PostgreSQL (as publisher) in an automatic fashion. Since both databases are very different in nature, physical replication of WAL files is not applicable in this case. The logical replication supported by PostgreSQL is a method of replicating data objects changes based on replication identity (usually a primary key) and it would be the ideal choice for this purpose as it is designed to allow sharing the object changes between PostgreSQL and multiple other databases. The MongoDB Decoder Plugin will play a very important role as it is directly responsible for producing a series of WAL changes in a format that MongoDB can understand (ie. Javascript and JSON). In this blog, I would like to share some of my initial research and design approach towards the development of MongoDB Decoder Plugin. 2. ArchitectureSince it is not possible yet to establish a direct logical replication connection between PostgreSQL and MongoDB due to two very different implementations, some kind of software application is ideally required to act as a bridge between PostgreSQL and MongoDB to manage the subscription and publication. As you can see in the image below, the MongoDB Decoder Plugin associated with a logical replication slot and the bridge software application are required to achieve a fully automated replication setup. Unfortunately, the bridge application does not exist yet, but we do have a plan to develop such application in near future. So, for now, we will not be able to have a fully automated logical replication setup. Fortunately, we can utilize the existing pg_recvlogical front end tool to act as a subscriber of database changes and publish these changes to MongoDb in the form of output file, as illustrated below. With this setup, we are able to verify the correctness of the MongoDB Decoder Plugin output against a running MongoDB in a semi-automatic fashion. 3. Plugin UsageBased on the second architecture drawing above without the special bridge application, we expect the plugin to be used in similar way as normal logical decoding setup. The Mongodb Decoder Plugin is named wal2mongo as of now and the following examples show the envisioned procedures to make use of such plugin and replicate data changes to a MongoDB instance. First, we will have to build and install wal2mongo in the contrib source folder and start a PostgreSQL cluster with the following parameters in postgresql.conf. The wal_level = logical tells PostgreSQL that the replication should be done logically rather than physically (wal_level = replica). Since we are setting up replication between 2 very different database systems in nature (PostgreSQL vs MongoDB), physical replication is not possible. All the table changes will be replicated to MongoDB in the form of logical commands. max_wal_senders = 10 limits the maximum number of wal_sender proccesses that can be forked to publish changes to subscriber. The default value is 10, and is sufficient for our setup. wal_level = logicalmax_wal_senders = 10 On a psql client session, we create a new logical replication slot and associate it to the MongoDB logical decoding plugin. Replication slot is an important utility mechanism in logical replication and this blog from 2ndQuadrant has really good explaination of its purpose: (https://www.2ndquadrant.com/en/blog/postgresql-9-4-slots/) $ SELECT * FROM pg_create_logical_replication_slot('mongo_slot', 'wal2mongo'); where mongo_slot is the name of the new logical replication slot and wal2mongo is the name of the logical decoding plugin that you have previously installed in the contrib folder. We can check the created replication slot with this command: $ SELECT * FROM pg_replication_slots; At this point, the PostgreSQL instance will be tracking the changes done to the database. We can verify this by creating a table, inserting or deleting some values and checking the change with the command: $ SELECT * FROM pg_logical_slot_get_changes('mongo_slot', NULL, NULL); Alternatively, one can use pg_recvlogical front end tool to subscribe to the created replication slot, automatically receives streams of changes in MongoDB format and outputs the changes to a file. $ pg_recvlogical --slot mongo_slot --start -f mongodb.js Once initiated, pg_recvlogical will continuously stream database changes from the publisher and output the changes in MongoDB format and in mongodb.js as output file. It will continue to stream the changes until user manually terminates or the publisher has shutdown. This file can then be loaded to MongoDB using the Mongo client tool like this: $ mongo &lt; mongodb.jsMongoDB shell version v4.2.3connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { \"id\" : UUID(\"39d478df-b8ca-4030-8a05-0e1ebbf6bc44\") }MongoDB server version: 4.2.3switched to db mydbWriteResult({ \"nInserted\" : 1 })WriteResult({ \"nInserted\" : 1 })WriteResult({ \"nInserted\" : 1 })bye where the mongodb.js file contains: use mydb;db.table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});db.table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});db.table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”}); 4. TerminologyBoth databases use different terminologies to describe the data storage. Before we can replicate the changes of PostgreSQL objects and translate them to MongoDB equivalent, it is important to gain clear understanding of the terminologies used on both databases. The table below is our initial terminology mappings: PostgreSQL Terms MongoDB Terms MongoDB Description Database Database A physical container for collections Table Collection A grouping of MongoDB documents, do not enforce a schema Row Document A record in a MongoDB collection, can have difference fields within a collection Column Field A name-value pair in a document Index Index A data structure that optimizes queries Primary Key Primary Key A record’s unique immutable identified. The _id field holds a document’s primary key which is usually a BSON ObjectID Transaction Transaction Multi-document transactions are atomic and available in v4.2 5. Supported Change OperationsOur initial design of the MongoDB Decoder Plugin is to support database changes caused by clauses “INSERT”, “UPDATE” and “DELETE”, with future support of “TRUNCATE”, and “DROP”. These are few of the most common SQL commands used to alter the contents of the database and they serve as a good starting point. To be able to replicate changes caused by these commands, it is important that the table is created with one or more primary keys. In fact, defining a primary key is required for logical replication to work properly because it serves as replication identity so the PostgreSQL can accurately track a table change properly. For example, if a row is deleted from a table that does not have a primary key defined, the logical replication process will only detect that there has been a delete event, but it will not be able to figure out which row is deleted. This is not what we want. The following is some basic examples of the SQL change commands and their previsioned outputs: $ BEGIN;$ INSERT INTO table1(a, b, c) VALUES(1, 'Cary', '2020-02-01');$ INSERT INTO table1(a, b, c) VALUES(2, 'David', '2020-02-02');$ INSERT INTO table1(a, b, c) VALUES(3, 'Grant', '2020-02-03');$ UPDATE table1 SET b='Cary'; $ UPDATE table1 SET b='David' WHERE a = 3;$ DELETE FROM table1;$ COMMIT; The simple SQL commands above can be translated into the following MongoDB commands. This is a simple example to showcase the potential input and output from the plugin and we will introduce more blogs in the near future as the development progresses further to show case some more advanced cases. db.table1.insert({“a”: 1, “b”: “Cary”, “c”: “2020-02-01”})db.table1.insert({“a”: 2, “b”: “David”, “c”: “2020-02-02”})db.table1.insert({“a”: 3, “b”: “Grant”, “c”: “2020-02-03”})db.table1.updateMany({“a”: 1, “c”: ”2020-02-01”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 2, “c”: ”2020-02-02”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 3, “c”: ”2020-02-03”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 3, “c”: “2020-02-03”, {$set:{“b”: “David”}})db.table1.remove({“a”: 1, “c”: ”2020-02-01”}, true)db.table1.remove ({“a”: 2, “c”: ”2020-02-02”}, true)db.table1.remove ({“a”: 3, “c”: ”2020-02-03”}, true) 6. Atomicity and TransactionsA write operation in MongoDB is atomic on the level of a single document, and since MongoDB v4.0, multi-document transaction control is supported to ensure the atomicity of multi-document write operations. For this reason, the MongoDB Deocoder Plugin shall support 2 output modes, normal and transaction mode. In normal mode, all the PostgreSQL changes will be translated to MongoDB equivalent without considering transactions. In other words, users cannot tell from the output if these changes are issued by the same or different transactions. The output can be fed directly to MongoDB, which can gurantee certain level of atomicity involving the same document Since MongoDB v4.0, there is a support for multi-document transaction mechanism, which acts similarly to the transaction control in PostgreSQL. Consider a normal insert operation like this with transaction ID = 500 within database named “mydb” and having cluster_name = “mycluster” configured in postgresql.conf: $ BEGIN;$ INSERT INTO table1(a, b, c) VALUES(1, 'Cary', '2020-02-01');$ INSERT INTO table1(a, b, c) VALUES(2, 'Michael', '2020-02-02');$ INSERT INTO table1(a, b, c) VALUES(3, 'Grant', '2020-02-03');$ COMMIT; In normal output mode, the plugin will generate: use mydb;db.table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});db.table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});db.table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”}); In transaction output mode, the plugin will generate: session500_mycluster = db.getMongo().startSession();session500_mycluster.startTransaction();use mydb;session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”});session500_mycluster.commitTransaction();session500_mycluster.endSession(); Please note that the session variable used in the MongoDB output is composed of the word session concatenated with the transaction ID and the cluster name. This is to gurantee that the variable name will stay unique when multiple PostgrSQL databases are publishing using the same plugin towards a single MongoDB instance. The cluster_name is a configurable parameter in postgresql.conf that is used to uniquely identify the PG cluster. The user has to choose the desired output modes between normal and transaction depending on the version of the MongoDB instance. MongoDB versions before v4.0 do not support multi-document transaction mechanism so user will have to stick with the normal output mode. MongoDB versions after v4.0 have transaction mechanism supported and thus user can use either normal or transaction output mode. Generally, transaction output mode is recommended to be used when there are multiple PostgreSQL publishers in the network publishing changes to a single MongoDB instance. 7. Data TranslationPostgreSQL supports far more data types than those supported by MongoDB, so some of the similar data types will be treated as one type before publishing to MongoDB. Using the same database name, transaction ID and cluster name in previous section, the table below shows some of the popular data types and their MongoDB transaltions. PostgreSQL Datatype MongoDB Datatype Normal Output Transaction Output smallint integer bigint numeric integer db.table1.insert({“a”:1}) session500_mycluster.getDatabase(“mydb”).table1.insert(“db”).table1.insert({“a”: 1}) character character varying text json xml composite default other types string db.table1.insert({“a”: “string_value”}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: “string_value”}) boolean boolean db.table1.insert({“a”:true}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: true}) double precision real serial arbitrary precision double db.table1.insert({“a”:34.56}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: 34.56}) interval timestamp data time with timezone time without timezone timestamp db.table1.insert({“a”: new Date(“2020-02-25T19:33:10Z”)}) db.table1.insert({“a”: new Date(“2020-02-25T19:33:10+06:00”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10Z”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10+06:00”)}) hex bytea bytea UUID binary data db.table1.insert({“a”: UUID(“123e4567-e89b-12d3-a456-426655440000”)}) db.table1.insert({“a”:HexData(0,”feffc2”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:UUID(“123e4567-e89b-12d3-a456-426655440000”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:HexData(0,”feffc2”)}) array array db.table1.insert({ a: [ 1, 2, 3, 4, 5 ] } ) db.table1.insert({ a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } ) session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ 1, 2, 3, 4, 5 ] } ) session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } ) 8. ConclusionMongoDB has gained a lot of popularity in recent years for its ease of development and scaling and is ideal database for data analytic purposes. Having the support to replicate data from multiple PostgreSQL clusters to a single MongoDB instance can bring a lot of value to industries focusing on data analytics and business intelligence. Building a compatible MongoDB Decoder Plugin for PostgreSQL is the first step for us and we will be sharing more information as development progresses further. The wal2mongo project is at WIP/POC stage and current work can be found here: https://github.com/HighgoSoftware/wal2mongo.","link":"/2020/03/12/Logical-Replication-Between-PostgreSQL-and-MongoDB/"},{"title":"Can Sequence Relation be Logically Replicated?","text":"1. IntroductionI have noticed that there is a page on the offical PostgreSQL documentation (https://www.postgresql.org/docs/current/logical-replication-restrictions.html) that states several restrictions to the current logical replication design. One of the restrictions is about sequence relation type where any changes associated with a sequence relation type is not logically replicated to the subscriber or to the decoding plugin. This is an interesting restriction and I took the initiative to look into this restriction further and evaluate if it is possible to have it supported. I have consulted several senior members in the PostgreSQL communitiy and got some interesting responses from them. In this blog, I will share my current work in the area of supporting sequence replication. 2. What is a Sequence?Sequence is a special type of relation that is used as a number generator manager, which allows an user to request the next number from the sequence, reset the current value, change the size of increment (or decrement) and perform several other configurations that suit their needs. A sequence is automatically created when an user creates a regular table that contains a column of type SERIAL. Alternatively, a sequence can also be created manually by using the CREATE SEQUENCE seqname; command. A sequence is similar to a regular table except that it can only contain 1 single row, is created with a special schema by default that contains several control parameters for managing the number generation and user cannot use UPDATE clause on a sequence. SQL functions such as nextval(), currval(), setval() and ALTER commands are the proper methods of accessing or modifying sequence data. 3. Why is Sequence not Replicated in Current Design?This is the question I ask myself and the PostgreSQL community for several times and I have received several interesting responses to this question. Like a regular table, sequence also emits a WAL update upon a change to the sequence value but with a major difference. Instead of emitting a WAL update at every nextval() call, sequence actually does this at every 32 increments and it logs a future value 32 increments after instead of current value. Doing WAL logging every 32 increments adds a significant gain in performance according to a benchmark report shared by the community. For example, if current sequence value is 50 with increment of 5, the value that is written to WAL record will be 210, because ( 50 + (32x5) = 210). This also means that in an events of a crash, some sequence values will be lost. Since sequence does not guarentee free of gap and is not part of user data, such a sequence loss is generally ok. Logical replication is designed to track the WAL changes and report to subscribers about the current states and values. It would be quite contradicting to replicate sequence because the current sequence value does not equal to the value stored in the WAL. The subscriber in the sequence case will receive a value that is 32 increments in the future. Another response I have got is that the implementation of sequence intermixed a bunch of transactional and non-transactional states in a very messy way, thus making it difficult to achieve sensible behaviour for logical decoding. 4. Can Sequence Relation be Logically Replicated?In the current PostgreSQL logical replication architecture, yes it is possible to have a patch to replicate changes to a sequence relation. Before we dive in further, we have to understand what the benefit would be if we were able to replicate a sequence. In the current design, an user is able to set up a PostgreSQL publisher and subscriber to replicate a table that could be associated with a sequence if it has a column of data type SERIAL. The values of the table will be copied to the subscriber of course, but the state of sequence will not. In the case of a failover, the subscriber may not be able to insert more data to the table because SERIAL data is often declared as PRIMARY KEY and it could use an unexpected sequence value that conflicts with existing records. To remedy this, PostgreSQL documentation suggests manually copying over the sequence values or use utility such as pg_dump to do the copying. I believe it is the biggest benefit if sequence relation can be replicated such that in a fail over case, the user is no longer required to manually synchronize the sequence states. 5. Where to Add the Sequence Replication Support?Logical replication actually has 2 routes, first is via the logical decoding plugin to a third party subscriber, second is between a PostgreSQL publisher and subscriber. Both routes are achieved differently in multiple source files but both do invoke the same common modules in the replication module in the PostgreSQL source repository. This section will describe briefly these common modules 5.1 Define a New Change TypeSince sequence change has some fundamental difference between the usual changes caused by INSERT, UPDATE or DELETE, it is better to define a new change type for sequence in reorderbuffer.h first: src/include/replication/reorderbuffer.henum ReorderBufferChangeType{ REORDER_BUFFER_CHANGE_INSERT, REORDER_BUFFER_CHANGE_UPDATE, REORDER_BUFFER_CHANGE_DELETE, REORDER_BUFFER_CHANGE_MESSAGE, REORDER_BUFFER_CHANGE_INTERNAL_SNAPSHOT, REORDER_BUFFER_CHANGE_INTERNAL_COMMAND_ID, REORDER_BUFFER_CHANGE_INTERNAL_TUPLECID, REORDER_BUFFER_CHANGE_INTERNAL_SPEC_INSERT, REORDER_BUFFER_CHANGE_INTERNAL_SPEC_CONFIRM, REORDER_BUFFER_CHANGE_TRUNCATE, /* added a new CHANGE TYPE */ REORDER_BUFFER_CHANGE_SEQUENCE,}; Create a new struct that stores the context data for sequence changes within the ReorderBufferChange union src/include/replication/reorderbuffer.htypedef struct ReorderBufferChange{ ... union { ... /* * Context data for Sequence changes */ struct { RelFileNode relnode; ReorderBufferTupleBuf *newtuple; } sequence; } data; ...} ReorderBufferChange; As you can see, for sequence change, we will only have the newtuple that represents the new sequence value. Old tuple is not needed here. 5.2 The Logical Decoder Module (src/backend/replication/logical/decode.c)This module decodes WAL records for the purpose of logical decoding, utilizes snapbuild module to build a fitting catalog snapshot and passes information to the reorderbuffer module for properly decoding the changes. For every WAL log read, the handle will be passed to LogicalDecodingProcessRecord for further decoding. As you can see for the type RM_SEQ_ID, there is no dedicated decoding function invoked. We should create a dedicated decoding function called DecodeSequence and update the switch statement such that the sequence type will use this decoding method. src/backend/replication/logical/decode.cvoid LogicalDecodingProcessRecord(LogicalDecodingContext *ctx, XLogReaderState *record){ ... /* cast so we get a warning when new rmgrs are added */ switch ((RmgrId) XLogRecGetRmid(record)) { ... case RM_HEAP_ID: DecodeHeapOp(ctx, &amp;buf); break; case RM_LOGICALMSG_ID: DecodeLogicalMsgOp(ctx, &amp;buf); break; /* added a new decoder function to handle the sequence type */ case RM_SEQ_ID: DecodeSequence(ctx, &amp;buf); break; ...} Now, we shall define the DecodeSequence function to actually do the decoding. Comments are embedded in the below code block to explain what each line is doing briefly. src/backend/replication/logical/decode.cstatic voidDecodeSequence(LogicalDecodingContext *ctx, XLogRecordBuffer *buf){ ReorderBufferChange *change; RelFileNode target_node; XLogReaderState *r = buf-&gt;record; char *tupledata = NULL; Size tuplelen; Size datalen = 0; uint8 info = XLogRecGetInfo(buf-&gt;record) &amp; ~XLR_INFO_MASK; /* only decode changes flagged with XLOG_SEQ_LOG */ if (info != XLOG_SEQ_LOG) return; /* only interested in our database */ XLogRecGetBlockTag(r, 0, &amp;target_node, NULL, NULL); if (target_node.dbNode != ctx-&gt;slot-&gt;data.database) return; /* output plugin doesn't look for this origin, no need to queue */ if (FilterByOrigin(ctx, XLogRecGetOrigin(r))) return; /* Obtain the change from the decoding context */ change = ReorderBufferGetChange(ctx-&gt;reorder); /* Set the new Sequence change type */ change-&gt;action = REORDER_BUFFER_CHANGE_SEQUENCE; /* Set origin of the change. Used in logical decoding plugin to filter the sources of incoming changes */ change-&gt;origin_id = XLogRecGetOrigin(r); memcpy(&amp;change-&gt;data.sequence.relnode, &amp;target_node, sizeof(RelFileNode)); /* read the entire raw tuple data as a series of char */ tupledata = XLogRecGetData(r); /* read the length of raw tuple data as a series of char */ datalen = XLogRecGetDataLen(r); /* calculate the size of actual tuple by minusing the headers */ tuplelen = datalen - SizeOfHeapHeader - sizeof(xl_seq_rec); /* allocate a new tuple */ change-&gt;data.sequence.newtuple = ReorderBufferGetTupleBuf(ctx-&gt;reorder, tuplelen); /* decode the raw tuple data and save the results as new tuple */ DecodeSeqTuple(tupledata, datalen, change-&gt;data.sequence.newtuple); /* set the catalog change, so snapbuild module will be called to build a snapshot for this sequence change */ ReorderBufferXidSetCatalogChanges(ctx-&gt;reorder, XLogRecGetXid(buf-&gt;record), buf-&gt;origptr); /* queue this change in reorderbuffer module */ ReorderBufferQueueChange(ctx-&gt;reorder, XLogRecGetXid(r), buf-&gt;origptr, change);} The above will call a new function DecodeSeqTuple to actually turn raw tuple data into a ReorderBufferTupleBuf which is needed in reorderbuffer module. This function tries to break down each section of the WAL (written by sequence.c) into a ReorderBufferTupleBuf. src/backend/replication/logical/decode.cstatic voidDecodeSeqTuple(char *data, Size len, ReorderBufferTupleBuf *tuple){ int datalen = len - sizeof(xl_seq_rec) - SizeofHeapTupleHeader; Assert(datalen &gt;= 0); tuple-&gt;tuple.t_len = datalen + SizeofHeapTupleHeader;; ItemPointerSetInvalid(&amp;tuple-&gt;tuple.t_self); tuple-&gt;tuple.t_tableOid = InvalidOid; memcpy(((char *) tuple-&gt;tuple.t_data), data + sizeof(xl_seq_rec), SizeofHeapTupleHeader); memcpy(((char *) tuple-&gt;tuple.t_data) + SizeofHeapTupleHeader, data + sizeof(xl_seq_rec) + SizeofHeapTupleHeader, datalen);} 5.3 The Reorder Buffer Module (src/backend/replication/reorderbuffer.c)reorderbuffer module receives transaction records in the order they are written to the WAL and is primarily responsible for reassembling and passing them to the logical decoding plugin (test_decoding for example) with individual changes. The ReorderBufferCommit is the last function before the change is passed down to the logical decoding plugin by calling the begin, change and commit callback handlers. This is where we will add a new logics to pass a sequence change. src/backend/replication/reorderbuffer.cvoidReorderBufferCommit(ReorderBuffer *rb, TransactionId xid, XLogRecPtr commit_lsn, XLogRecPtr end_lsn, TimestampTz commit_time, RepOriginId origin_id, XLogRecPtr origin_lsn){ ... PG_TRY(); { ... /* call the begin callback */ rb-&gt;begin(rb, txn); ReorderBufferIterTXNInit(rb, txn, &amp;iterstate); while ((change = ReorderBufferIterTXNNext(rb, iterstate)) != NULL) { Relation relation = NULL; Oid reloid; switch (change-&gt;action) { ... case REORDER_BUFFER_CHANGE_SEQUENCE: /* check on snapshot */ Assert(snapshot_now); /* get the relation oid from sequence change context */ reloid = RelidByRelfilenode(change-&gt;data.sequence.relnode.spcNode, change-&gt;data.sequence.relnode.relNode); /* check on relation oid */ if (reloid == InvalidOid) elog(ERROR, \"could not map filenode \\\"%s\\\" to relation OID\", relpathperm(change-&gt;data.tp.relnode, MAIN_FORKNUM)); /* get the relation struct from relation oid */ relation = RelationIdGetRelation(reloid); /* check on relation struct */ if (!RelationIsValid(relation)) elog(ERROR, \"could not open relation with OID %u (for filenode \\\"%s\\\")\", reloid, relpathperm(change-&gt;data.sequence.relnode, MAIN_FORKNUM)); /* call the change callback */ if (RelationIsLogicallyLogged(relation)) rb-&gt;apply_change(rb, txn, relation, change); break; } } ... /* call commit callback */ rb-&gt;commit(rb, txn, commit_lsn); ... } PG_CATCH(); { ... } PG_END_TRY();} Once the decoding plugin receives a change of type REORDER_BUFFER_CHANGE_SEQUENCE, it will need to handle it and look up the proper change context to get the tuple information contrib/test_decoding/test_decoding.cstatic voidpg_decode_change(LogicalDecodingContext *ctx, ReorderBufferTXN *txn, Relation relation, ReorderBufferChange *change){ ... switch (change-&gt;action) { case REORDER_BUFFER_CHANGE_INSERT: ... break; case REORDER_BUFFER_CHANGE_UPDATE: ... break; case REORDER_BUFFER_CHANGE_DELETE: ... break; case REORDER_BUFFER_CHANGE_SEQUENCE: /* print the sequence tuple out */ appendStringInfoString(ctx-&gt;out, \" SEQUENCE:\"); if (change-&gt;data.sequence.newtuple == NULL) appendStringInfoString(ctx-&gt;out, \" (no-tuple-data)\"); else tuple_to_stringinfo(ctx-&gt;out, tupdesc, &amp;change-&gt;data.sequence.newtuple-&gt;tuple, false); break; default: Assert(false); } ...} 6. ConclusionWe have discussed about the current implementation of logical decoding and some potential reasons why sequence is not supported in PostgreSQL logical replication. We have also gone through some important source files that could be updated to allow sequence replication. In the above approach, whenever the sequence module emits a WAL update, (which is a future value 32 increments later as discussed previously), the logical decoding plugin will receive this same future value, which is in fact different from the actual sequence value currently. This can be justified if we think about the purpose of sequence replication for a second, which is useful in fail over cases. With this future sequence value, the subsequent data insersions will be able to continue starting from this future sequence value.","link":"/2020/04/22/Can-Sequence-Relation-be-Logically-Replicated/"},{"title":"Benefits of External Key Management System Over the Internal and how these could help securing PostgreSQL","text":"1. IntroductionData and user security have always been important considerations for small to large enterprises during the deployment of their database or application servers. PostgreSQL today has rich support for many network level and user level security features. These include TLS to secure database connections, internal user authentication, integration with external user authentication services such as RADIUS, LDAP and GSSAPI, and TLS certificate based user authentication …etc. However, it does not yet support Transparent Data Encryption (TDE) feature where all the database files and logs have an option to be encrypted before written to disk or decrypted when retrieving from the disk. This adds extra security measure to protect against disk theft. All these features have something in common; they all use cryptographic keys (either symmetrical or asymmetrical, statically generated or exchanged on the fly using Diffie Hellman) in some ways to achieve the security goals. It is quite common for an organization to focus entirely on the actual data encryption part but pay minimal attention to the cryptographic keys that make the encryption possible. In fact, data encryption is the easy part, the protection of the cryptographic keys is often the hardest as it has several levels of complexities. A group of members (including myself) from the PostgreSQL community are actively working on TDE and internal KMS features towards PG14 and there has been some good work done on the internal KMS module with future support of integrating with an external KMS already in discussion. You may find the current status and the work in progress from these links below: PostgreSQL TDE Wiki PageInteral KMS for PostgreSQL Today I would like to discuss the benefits of external key management system over the internal. 2. CompliancePerhaps one of the biggest benefit of having an external KMS is compliance. For organizations that are mandated by the government to have the FIPS 140-2 compliance certification, the external KMS could potentially help them achieve FIPS 140-2 level 3 (tamper resistant key storage) certification. Internal KMS, on the other hand, may provide FIPS 140-2 level 1 (stop the use of unsafe algorithms) and at most up to FIPS 140-2 level 2 (store keys in tamper proof evident hardware). Depending on the industry governance, there may be a strict requirement that any key materials and encrypted data must be stored separately. For example, in the Payment Card Industry Data Security Standard (PCI DSS) requires that the cardholder data and encryption keys must be protected and stored separately. For this reason alone, certain data sensitive organizations cannot consider PostgreSQL as their choice of database due to lack of support to external KMS. 3. Deployment FlexibilityAnother benefit of external KMS is the flexibility in deployment. When deploying their IT infrastructure, many organizations face the decision whether to maintain all applications including key management system on site or host them in a dedeciated data center or even to the cloud. With external KMS, it is possible for these organizations to have a future proof deployment where all the cryptographic keys are centrally managed and will ensure their solution will work in any of the deployment scenarios. With internal KMS, these organizations will not have the flexibility in the deployment because the application will be tied to the storage space within itself. Taking PostgreSQL as an example, each database cluster has a different storage space. With the internal KMS, each cluster manages its own set of cryptographic keys. In a larger deployment scenario where multiple PG instances will be deployed, it will become very difficult to manage the life cycle of the cryptographic keys used in each cluster. 4. Complete Life Cycle Management for Encryption KeysNormally, a complete life cycle of an encryption key is very likely involved in the following phases. Depending on the business cases, some phases can be omitted. Key generation Key registration Key storage Key distribution and installation Key usage Key rotation Key backup Key recovery Key revocation Key suspension Key destruction It is possible for an Internal KMS to support all of the phases within its storage space and allows an user to manage the key life cycle but this process will not scale as scope requirement increases. External KMS, on the other hand, provides a centralized life cycle management of all the keys that it is managing. This is a much simpler approach to manage all the key materials in a larger deployment scenario. While it is not a security best practice, it is quite common to start a project with the internal KMS and later migrate to external one. Many of the key manager software vendors have support for key migration. 5. Security AuditThis is again a requirement for certain corporate and industry compliance where there must be a detailed audit log capturing all the key usages, rotations, who accessed the key and at what time. Depending on the requirement, certain alert mechanism may be required to be implemented to alert the key administrator about any potential issues that could rise from the cryptographic key operations. With an external KMS system, it tends to be much easier to streamline the key audit reports for all the keys it is managing and easier to prove to customers or potential auditors that the keys are indeed very securied and closely monitored. Taking the PostgreSQL as an example and it’s current work on internal KMS, it is quite difficult to prove that the keys are stored securely without the auditing mechanism to closely monitor the key usages and their lifecycles. Even if there is a complete suite of auditing mechanism in place, the auditing is still a difficult and costly operation to perform especially when there are multiple servers deployed. This would result in all storage systems to be audited individually. 6. Key Management Duty SeparationNormally, the key administrators of an external KMS has the ability to configure permissions for all the cryptographic keys that it manages. Permissions such as intended purpose, owner, validity period and other user attributes. PostgreSQL and the current work on internal KMS, on the other hand, does not have this level of granularity in adminsitrative roles. The database administrator is also the encryption key administrator. This may be an issue in certain compliance requirement like HIPAA where it is required that both roles must be separated for proper data access. 7. ConclusionExternal KMS indeed has several more benefits over the internal KMS as it provides additional management, compliance and control. It is an increasing trend that more security-conscious organiations are driving their integrations and deployments with an external KMS in the design. However, it does not mean the internal KMS should not be used at all. For smaller organziation and deployment, it is quite normal to start with the internal KMS and later migrate the key to an external KMS as the deployment gets larger in size. There is an active internal KMS development in PostgreSQL community and it can achieve basic key life cycle management and the next big focus would be on Transparent Data Encryption (TDE) and eventually an extension that supports communication to an external KMS. Key Management Interoperability Protocol (KMIP) is a communication standard set out by Organization for the Advancement of Structured Information Standards (OASIS) to enable a secured communication between key management systems and crptographically-enabled applications such as PostgreSQL. There is not many C-based open source KMIP client implementations today that can be utilized to develop a PostgreSQL extension that acts as a KMIP client to talk to external KMS, but I am sure as externa key management becomes a maintream, there will be many different versions of implementation emerging in the market.","link":"/2020/05/14/Benefits-of-External-Key-Management-System-Over-the-Internal-and-how-these-could-help-securing-PostgreSQL/"},{"title":"Understanding Security Features in PostgreSQL - Part 2","text":"1. IntroductionThis is part 2 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing TLS in greater details. I will begin by going over some of the most important security concepts around TLS before jumping into enabling TLS on PostgreSQL server. I believe it is crucial to have sufficient background information on TLS before tweaking the TLS settings in both client and server sides. In part 1 of this blog, we mostly discussed about authentication and authorization (AA), which is important to identify which client is permitted to connect and which table or column he/she is permitted to operate. Even with the strongest authentication and authorization, the actual communication between client and server will not be encrypted unless Transport Layer Security (TLS) is specifically enabled in the database server. TLS is one of the least understood but commonly used security protocol that ensures the security of many HTTPS sites and other services. TLS is a big protocol and this blog will describe how it works and how to enable TLS in your PostgreSQL server. Here is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. Security Concepts around TLSBefore we jump into configuring TLS in PostgreSQL. It is super important to have some background information on the following security topics build around TLS. 2.1 Symmetrical EncryptionSymmetrical Encryption is a type of encryption where only one secret key is used to encrypt and decrypt a message. In other words, the connecting client will use the secret key to encrypt the message and send to server, the server uses the same key to decrypt the ciphered message and obtain the original message. This is a very fast encryption operation and may sound simple, but the challenge here is how to securely share this one and only secret key between the client and server, how long should the secret key be used before next rotation? Should the secret key be pre-configured on both client and server sides? Should third-party key management software be integrated? These are some of the common challenges with symmetrical encryption. The following is some of the most common symmetrical encryption algorithms today with the AES being the most popular: (reference: https://en.wikipedia.org/wiki/Symmetric-key_algorithm). Each algorithm supports key lengths having multiple sizes and normally is denoted after the encryption algorithm name, for example, AES-128, AES-256…etc. AES (Advanced Encryption Standard) DES (Data Encryption Standard) Triple DES Blowfish Symmetrical encryption is normally paired with a Block Cipher Mode of Operation to encrypt or decrypt a stream of data. Imagine there is a data stream of size 30GB that needs to be encrypted. Without Block Cipher Mode, we will have to load all 30GB of data into memory and encrypt it with (say AES128) and most likely we do not large enough memory to load all the data stream. This is where Block Cipher Mode of Operations come in handy, it encrypts the data stream block by block (most likely 16 byte block) until the entire block is encrypted. We basically can encrypt the 30GB data stream without having to have at least 30GB of memory. 2.2 Asymmetrical Encryption (a.k.a Public Key Cryptography)Unlike symmetrical encryption, asymmstrical encryption uses two distinct keys called public and private keys; Public key is used for encryption and private key is used for decryption. Both keys are different but related by math and it is much slower than symmetrical encryptions. As name implies, public key can be distributed publicly while private key is to be kept private as it is the only key that is able to decrypt the messages encrypted by public key. This essentially forms a secured one-way communication. Generally, asymmetrical encryption is not desirable to be used as stream data encryption algorithm though it is more secured; it requires more computational power to perform encryption and decryption and this is a major drawback. Asymmetrical encryption is commonly used as authentication protocol for a client to verify that server is indeed valid. During a TLS handshake for example, server will present its TLS certificate, which contains a public key, to the client, client uses the public key to encrypt a message and asks the server to decrypt with its private key and send back the result. If message match, then client is sure that the server possess the private key and therefore is valid. The following is some of the most common asymmetrical encryption algorithms today with the RSA and Elliptic curve being the most popular: (reference: https://en.wikipedia.org/wiki/Public-key_cryptography). RSA DSS Elliptic curve 2.3 Block Cipher Mode of Operation (a.k.a Stream Cipher)Block Cipher Mode of Operation is normally used with Symmetrical encryption to encrypt or decrypt a stream of data block by block. There are several available modes of block cipher operations that have different strengths and weaknesses. Most modes require a complete block of 16 bytes to be able to encrypt. In the case where the input stream is not in multiple of 16, padding is normally use to fill the block. The following is some of the most common block cipher mode of operations today with the CBC and CTR being the most popular and ECB being the least secured: (reference: https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation). Cipher Block Chaining (CBC) Counter (CTR) Cipher Feedback (CFB) Output Feedback (OFB) Electronic Codebook (ECB) The mode is normally denoted with the desired symmetrical encryption algorithm, for example, AES-128-CBC or AES-256-CTR are quite common. 2.4 Key Exchange AlgorithmKey exchange algorithm is a math algorithm designed to make both client and server agree on a secret key without actually sending the key to each other. This is done by pure math equations and require several steps of intermediate token exchange. In the end both client and server will end up with the same value, which is to be used as the secret key for symmetrical encryption algorithms. Key exchange algorithm is common used in many services such as SSH and TLS. Services like these normally have a handshake stage where both client and server have to agree on the subsequent algorithms to use for encryption and perform key exchange algorithm to get the secret key for symmetrical encryption. The following is some of the most common key exchange algorithms with diffie-hellman and elliptic curve diffie-hellman being the most popular (reference: https://en.wikipedia.org/wiki/Key_exchange). Diffie-Hellman (DH) Elliptic Curve Diffie-Hellman (ECDH) Ephemeral Diffie-Hellman (DHE) RSA 2.6 Data Integrity Check / AuthenticationThe data integrity authentication is not to be confused with host-based or role-based authentication mentioned in part 1. Data integrity authentication refers to the methods to ensure that the data stream has been received without being altered during transmission. Think of it as a data checksum. In addition to encryption, ensuring data integrity is also very important security measure to avoid man-in-the-middle attack. Please note that data integrity check and data encryption are 2 separate processes, meaning that you can have data authentication without encryption, or encryption without authentication. SNMPv3 is a good example that treasts data authentication and encryption separtely while TLS requires both at the same time. The following is some of the most common hash algorithms with SHA1 and MD5 being the most common (reference: https://en.wikipedia.org/wiki/Message_authentication). SHA1 SHA2 MD5 BLAK2 2.5 TLS Certificate and Chain of TrustTLS certificate and chain of trust are the core concepts in TLS to ensure maximum trust between a client and a server. The certificate used by PostgreSQL is X509 version 3 certificate, which has extension support to further refine the purpose of the certificate issued. The certificates are created and signed in hierarchy. The certificate created at the top hierarchy is called a root CA (root Certificate Authority) and is normally created by a trusted organization. This root CA is able to sign additional Intermediate CA that can be distributed to other organizations. The intermediate CA can then be used to create and sign individual certificates to be used by services like HTTPS, FTPS…etc. There are several types of TLS certificate and each has its own place in the certificate hierarchy and serve different purposes. A TLS certificate is a small data file that contains the public key, organization details, trustee’s digital signature, extensions and validity dates. Normally a TLS certificate is generated with a private key. The key pair bounded with the certificate is important as they are required for authentication when a TLS client wishes to connect to the server. The following image illustrates the idea of certificate trust chain: As you can see, the root CA is on top of hierarchy and is able to generate and sign additional intermediate CA and issue to several organizations. The organization then is able to take the intermediate CA and generate additional CA-signed certificates and matching private keys to use in their services such as PostgreSQL server, FTPS and HTTPS server. A CA certificate can be purchased from a trusted organization or generated by oneself using openssl and java key tool. We will go over the procedure to generate these certificates using OpenSSL as examples in part 3 of this blog. 2.7 TLS versionsTLS is a newer protocol that replaces its predecessor, Secured Sockets Layer (SSL). Below is a list of TLS versions that we should use as of today: TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3 TLSv1.3 is the newest TLS version that has significant improvement in the handshake process and introduces many more cipher suites specifically designed for TLSv1.3. Before TLSv1.3, TLSv1.2 is the most popular TLS version deployed in the world today. PostgreSQL server defaults to accept client connection that supports minimum TLS version to be TLSv1.2 and will reject any connection in the versions earlier than v1.2 2.8 TLS Cipher Suite and TLS handshakeTLS cipher suite refers to a set of algorithms that help secure a network connection. The suite of algorithms normally contains Key exchange algorithm Authentication algorithm Asymmetrical encryption algorithm Message authentication algorithm for example, a TLSv1.2 cipher suite TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 indicates the following DHE - use Ephemeral Diffie-Hellman key exchange algorithm RSA - use RSA asymmetrical keys for authentication AES_256_CBC - use AES-256 symmetrical encryption with CBC block cipher mode SHA256 - use SHA-256 as message authentication algorithm to make sure exchanged messages are not tempered with. When a TLS client initiates a TLS connection to a server, a TLS handshake process takes place that roughly performs the following: Agree on the TLS version to use. Abort if version cannot be agreed Agree on the cipher suite to use. Abort if cipher suite cannot be agreed Certificate exchange Client authenticates the server using agreed algorithm perform key exchange using agreed algorithm ensure handshake message is not tempered with the agreed message authentication algorithm secured communication then begins. 7. SummaryTLS is a big protocol involving a lot of steps including certificate exchange, chain of trust verification, key exchange, cipher suite exchange, authentication, data integrity check and finally symmetrical encryption of application data with appropriate block cipher mode. Having adequate fundamental understanding to TLS is crucial to ensure a correct and secured database environment setup. Of course there is more to what we have discussed here so far and I will be producing more articles in the near future to address some of the advanced TLS related practices.","link":"/2020/01/13/Understanding-Security-Features-in-PostgreSQL-Part2/"},{"title":"Understanding Security Features in PostgreSQL - Part 3","text":"1. IntroductionThis is part 3 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing how to apply TLS in both PostgreSQL server and client using the principles we have learned in part 2 of the blog. In the end, I will also briefly talk about Transparent Data Encryption (TDE) and security vulnerability. Here is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. Preparing TLS CertificatesBefore we can utilize TLS to secure both the server and the client, we must prepare a set of TLS certificates to ensure mutual trust. Normally the CA (Certificate Authority) certificates can be purchased from a trusted organization and used it to create more CA-Signed certificates for services and applications. In this section, I will show you how to create your own CA Certificate and CA-Signed certificates using OpenSSL command line tool for both PostgreSQL server and client. You may also have heard the term self-signed certificate. This type of certificate is not signed by a trusted CA and is normally considered insecured in many applications. We will not go over the self-signed certificate generation in this blog. 2.1 Generate a Private Key for CA CertificateRemember in last blog we mention that each certificate contains organization information and public key, which is paired with a private key file. Let’s generate a private key file for our CA first. $ openssl genrsa -des3 -out cacert.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)...............+++++........................+++++e is 65537 (0x010001)Enter pass phrase for cacert.key:Verifying - Enter pass phrase for cacert.key: Your will be prompted with pass phrase, which is recommended to provide as it will prevent someone else from generating more root CA certificate from this key. 2.2 Generate CA Certificate Using the Private keyNow, let’s generate the CA Certificate with the private key $ openssl req -x509 -new -nodes -key cacert.key -sha256 -days 3650 -out cacert.pemEnter pass phrase for cacert.key:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CAState or Province Name (full name) [Some-State]:BCLocality Name (eg, city) []:VancouverOrganization Name (eg, company) [Internet Widgits Pty Ltd]:HighGoOrganizational Unit Name (eg, section) []:SoftwareCommon Name (e.g. server FQDN or YOUR name) []:va.highgo.comEmail Address []:cary.huang@highgo.ca Please note that OpenSSL will prompt you to enter several pieces of organizational information that identifies the CA certificate. You should enter these information suited to your organization. The most important field is Common Name, which is commonly checked against the hostname or domain name of the service. Depending on the seurity policy, some server will enforce the rule that common name must equal its host / domain name; some servers do not have this restriction. 2.3 Generate a private key for CA-Signed certificateLike in the CA case, CA-signed certificate is also paired with a private key file $ openssl genrsa -out server.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)...................+++++............................................................................+++++e is 65537 (0x010001) 2.4 Generate a Certificate Signing Request for CA-Signed certificateThen we create a Certificate Signing Request (CSR), which contains a list of organizational information to be presented to the CA server for verification. The CA server then decide if the CSR should be granted a new certificate according to the security policy configured. Since we are using OpenSSL for certificate generation, the CA server here refers to OpenSSL itself, and the security policy configuration is located in openssl.cnf, which is commonly located in /usr/local/ssl/openssl.cnf. In an enterprise environment where Public Key Infrastructure (PKI) is deployed, the CA Server could refer to an actual service whose sole purpose is to verify incoming CSRs and renew or issue new certificates to requesting clients. $ openssl req -new -key server.key -out server.csrYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CAState or Province Name (full name) [Some-State]:BCLocality Name (eg, city) []:VancouverOrganization Name (eg, company) [Internet Widgits Pty Ltd]:HighGoOrganizational Unit Name (eg, section) []:SoftwareCommon Name (e.g. server FQDN or YOUR name) []:va.highgo.caEmail Address []:cary.huang@highgo.caPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:HighGo Canada 2.5 Generate a CA-Signed certificateSince we are generating CA-signed certificate with OpenSSL locally, we can configure how the certificate should be generated using openssl.cnf file. We will just be using the default policty set in openssl.cnf. Here’s a snapshot of the default settings: [ usr_cert ]# These extensions are added when 'ca' signs a request.# This goes against PKIX guidelines but some CAs do it and some software# requires this to avoid interpreting an end user certificate as a CA.basicConstraints=CA:FALSE# Here are some examples of the usage of nsCertType. If it is omitted# the certificate can be used for anything *except* object signing.# This is OK for an SSL server.# nsCertType = server# For an object signing certificate this would be used.# nsCertType = objsign# For normal client use this is typical# nsCertType = client, email# and for everything including object signing:# nsCertType = client, email, objsign# This is typical in keyUsage for a client certificate.# keyUsage = nonRepudiation, digitalSignature, keyEncipherment Let’s generate the CA-signed certificate. Note that the command will take cacert.pem, cacert.key and server.csr as inputs, in which we have already generated from previous steps. server.pem will be the output. $ openssl x509 -req -in server.csr -CA cacert.pem -CAkey cacert.key -CAcreateserial -out server.pem -days 3650 -sha256Signature oksubject=C = CA, ST = BC, L = Vancouver, O = HighGo, OU = Software, CN = va.highgo.ca, emailAddress = cary.huang@highgo.caGetting CA Private KeyEnter pass phrase for cacert.key: We can repeat from step 2.3 to 2.5 to generate a new pair for the client application. To conclude, we have the following files generated: cacert.pem - Root CA certificate that is at the top of the chain of trust. We use it to sign and create other certificates cacert.key - key for the Root CA Certificate - must keep it secured. server.pem - CA-signed certificate for server application server.key - key for the server certificate client.pem - CA-signed certificate for client application client.key - key for the client certificate 3. Enabling Transport Layer Security (TLS) to PostgreSQL ServerPostgreSQL has native support for TLS to secure connection between client and server. The TLS support has to be enabled during build time and requires OpenSSL libraries. Depending on the versions of OpenSSL that the client or server is built with, TLS versions and ciphersuites may differ as well. This does not mean that both client and server must be linked with the same version of OpenSSL. It is possible that a client with older OpenSSL can connect to a server with newer OpenSSL if the server is configured to accept it. The TLS handshake process is initiated when a client first connects to the server in which they will evaluate TLS version used and negotiate ciphersuite that both ends are able to support. In this case, the server may use less secured ciphersuite and TLS version to communiate with the client, which may not be ideal. The TLS support for a PostgreSQL server can be enabled in postgresql.conf. postgresql.confssl = onssl_ca_file = '~/cert/cacert.pem'ssl_cert_file = '~/cert/server.pem'ssl_crl_file = ''ssl_key_file = '~/cert/server.key'ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphersssl_prefer_server_ciphers = onssl_ecdh_curve = 'prime256v1'ssl_min_protocol_version = 'TLSv1.2'ssl_max_protocol_version = ''ssl_dh_params_file = ''ssl_passphrase_command = ''ssl_passphrase_command_supports_reload = off Let’s examine the configuration parameters. ssl = on This line turns on the TLS support. Please note that even if TLS is turned on, the server will still be able to accept connections that do not use TLS. Normally, the client is the entity that decides if TLS should be used or not. The server can also enforce the incoming connections to use TLS by modifying the pg_hba.conf file like this, where the connections from 172.16.30.0/24 must be TLS, otherwise the server will deny. hostssl sales_team all 172.16.30.0/24 trust ssl_ca_file = '~/cert/cacert.pem'ssl_cert_file = '~/cert/server.pem'ssl_crl_file = ''ssl_key_file = '~/cert/server.key' These 4 lines tell PostgreSQL where to load the X509 certificate, the CA certificate, server private key and the certificate revokation list. These certificates must be pre-generated by OpenSSL command or purchased from a trusted organization. For TLS to work, ssl_ca_file, ssl_cert_file and ssl_key_file must be provided. We will use the certificates we have generated for server in the previous section. The file pointed by ssl_ca_file will be used to determined if the certificate can be trusted by deriving the chain of trust.The file pointed by ssl_cert_file will be sent to the connecting client during TLS handshake for authentication purposes.The file pointed by ssl_key_file will be used for asymmetrical encryption during authentication The file pointed by ssl_crl_file is optional and it contains a list of certificates that cannot be trusted (or revoked). Distributing revoked certificates using this file is not the most ideal but still being practice today. It may have performance impact if the list is very large and it introduces a problem of when the list should be renewed and how often. Online Certificate Status Protocol (OCSP. ref:https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol) is a newer protocol designed for Public Key Infrastructure (PKI) for querying certificate revokation status that addresses some of the issues with revokation file. Feel free to give a read on OCSP in the link above. ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphersssl_prefer_server_ciphers = onssl_ecdh_curve = 'prime256v1' During TLS handshake, both client and server will present to each other a list of desired ciphersuites ordered by preference. Handshake process will go through both lists and find a common ciphersuite supported by both sides or abord if there is nothing in common. The ssl_ciphers configuration is used to configure the size of the ciphersuite lists to be presented to the client during handshake. ssl_ciphers is a string list consisting of one or more cipher strings separated by colons ( ref: https://www.openssl.org/docs/man1.1.1/man1/ciphers.html) and defaults to HIGH:MEDIUM:+3DES:!aNULL which translates to: allows high strength ciphersuites (HIGH) allows medium strength ciphersuites (MEDIUM) move any ciphersuite using 3DES algorithm to the end of the list (+3DES) remove any ciphersuite that does not have authentication algorithm (!aNULL) For example, “HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256” will use high strength ciphersuites while removing any ciphersuites containing ADH, MD5, RC4…etc. Before applying the cipher string to PostgreSQL, it is recommended to check the output cipher list after tuning the cipher string using Openssl client tool. $ openssl ciphers -v 'HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256'TLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEADTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEADTLS_AES_128_GCM_SHA256 TLSv1.3 Kx=any Au=any Enc=AESGCM(128) Mac=AEADAES256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(256) Mac=AEADAES256-CCM8 TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM8(256) Mac=AEADAES256-CCM TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM(256) Mac=AEADARIA256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=ARIAGCM(256) Mac=AEADAES128-GCM-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(128) Mac=AEADAES128-CCM8 TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM8(128) Mac=AEADAES128-CCM TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM(128) Mac=AEADARIA128-GCM-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=ARIAGCM(128) Mac=AEADAES256-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA256AES128-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA256CAMELLIA128-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=Camellia(128) Mac=SHA256AES256-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA1AES128-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA1CAMELLIA128-SHA SSLv3 Kx=RSA Au=RSA Enc=Camellia(128) Mac=SHA1 ssl_prefer_server_ciphers specifies whether to use the server’s SSL cipher preferences, rather than the client’s. It should always be on for more control in terms of ciphersuite selection. ssl_ecdh_curve specifies the name of the curve to use in ECDH key exchange algorithms and is useful only if the ciphersuite uses ECDHE key exchange algorithm. The most common curves are : prime256v1, secp384r1 and secp521r1 and normally leaving it default should suffice. ssl_min_protocol_version = 'TLSv1.2'ssl_max_protocol_version = '' These 2 lines configure the minimum and maximun TLS versions to accept. By default the server will only serve the TLS client using TLSv1.2 and above. TLSv1.2 is a very secured TLS version and it is widely used in the world. Normally, we only change the minimum TLS version with assumption that all future versions will be more secured and for this reason, we normally don’t put restriction on the max version. TLSv1.3 is recently introduced that has new ciphersuite support and has more improvement in the handshake process. To enforce TLSv1.3 to be used, set the ssl_min_protocol_version to ‘TLSv1.3’ will suffice. #ssl_dh_params_file = ''#ssl_passphrase_command = ''#ssl_passphrase_command_supports_reload = off ssl_dh_params_file points to a file that contains custom diffie-hellman key exchange algorithm parameter. This is an optional parameter and is only useful if the ciphersuite uses DHE key exchange algorithm. If left empty, compiled-in defaults will be used. Custom DH parameters can be generated using command openssl dhparam -out dhparams.pem 2048 and will normally reduce the attack exposure as attacker will have hardtime cracking the key exchange process using custom parameter instead of the well-known default. ssl_passphrase_command is the command to obtain the password for the private key file specified by ssl_key_file. There is an option to add a password to a private key file during its generation and if password is used, ssl_passphrase_command must be set with the system command that will retrieve such password. Otherwise, TLS handshake will abord as PostgreSQL will not be able to access private key without password. ssl_passphrase_command_supports_reload configures if the ssl_passphrase_command should be re-run at every reload (ie. SIGHUP). It is default to off, so the ssl_passphrase_command will not be run at every reload. 4. Enabling Transport Layer Security (TLS) to PostgreSQL ClientNow that we have a PostgreSQL server with TLS setup, we can use psql client to connect to the server also using TLS. Depending on the client connect parameters given, we can utilize TLS in different security levels. I will show the most common usages here: # Case 1: connect to server in TLS mode$ psql -U user -h localhost -d \"sslmode=require dbname=postgres\"# Case 2: connect to server in TLS mode if server supports it$ psql -U user -h localhost -d \"sslmode=prefer dbname=postgres\"# Case 3: connect to server in TLS mode and verify server CA against client CA$ psql -U user -h localhost -d \"sslmode=verify-ca dbname=postgres sslrootcert=~/cert/cacert.pem\"# Case 4: connect to server in TLS mode and present client certificate. Verify all certificate details and trust chain. Check certificate revokation list does not contain server cert.$ psql -U user -h localhost -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\" The usage in Case 4 is the most secured becuase both server and client will verify each other’s certificate and decide if both can be mutually trusted. The common name field in the certificate is checked against the server hostname; certificate validity period is checked, organization details are checked; certificate trust chain is checked; revokation list is checked. Please note that PostgreSQL server with TLS enabled by default does not force the client to present a TLS certificate for verification. If client presents one like in Case 4 above, the server will verify and deny connection is certificate is bad. If client does not provide a certificate like in Case 1 ~ 3, the server will skip the client certificate verification as there is nothing to verify, which is less secure. To enforce the connecting client to present a TLS certificate for verification, we will need to add a special clientcert=1 argument in existing authentication rules defined in pg_hba.conf. # TYPE DATABASE USER ADDRESS METHODhostssl production_team production_user 0.0.0.0/0 pam clientcert=1 The example above will enforce connecting client to present TLS certificate to access production_team database as production_user. If a TLS certificate is not provided by client, the connection will abort. 5. TLS Connect Examples$ psql -U user -h localhost -d \"sslmode=require dbname=postgres\"psql (13devel)SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)Type \"help\" for help.postgres=# Please note that psql prints the TLS version used (TLSv1.2) and the cipher suite negotiated during handshake (ECDHE-RSA-AES256-GCM-SHA384). Below is the wireshark capture of the above TLS connection: Another Example: $ psql -U cary -h localhost -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\"psql: error: could not connect to server: server certificate for \"va.highgo.ca\" does not match host name \"localhost\" Here, we have an error when we set sslmode to verify-full, where both server and client will verify each other with the most strict criteria. This error happens because the Common Name field in the certificate does not match the host name. Did I mention that Common Name is the most important field of a certificate? To resolve this error, we can either re-generate certificate with matching Common name, or change the host name. I simply add an entry to /etc/hosts to resolve the error 127.0.0.1 localhost127.0.0.1 va.highgo.ca and the error will disappear when both Common Name and Hostname match $ psql -U cary -h va.highgo.ca -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\"psql (13devel)SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)Type \"help\" for help. Please note that this command also forces the client to submit a certificate to server as well as seen from the wireshark capture. We can tell by looking at the length field of the packet capture. There are 2 exchanges having lengths = 2675 and 2446. Those are the actual certificate contents being transmitted. Previous capture only has 1 exchanges having packet length = 2675; it means only server is providing certificate to client for verification. 6. Transparent Data Encryption (TDE)Transparent Data Encryption refers to the process of protecting data at rest by encrypting database files on the hard disk level and decrypting them while reading from hard disk. This is to prevent physical storage media theft. This is called transparent because the encryption and decryption happen between PostgreSQL server and the physical hard disk and it is not visible to the client applications. TDE uses symmetrical encryption for securing blocks of database files such as shared buffer and WAL files, and it is designed to accompany with a internal Key Management System (KMS) to manage the lifecycle of the encryption keys. TDE and KMS are still under development by the PostgreSQL community. The KMS feature is expected to be released in PG13 while the TDE feature to be in PG14. With its completion, it will add another layer of security feature on top of already security-rich PostgreSQL database. 7. Security VulnerabilitySecurity Vulnerability is a weakness which can be exploited by an attacker to perform unauthorized actions, sabotage a service, or inject malicious software or virus. These weaknesses are generally implementation mistakes, undiscovered bugs or a legacy problem that require an update to the server to resolve. PostgreSQL also has a list of known security vulnerability that has been discovered and fixed by the community. The list can be found here: https://www.postgresql.org/support/security/. These vulnerability ranges from different severity levels, from simple memory leak to crash the server. This is why doing regular PostgreSQL server upgrade is important because each minor release fixes some of the discovered security vulnerabilities and therefore reducing the attack surface on your server. 8. SummaryIn part 3 of the blog, we have learned and understood what each TLS related configuration means in postgresql.conf and how to initiate TLS connection with psql client. We learned that keeping PostgreSQL server up-to-date can reduce the attack surface on some of the discovered vulnerabilities. We can ensure a fairly secured database network environment with TLS having adequate understanding of its fundamentals and practices. With the TDE feauture coming in near future, we can further secure the database envrionment in the disk level and prevent possible data loss due to disk theft.","link":"/2020/01/20/Understanding-Security-Features-in-PostgreSQL-Part3/"}],"tags":[{"name":"snmp","slug":"snmp","link":"/tags/snmp/"},{"name":"networking","slug":"networking","link":"/tags/networking/"},{"name":"oid","slug":"oid","link":"/tags/oid/"},{"name":"partition","slug":"partition","link":"/tags/partition/"},{"name":"trigger","slug":"trigger","link":"/tags/trigger/"},{"name":"replication","slug":"replication","link":"/tags/replication/"},{"name":"pg_rewind","slug":"pg-rewind","link":"/tags/pg-rewind/"},{"name":"failover","slug":"failover","link":"/tags/failover/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"extension","slug":"extension","link":"/tags/extension/"},{"name":"gdb","slug":"gdb","link":"/tags/gdb/"},{"name":"trace","slug":"trace","link":"/tags/trace/"},{"name":"query processing","slug":"query-processing","link":"/tags/query-processing/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"wal2mongo","slug":"wal2mongo","link":"/tags/wal2mongo/"},{"name":"logical replication","slug":"logical-replication","link":"/tags/logical-replication/"},{"name":"logical decoding","slug":"logical-decoding","link":"/tags/logical-decoding/"},{"name":"sequence","slug":"sequence","link":"/tags/sequence/"},{"name":"key management system","slug":"key-management-system","link":"/tags/key-management-system/"},{"name":"external key management system","slug":"external-key-management-system","link":"/tags/external-key-management-system/"},{"name":"tde","slug":"tde","link":"/tags/tde/"}],"categories":[{"name":"Netsnmp","slug":"Netsnmp","link":"/categories/Netsnmp/"},{"name":"PostgreSQL","slug":"PostgreSQL","link":"/categories/PostgreSQL/"}]}