{"pages":[{"title":"About Cary","text":"Cary is a multi-discpline software developer with 8 years of industrial experience developing innovative software solutions in C/C++ in the field of smart grid &amp; metering. He holds a bachelor degree in Electrical Engineering from University of British Columnbia (UBC) in Vancouver in 2012 and has been employed by Corinex Communication Corp. since then. He has extensive hands-on experience in technologies such as: Advanced Networking, Network &amp; Data security, Smart Metering Innovations, deployment management with Docker, Software Engineering Lifecycle, scalability, authentication, cryptology, relational &amp; non-relational database, web services, firewalls, embedded systems, RTOS, ARM, PKI, Cisco equipment, functional and Architecture Design with UML. Cary is a new member to the PostgreSQL community and has been contributing in terms of publishing technical blogs and performing patch reviews. He is actively involved in the development of Transparent Data Encryption (TDE) and internal Key Management System (KMS) features that are targeted to be released in the future PostgreSQL version 13 and 14. Putting the technical backgrounds aside. Cary is also a Forex trader, who automates most of his trading techniques and criteria with software. His trading criteria is rather simple. He trades mostly according to the price action and candle stick patterns as these reflect the true intent of the market. Cary is also a e-commerce entrepreneur who operates 2 online stores, “Shokunin Design Co” on Etsy.com and “Simplicity Design Inc” on Amazon.com. Check them out at the “links”. He is seasoned wood crafter, who designs, constructs and sells wooden hexagon shelves for home decoration under the “Shokunin Design Co” brand and the owner of baby product brand “Mini4More” under “Simplicity Design Inc” store front on Amazon. Cary is a believer that Your mind is a very powerful weapon; it can be your best friend, or your worst enemy. Having a positive mindset is absolutely crucial to your own success.","link":"/about/me.html"}],"posts":[{"title":"A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres","text":"1. OverviewPostgres is a huge database system consisting of a wide range of built-in data types, functions, features and operators that can be utilized to solve many common to complex problems. However, in the world full of complex problems, sometimes these are just not enough depending on the use case complexities. Worry not, since Postgres version 9, it is possible to extend Postgres’s existing functionalities with the use of “extensions” In this article, I will show you how to create your own extensions and add to Postgres. Please note that this article is based on Postgres version 12 running on Ubuntu 18.04 and before you can create your own extensions, PG must have been built and installed first 2. Built-in ExtensionsBefore we jump into creating your own extensions, it is important to know that there is already a list of extensions available from the PG community included in the Postgres software distribution. The detailed information of community supplied extensions can be found in this link: https://www.postgresql.org/docs/9.1/contrib.html 3. Build and Install PG Default ExtensionsAll the PG community extensions are located in the directory below. This is also where we will be adding our own extensions $PG_SOURCE_DIR/postgres/contrib where [PG SOURCE DIR] is the directory to your PG source code These modules are not built automatically unless you build the ‘world’ target. To Manually build and install them, use these commands. cd contribmakesudo make install The above command will install the extensions to $SHAREDIR/extension and required C shared libraries to $LIBDIR where $SHAREDIR and $LIBDIR are the values returned by pg_config For the extensions that utilize the C language as implementation, there will be a C shared libraries (.so) being produced by the make command. This C shared library contains all the methods supported by the extension. With default extensions and libraries installed, we can then see the installed extensions by the following queries SELECT pg_available_extensions();SELECT pg_available_extension_versions(); 4. Create Extension Using plpqsql LanguageFor this example, we will create a very simple extension that will count the number of specified character of a given string. This extension takes 2 input arguments, first being the string, and second being the desired character. It will return an integer indicating the number of occurance of the desired characters presented in the string first, let’s navigate to the contrib directory to add our extension cd [PG SOURCE DIR]/contrib let’s create a new directory called char_count. This will be the name of the extension mkdir char_countcd char_count create the folders for defining testcases later mkdir sqlmkdir expected create and an extension control file using this naming convention: [Extension name].control char_count.control# char_count extensioncomment = 'function to count number of specified characters'default_version = '1.0'module_pathname = '$libdir/char_count'relocatable = true create a data sql file using this naming convention: [Extension name]--[Extension version].sql char_count--1.0.sql\\echo Use \"CREATE EXTENSION char_count\" to load this file. \\quitCREATE FUNCTION char_count(TEXT, CHAR)RETURNS INTEGERLANGUAGE plpgsql IMMUTABLE STRICT AS $$ DECLARE charCount INTEGER := 0; i INTEGER := 0; inputText TEXT := $1; targetChar CHAR := $2; BEGIN WHILE i &lt;= length(inputText) LOOP IF substring( inputText from i for 1) = targetChar THEN charCount := charCount + 1; END IF; i := i + 1; END LOOP; RETURN(charCount); END; $$; Please note that the first echo line enforces the function to be loaded as extension Create a Makefile Makefile# contrib/char_count/MakefileEXTENSION = char_countDATA = char_count--1.0.sqlPGFILEDESC = \"char_count - count number of specified character\"REGRESS = char_countifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/char_counttop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif With the files in place ,we can go ahread and run within the char_count extension folder sudo make install This will install char_count extension to $SHAREDIR Now we can connect to the PG server and make use of the new extension that we have just added: 5. Create a Test Case for the New ExtensionWe have already created a sql folder from previous steps, let’s create a new .sql file for our test case char_count.sqlCREATE EXTENSION char_count;SELECT char_count('aaaabbbbbbbcc','a');SELECT char_count('aaaabbbbbbbcc','b');SELECT char_count('aaaabbbbbbbcc','c');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','x');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','c');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','b');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','5');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','3');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','2');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','1');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','0');SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','asd'); Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line: REGRESS = char_count Run the testcase and Obtain Results make installcheck For the first time, the regression test will fail, because we have not provided the expected output file (.out file) for the test case. A new folder “results” is created upon running the regression test, and there is a (.out) file inside containing all the output from the test case CREATE EXTENSION char_count;SELECT char_count('aaaabbbbbbbcc','a'); char_count ------------ 4(1 row)SELECT char_count('aaaabbbbbbbcc','b'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc','c'); char_count ------------ 2(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','x'); char_count ------------ 0(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','c'); char_count ------------ 2(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','b'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','5'); char_count ------------ 5(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','3'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','2'); char_count ------------ 7(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','1'); char_count ------------ 4(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','0'); char_count ------------ 1(1 row)SELECT char_count('aaaabbbbbbbcc1111222222233333335555590','asd');ERROR: value too long for type character(1)CONTEXT: PL/pgSQL function char_count(text,character) line 7 during statement block local variable initialization We should examine this .out file and made sure the outputs are all correct and we will copy it over to the expected folder cp char_count/results/char_count.out char_count/expected 6. Create your Own Extension Using C LanguageIn the previous section, we created a extension using plpgsql function language. This is in many ways very similar to the ‘CREATE FUNCTION’ commands except that in the above example, we specifically states that the function can only be loaded through the CREATE EXTENSION command. In most cases, the custom extensions are mostly built in C codes because of its flexibility and performance benefits. To demonstrate this, we will create a new extension called char_count_c. Let’s repeat some of the process above: cd [PG_SOURCE_DIR]/contribmkdir char_count_ccd char_count_cmkdir expectedmkdir sql create a control file: char_count_c.control# char_count_c extensioncomment = 'c function to count number of specified characters'default_version = '1.0'module_pathname = '$libdir/char_count_c'relocatable = true create a data sql file char_count_c--1.0.sql\\echo Use \"CREATE EXTENSION char_count\" to load this file. \\quitCREATE FUNCTION char_count_c(TEXT, TEXT) RETURNS INTEGERAS '$libdir/char_count_c'LANGUAGE C IMMUTABLE STRICT This is where it differs from the previous method to add extension. In here we specifically set the LANGUAGE to be C as oppose to plpgsql. $libdir/char_count_c is important as this is the path in which the PG will try to find a corresponding C share library when char_count_c extension is loaded. Now, create a Makefile MakefileMODULES = char_count_cEXTENSION = char_count_cDATA = char_count_c--1.0.sqlPGFILEDESC = \"char_count_c - count number of specified character\"REGRESS = char_count_cifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/char_count_ctop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif Here we added a new line called MODULES = char_count_c. This line will actually compile your C code into a shared library (.so) file which will be used by PG when char_count_c extension is loaded. Create a new C source file char_count_c.c#include \"postgres.h\"#include \"fmgr.h\"#include \"utils/builtins.h\"PG_MODULE_MAGIC;PG_FUNCTION_INFO_V1(char_count_c);Datumchar_count_c(PG_FUNCTION_ARGS){ int charCount = 0; int i = 0; text * inputText = PG_GETARG_TEXT_PP(0); text * targetChar = PG_GETARG_TEXT_PP(1); int inputText_sz = VARSIZE(inputText)-VARHDRSZ; int targetChar_sz = VARSIZE(targetChar)-VARHDRSZ; char * cp_inputText = NULL; char * cp_targetChar = NULL; if ( targetChar_sz &gt; 1 ) { elog(ERROR, \"arg1 must be 1 char long\"); } cp_inputText = (char *) palloc ( inputText_sz + 1); cp_targetChar = (char *) palloc ( targetChar_sz + 1); memcpy(cp_inputText, VARDATA(inputText), inputText_sz); memcpy(cp_targetChar, VARDATA(targetChar), targetChar_sz); elog(INFO, \"arg0 length is %d, value %s\", (int)strlen(cp_inputText), cp_inputText ); elog(INFO, \"arg1 length is %d, value %s\", (int)strlen(cp_targetChar), cp_targetChar ); while ( i &lt; strlen(cp_inputText) ) { if( cp_inputText[i] == cp_targetChar[0] ) charCount++; i++; } pfree(cp_inputText); pfree(cp_targetChar); PG_RETURN_INT32(charCount);} Now we can compile the extension make If make is successful, there should be a new C shared library created Let’s go ahread and install sudo make install This will copy thechar_count_c–1.0.sql and char_count_c.control to $SHAREDIR/extensionand char_count_c.so to $LIBDIR Make sure char_count_c.so is indeed installed to the $LIBDIR, otherwise, PG will not be able to find it when the extension is loaded. With the extension installed, we can connect to the PG server and use the new extension Create a new test case in char_count_c/sql let’s make a copy of the test case from previous “char_count” example and change the names to “char_count_c” CREATE EXTENSION char_count_c;SELECT char_count_c('aaaabbbbbbbcc','a');SELECT char_count_c('aaaabbbbbbbcc','b');SELECT char_count_c('aaaabbbbbbbcc','c');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','x');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','c');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','b');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','5');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','3');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','2');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','1');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','0');SELECT char_count_c('aaaabbbbbbbcc1111222222233333335555590','asd'); Please note that in the Makefile, we have to also specifiy the name of the regression tests with this line: REGRESS = char_count_c Run the test case make installcheck copy the .out file to expected folder cp char_count_c/results/char_count_c.out char_count_c/expected 7. Add the new extensions to global MakefileIf you would like to have your extensions built along with the community ones, instead of building individually, you will need to modify the global extension Makefile located in [PG SOURCE DIR]/contrib/Makefile, and add: char_count and char_count_c in SUBDIRS parameter 8. SummaryPostgres is a very flexibile and powerful database system that provides different ways for the end users to extend existing functionalities to fulfill his or her business needs. From the examples above, we have learned that since Postgres version 9, we are able to create new extensions using either plpgsql or C language and be able to create regression tests as part of the extension build to ensure the extensions will work as intended.","link":"/2019/09/25/A-Guide-to-Create-User-Defined-Extension-Modules-to-Postgres/"},{"title":"Trace-Postgres-query-processing-internals-with-debugger","text":"1. OverviewIn this article we will use GDB debugger to trace the internals of Postgres and observe how an input query passes through several levels of transformation (Parser -&gt; Analyzer -&gt; Rewriter -&gt; Planner -&gt; Executor) and eventually produces an output. This article is based on PG12 running on Ubuntu 18.04, and we will use a simple SELECT query with ORDER BY , GROUP BY, and LIMIT keywords to go through the entire query processing tracing. 2. PreparationGDB debugger is required to be installed to trace the internals of Postgres. Most recent distribution of Linux already comes with gdb pre-installed. If you do not have it, please install. 2.1 Enable Debugging and Disable Compiler Optimization on PG BuildFor GDB to be useful, the postgres binaries have to be compiled with debugging symbols enabled (-g). In addition, I would suggest to turn off compiler optimization (-O0) such that while tracing we will be able to examine all memory blocks and values, and observe the execution flow properly. Enable debugging using the ./configure utility in the Postgres source code repository cd [PG_SOURCE_DIR]./configure --enable-debug This will add the (-g) parameter to the CFLAGS in the main Makefile to include debugging symbols. Once finished, let’s disable compiler optimization by editing src/Makefile.global Find the line where CFLAGS is defined and changed -O2 to -O0 like this: CFLAGS = -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -g -O0 Then we need to build and install with the new Makefile makesudo make install 2.2 Initialize Postgres ServerFor a new build, we will need to initialize a new database initDb /home/caryh/postgresqlcreate user caryhcreatedb carytest For referencing purposes, I would suggest enable debug log for the Postgres server by modifying postgres.conf located in database home directory. In this case it is located in /home/caryh/postgresql/postgres.conf Enable the following lines in postgres.conf postgres.conflog_destination = 'syslog'syslog_facility = 'LOCAL0'syslog_ident = 'postgres'syslog_sequence_numbers = onsyslog_split_messages = ondebug_print_parse = ondebug_print_rewritten = ondebug_print_plan = ondebug_pretty_print = onlog_checkpoints = onlog_connections = onlog_disconnections = onlog_duration = on Why do we enable debug log when we will be tracing postgres with gdb? This is because the output at some of the stages of query processing is represented as a complex list of structures and it is not very straightforward to print this structure unless we have written a third party print script that can help us recursively print the content of the complex structure. Postgres already has this function built-in and presented in the form of a debugging log. 2.3 Start Postgres Server and Connect with Client ToolStart the PG database pg_ctl -D /home/caryh/postgresql start Connect to PG database as user psql -d carytest -U cary 2.4 Populate Example Tables and ValuesCREATE TABLE deviceinfo ( serial_number varchar(45) PRIMARY KEY, manufacturer varchar(45), device_type int, password varchar(45), registration_time timestamp);CREATE TABLE devicedata ( serial_number varchar(45) REFERENCES deviceinfo(serial_number), record_time timestamp, uptime int, temperature numeric(10,2), voltage numeric(10,2), power numeric(10,2), firmware_version varchar(45), configuration_file varchar(45));INSERT INTO deviceinfo VALUES( 'X00001', 'Highgo Inc', 1, 'password', '2019-09-18 16:00:00');INSERT INTO deviceinfo VALUES( 'X00002', 'Highgo Inc', 2, 'password', '2019-09-18 17:00:00');INSERT INTO deviceinfo VALUES( 'X00003', 'Highgo Inc', 1, 'password', '2019-09-18 18:00:00');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 16:00:00', 2000, 38.23, 189.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 17:00:00', 3000, 68.23, 221.00, 675.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 18:00:00', 4000, 70.23, 220.00, 333.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00001', '2019-09-20 19:00:00', 5000, 124.23, 88.00, 678.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 11:00:00', 8000, 234.23, 567.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 12:00:00', 9000, 56.23, 234.00, 345.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 13:00:00', 3000, 12.23, 56.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 14:00:00', 4000, 56.23, 77.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 11:00:00', 8000, 234.23, 567.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 12:00:00', 9000, 56.23, 234.00, 345.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 13:00:00', 3000, 12.23, 56.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00002', '2019-09-20 14:00:00', 4000, 56.23, 77.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 07:00:00', 25000, 68.23, 99.00, 43.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 08:00:00', 20600, 178.23, 333.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 09:00:00', 20070, 5.23, 33.00, 123.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 10:00:00', 200043, 45.23, 45.00, 456.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 09:00:00', 20070, 5.23, 33.00, 123.1, 'version01', 'config01');INSERT INTO devicedata VALUES ('X00003', '2019-09-20 10:00:00', 200043, 45.23, 45.00, 456.1, 'version01', 'config01'); 3. Start gdb DebuggerFind the PID of the connecting client PG session $ ps -ef | grep postgrescaryh 7072 1946 0 Sep26 ? 00:00:01 /usr/local/pgsql/bin/postgres -D /home/caryh/postgresqlcaryh 7074 7072 0 Sep26 ? 00:00:00 postgres: checkpointer caryh 7075 7072 0 Sep26 ? 00:00:01 postgres: background writer caryh 7076 7072 0 Sep26 ? 00:00:01 postgres: walwriter caryh 7077 7072 0 Sep26 ? 00:00:01 postgres: autovacuum launcher caryh 7078 7072 0 Sep26 ? 00:00:03 postgres: stats collector caryh 7079 7072 0 Sep26 ? 00:00:00 postgres: logical replication launcher caryh 7082 7072 0 Sep26 ? 00:00:00 postgres: cary carytest [local] idle In this case it is the last line of the ps output as both my client and server reside in the same machine. Yours may be different. caryh 7082 7072 0 Sep26 ? 00:00:00 postgres: cary carytest [local] idle Now we can run gdb with the postgres binary sudo gdb /usr/local/pgsql/bin/postgresGNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-gitCopyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type \"show copying\"and \"show warranty\" for details.This GDB was configured as \"x86_64-linux-gnu\".Type \"show configuration\" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type \"help\".Type \"apropos word\" to search for commands related to \"word\"...Reading symbols from /usr/local/pgsql/bin/postgres...done.(gdb) Now, we can attach gdb to the PID identified in previous step (gdb) attach 7082Attaching to program: /usr/local/pgsql/bin/postgres, process 7082Reading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug/.build-id/28/c6aade70b2d40d1f0f3d0a1a0cad1ab816448f.debug...done.done.[Thread debugging using libthread_db enabled]Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.Reading symbols from /lib/x86_64-linux-gnu/librt.so.1...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/librt-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libdl.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libdl-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libm.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libm-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.27.so...done.done.Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.27.so...done.done.Reading symbols from /lib/x86_64-linux-gnu/libnss_files.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libnss_files-2.27.so...done.done.0x00007fce71eafb77 in epoll_wait (epfd=4, events=0x5633194c87e0, maxevents=1, timeout=-1) at ../sysdeps/unix/sysv/linux/epoll_wait.c:3030 ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.(gdb) Upon attach, Postgres process will be put on a break and we are able to issue breakpoints command from here 4. Start Tracing with gdbexec_simple_query is the function that will trigger all stages of query processing. Let’s put a breakpoint here. (gdb) b exec_simple_queryBreakpoint 1 at 0x56331899a43b: file postgres.c, line 985.(gdb) cContinuing. Now, let’s type in a SELECT query with ORDER BY keywords on the postgres client connection terminal to trigger break point carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; Breakpoint should be triggered Breakpoint 1, exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985(gdb) Let’s do a backtrace bt command to see how the control got here. (gdb) bt#0 exec_simple_query (query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:985#1 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#2 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#3 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#4 0x00005633188f753e in ServerLoop () at postmaster.c:1704#5 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#6 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 As you can see, PostmasterMain process is one of the early process to be started and this is where it will spawn all the backend processes and initialize the ‘ServerLoop’ to listen for client connections. When a client connets and issues some queries, the handle will be passed from the backend to PostgresMain and this is where the query processing will begine. 5. Parser StageParser Stage is the first stage in query processing, which will take an input query string and produce a raw un-analyzed parse tree. The control will eventually come to the raw_parser function, so let’s set a break point there and do a backtrace: (gdb) b raw_parserBreakpoint 2 at 0x5633186b5bae: file parser.c, line 37.(gdb) cContinuing.Breakpoint 2, raw_parser (str=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37(gdb) bt#0 raw_parser (str=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at parser.c:37#1 0x000056331899a03e in pg_parse_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:641#2 0x000056331899a4c9 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1037#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 In raw_parser, 2 things will happen, first to scan the query with flex-based scanner to check keyword validity and second to do the actual parsing with bison-based parser. In the end, it will return a parse tree for next stage. (gdb) n43 yyscanner = scanner_init(str, &amp;yyextra.core_yy_extra,(gdb) n47 yyextra.have_lookahead = false;(gdb) n50 parser_init(&amp;yyextra);(gdb) 53 yyresult = base_yyparse(yyscanner);(gdb) 56 scanner_finish(yyscanner);(gdb) 58 if (yyresult) /* error */(gdb) 61 return yyextra.parsetree; It is not very straight-forward to examine the content of the parse tree stored in yyextra.parsetree as above. This is why we enabled postgres debug log so that we can utilize it to recursively print the content of the parse tree. The parse tree illustrated by yyextra.parsetree can be visualized as this image below: 6.0 Analyzer StageNow we have a list of parse trees, size 1 in this example, PG will need to feed each item in the list into anaylzer and rewriter functions. Let’s set a break point at parse_analyze function (gdb) b parse_analyzeBreakpoint 3 at 0x56331867d608: file analyze.c, line 104.(gdb) cContinuing.Breakpoint 3, parse_analyze (parseTree=0x56331949dd50, sourceText=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at analyze.c:104104 ParseState *pstate = make_parsestate(NULL);(gdb) bt#0 parse_analyze (parseTree=0x56331949dd50, sourceText=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at analyze.c:104#1 0x000056331899a0a8 in pg_analyze_and_rewrite (parsetree=0x56331949dd50, query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at postgres.c:695#2 0x000056331899a702 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1140#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228 The above backtrace shows how the control gets to parse_analyze function, and 2 vital imputs are parseTree (type RawStmt) and (const char) sourceText Let’s traverse to the end of parse_analyze (gdb) n109 pstate-&gt;p_sourcetext = sourceText;(gdb) 111 if (numParams &gt; 0)(gdb) 114 pstate-&gt;p_queryEnv = queryEnv;(gdb) 116 query = transformTopLevelStmt(pstate, parseTree);(gdb) 118 if (post_parse_analyze_hook)(gdb) 121 free_parsestate(pstate);(gdb) 123 return query; At analyzer stage, it produces a result of type Query and it is in fact the data type return from the parser stage as a List of Query. This structure will be fed into the rewriter stage. 7.0 Rewriter StageRewriter is the next stage following analyzer, let’s create a break point at pg_rewrite_query and do a backtrace: (gdb) b pg_rewrite_queryBreakpoint 4 at 0x56331899a1c1: file postgres.c, line 773(gdb) cContinuing.Breakpoint 4, pg_rewrite_query (query=0x56331949dee0) at postgres.c:773773 if (Debug_print_parse)(gdb) bt#0 pg_rewrite_query (query=0x56331949dee0) at postgres.c:773#1 0x000056331899a0cf in pg_analyze_and_rewrite (parsetree=0x56331949dd50, query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;, paramTypes=0x0, numParams=0, queryEnv=0x0) at postgres.c:704#2 0x000056331899a702 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1140#3 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#4 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#5 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#6 0x00005633188f753e in ServerLoop () at postmaster.c:1704#7 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#8 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228(gdb) Rewriter takes the output of the previou stage and returns a querytree_list of type List*. Let’s trace the function to the end and print the output 773 if (Debug_print_parse)(gdb) n774 elog_node_display(LOG, &quot;parse tree&quot;, query,(gdb) 777 if (log_parser_stats)(gdb) 780 if (query-&gt;commandType == CMD_UTILITY)(gdb) 788 querytree_list = QueryRewrite(query);(gdb) 791 if (log_parser_stats)(gdb) 848 if (Debug_print_rewritten)(gdb) 849 elog_node_display(LOG, &quot;rewritten parse tree&quot;, querytree_list,(gdb) 852 return querytree_list; the line 774 elog_node_display and line 849 elog_node_display are the debug print function provided by postgres to recursively print the content of Query before and after rewriter stage. After examining the output query tree, we found that in this example, the rewriter does not make much modification to the origianl query tree and it can be visualized as: 8.0 Planner StagePlanner is the next stage immediately following the previous. The main planner function entry point is pg_plan_query and it takes the output from previous stage as input. Let’s create a breakpoint and do a backtrace again (gdb) b pg_plan_queriesBreakpoint 5 at 0x56331899a32d: file postgres.c, line 948.(gdb) cContinuing.Breakpoint 5, pg_plan_queries (querytrees=0x563319558558, cursorOptions=256, boundParams=0x0) at postgres.c:948948 List *stmt_list = NIL;(gdb) bt#0 pg_plan_queries (querytrees=0x563319558558, cursorOptions=256, boundParams=0x0) at postgres.c:948#1 0x000056331899a722 in exec_simple_query ( query_string=0x56331949cf00 &quot;select * from devicedata order by serial_number desc;&quot;) at postgres.c:1143#2 0x000056331899f01c in PostgresMain (argc=1, argv=0x5633194c89c8, dbname=0x5633194c8890 &quot;carytest&quot;, username=0x5633194c8878 &quot;cary&quot;) at postgres.c:4249#3 0x00005633188fba97 in BackendRun (port=0x5633194c0f60) at postmaster.c:4431#4 0x00005633188fb1ba in BackendStartup (port=0x5633194c0f60) at postmaster.c:4122#5 0x00005633188f753e in ServerLoop () at postmaster.c:1704#6 0x00005633188f6cd4 in PostmasterMain (argc=3, argv=0x5633194974c0) at postmaster.c:1377#7 0x000056331881a10f in main (argc=3, argv=0x5633194974c0) at main.c:228(gdb) Now, we are here, let’s trace the function until the end. Please note that for each content block in the input querytree list, the function will call a helper plan function called pg_plan_query and it will perform the real plan operation there and return the result in plannedStmt data type (gdb) n951 foreach(query_list, querytrees)(gdb) n953 Query *query = lfirst_node(Query, query_list);(gdb) n956 if (query-&gt;commandType == CMD_UTILITY)(gdb) n968 stmt = pg_plan_query(query, cursorOptions, boundParams);(gdb) spg_plan_query (querytree=0x56331949dee0, cursorOptions=256, boundParams=0x0) at postgres.c:866866 if (querytree-&gt;commandType == CMD_UTILITY)(gdb) n874 if (log_planner_stats)(gdb) 878 plan = planner(querytree, cursorOptions, boundParams);(gdb) n880 if (log_planner_stats)(gdb) 929 if (Debug_print_plan)(gdb) 930 elog_node_display(LOG, &quot;plan&quot;, plan, Debug_pretty_print);(gdb) 934 return plan; Line 930 elog_node_display will print the content of PlannedStmt recursively to syslog and it can be visualized as: The above plan tree corresponds to the output of EXPLAIN ANALYZE on the same query. carytest=&gt; EXPLAIN ANALYZE SELECT serial_number, COUNT(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; QUERY PLAN ------------------------------------------------------------------------------------ Limit (cost=1.32..1.33 rows=2 width=15) (actual time=0.043..0.044 rows=2 loops=1) -&gt; Sort (cost=1.32..1.33 rows=3 width=15) (actual time=0.042..0.042 rows=2 loops=1) Sort Key: (count(serial_number)) DESC Sort Method: quicksort Memory: 25kB -&gt; HashAggregate (cost=1.27..1.30 rows=3 width=15) (actual time=0.033..0.035 rows=3 loops=1) Group Key: serial_number -&gt; Seq Scan on devicedata (cost=0.00..1.18 rows=18 width=7) (actual time=0.013..0.016 rows=18 loops=1) Planning Time: 28.541 ms Execution Time: 0.097 ms(9 rows) Line 878 plan = planner(querytree, cursorOptions, boundParams); in the above trace is the real planner logic and it is a complex stage. Inside this function, it will compute the initial cost and run time cost of all possible queries and in the end, it will choose a plan that is the least expensive. with the plannedStmt produced, we are ready to enter the next stage of query processing. 9.0 Executor StageIn addition to planner, executor is also one of the complex stages of query processing. This module is responsible for executing the query plan produced from previous stage and sending the query results back to the connecting client. Executor is invoked and managed with a wrapper called portal and portal is an object representing the execution state of a query and providing memory management services but it does not actually run the executor. In the end, the portal will invoke one of the four executor routines as below -ExecutorStart()-ExecutorRun()-ExecutorFinish()-ExecutorEnd() Before we can use the above routines, the portal needs to be initialized first. In the previous stage, the control is left at exec_simple_query at line 1147, let’s continue tracing from here to enter portal initialization Let’s create a break point for each executor routine and do a back trace on each as we continue (gdb) b ExecutorStartBreakpoint 6 at 0x5633187ad797: file execMain.c, line 146.(gdb) b ExecutorRunBreakpoint 7 at 0x5633187ada1e: file execMain.c, line 306.(gdb) b ExecutorFinishBreakpoint 8 at 0x5633187adc35: file execMain.c, line 405.(gdb) b ExecutorEndBreakpoint 9 at 0x5633187add1e: file execMain.c, line 465. 9.1 Executor StartThe main purpose of ExecutorStart routine is to prepare the query plan, allocate storage and prepare rule manager. Let’s continue the tracing and do a backtrace. Breakpoint 6, ExecutorStart (queryDesc=0x5633195712e0, eflags=0) at execMain.c:146146 if (ExecutorStart_hook)(gdb) bt#0 ExecutorStart (queryDesc=0x564977500190, eflags=0) at execMain.c:146#1 0x0000564975eb87e0 in PortalStart (portal=0x5649774a18d0, params=0x0, eflags=0, snapshot=0x0) at pquery.c:518#2 0x0000564975eb27b5 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1176#3 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#4 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#5 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#6 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#7 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#8 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) 9.2 Executor RunExecutorRun is the main routine of executor module, and its main task is to execute the query plan, this routing will call the ExecutePlan function to actually execute the plan. In the end, before return, the result of query will be stored in Estate structure called estate and inside there is a count of how many tutples have been processed by the executor (gdb) cContinuing.Breakpoint 7, ExecutorRun (queryDesc=0x5633195712e0, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:306306 if (ExecutorRun_hook)(gdb) bt#0 ExecutorRun (queryDesc=0x564977500190, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:306#1 0x0000564975eb915c in PortalRunSelect (portal=0x5649774a18d0, forward=true, count=0, dest=0x564977539460) at pquery.c:929#2 0x0000564975eb8db6 in PortalRun (portal=0x5649774a18d0, count=9223372036854775807, isTopLevel=true, run_once=true, dest=0x564977539460, altdest=0x564977539460, completionTag=0x7ffff0b937d0 &quot;&quot;) at pquery.c:770#3 0x0000564975eb28ad in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1215#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) Continue tracing the ExecutorRun to the end. 306 if (ExecutorRun_hook)(gdb) n309 standard_ExecutorRun(queryDesc, direction, count, execute_once);(gdb) sstandard_ExecutorRun (queryDesc=0x564977500190, direction=ForwardScanDirection, count=0, execute_once=true) at execMain.c:325325 estate = queryDesc-&gt;estate;(gdb) n333 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) n336 if (queryDesc-&gt;totaltime)(gdb) n342 operation = queryDesc-&gt;operation;(gdb) 343 dest = queryDesc-&gt;dest;(gdb) 348 estate-&gt;es_processed = 0;(gdb) 350 sendTuples = (operation == CMD_SELECT ||(gdb) 353 if (sendTuples)(gdb) 354 dest-&gt;rStartup(dest, operation, queryDesc-&gt;tupDesc);(gdb) 359 if (!ScanDirectionIsNoMovement(direction))(gdb) 361 if (execute_once &amp;&amp; queryDesc-&gt;already_executed)(gdb) 363 queryDesc-&gt;already_executed = true;(gdb) 365 ExecutePlan(estate,(gdb) 367 queryDesc-&gt;plannedstmt-&gt;parallelModeNeeded,(gdb) 365 ExecutePlan(estate,(gdb) 379 if (sendTuples)(gdb) 380 dest-&gt;rShutdown(dest);(gdb) 382 if (queryDesc-&gt;totaltime)(gdb) 385 MemoryContextSwitchTo(oldcontext);(gdb) p estate$6 = (EState *) 0x56497751fbb0(gdb) p estate-&gt;es_processed$7 = 2 9.3 Executor FinishExecutorFinish must be called after the last ExecutorRun, its main task is to perform necessary clearn up actions and also fire up after Triggers. Let’s trace a little further. (gdb) cContinuing.Breakpoint 8, ExecutorFinish (queryDesc=0x5633195712e0) at execMain.c:405405 if (ExecutorFinish_hook)(gdb) bt#0 ExecutorFinish (queryDesc=0x564977500190) at execMain.c:405#1 0x0000564975c5b52c in PortalCleanup (portal=0x5649774a18d0) at portalcmds.c:300#2 0x0000564976071ba4 in PortalDrop (portal=0x5649774a18d0, isTopCommit=false) at portalmem.c:499#3 0x0000564975eb28d3 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228 Continue tracing the ExecutorFinish to the end. 405 if (ExecutorFinish_hook)(gdb) n408 standard_ExecutorFinish(queryDesc);(gdb) sstandard_ExecutorFinish (queryDesc=0x564977500190) at execMain.c:420420 estate = queryDesc-&gt;estate;(gdb) n429 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) 432 if (queryDesc-&gt;totaltime)(gdb) 436 ExecPostprocessPlan(estate);(gdb) 439 if (!(estate-&gt;es_top_eflags &amp; EXEC_FLAG_SKIP_TRIGGERS))(gdb) 442 if (queryDesc-&gt;totaltime)(gdb) 445 MemoryContextSwitchTo(oldcontext);(gdb) 447 estate-&gt;es_finished = true;(gdb) 448 } 9.4 Executor EndThis routing basically resets and releases some of the state variables in QueryDesc used during execution. ExecutorEnd is the last routine to be called and before entry, the PortalCleanup and PortalDrop are invoked first. So as we are in this routine the outer Portal object is also performing the cleanup process. Breakpoint 9, ExecutorEnd (queryDesc=0x5633195712e0) at execMain.c:465465 if (ExecutorEnd_hook)(gdb) bt#0 ExecutorEnd (queryDesc=0x564977500190) at execMain.c:465#1 0x0000564975c5b538 in PortalCleanup (portal=0x5649774a18d0) at portalcmds.c:301#2 0x0000564976071ba4 in PortalDrop (portal=0x5649774a18d0, isTopCommit=false) at portalmem.c:499#3 0x0000564975eb28d3 in exec_simple_query ( query_string=0x564977439f00 &quot;select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2;&quot;) at postgres.c:1225#4 0x0000564975eb701c in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4249#5 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#6 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#7 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#8 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#9 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) Let’s Continue tracing ExecutorEnd to the end. 465 if (ExecutorEnd_hook)(gdb) n468 standard_ExecutorEnd(queryDesc);(gdb) sstandard_ExecutorEnd (queryDesc=0x564977500190) at execMain.c:480480 estate = queryDesc-&gt;estate;(gdb) n495 oldcontext = MemoryContextSwitchTo(estate-&gt;es_query_cxt);(gdb) 497 ExecEndPlan(queryDesc-&gt;planstate, estate);(gdb) 500 UnregisterSnapshot(estate-&gt;es_snapshot);(gdb) 501 UnregisterSnapshot(estate-&gt;es_crosscheck_snapshot);(gdb) 506 MemoryContextSwitchTo(oldcontext);(gdb) 512 FreeExecutorState(estate);(gdb) 515 queryDesc-&gt;tupDesc = NULL;(gdb) 516 queryDesc-&gt;estate = NULL;(gdb) 517 queryDesc-&gt;planstate = NULL;(gdb) 518 queryDesc-&gt;totaltime = NULL;(gdb) 519 } This routine marks the end of the query processing stages, the control will be passed back to exec_simple_query to finish the transaction and present result back to the client. 10.0 Presenting the Result Back to ClientWith the transaction ended, the send_ready_for_query flag will be set, and the control is now able to enter ReadyForQuery to present the result to client. (gdb) b ReadyForQueryBreakpoint 10 at 0x56331899811d: file dest.c, line 251.(gdb) cContinuing.Breakpoint 10, ReadyForQuery (dest=DestRemote) at dest.c:251251 {(gdb) bt#0 ReadyForQuery (dest=DestRemote) at dest.c:251#1 0x0000564975eb6eee in PostgresMain (argc=1, argv=0x564977465a08, dbname=0x5649774658d0 &quot;carytest&quot;, username=0x5649774658b8 &quot;cary&quot;) at postgres.c:4176#2 0x0000564975e13a97 in BackendRun (port=0x56497745dfa0) at postmaster.c:4431#3 0x0000564975e131ba in BackendStartup (port=0x56497745dfa0) at postmaster.c:4122#4 0x0000564975e0f53e in ServerLoop () at postmaster.c:1704#5 0x0000564975e0ecd4 in PostmasterMain (argc=3, argv=0x5649774344c0) at postmaster.c:1377#6 0x0000564975d3210f in main (argc=3, argv=0x5649774344c0) at main.c:228(gdb) n252 switch (dest)(gdb) 257 if (PG_PROTOCOL_MAJOR(FrontendProtocol) &gt;= 3)(gdb) 261 pq_beginmessage(&amp;buf, 'Z');(gdb) 262 pq_sendbyte(&amp;buf, TransactionBlockStatusCode());(gdb) 263 pq_endmessage(&amp;buf);(gdb) p dest$90 = DestRemote(gdb) n268 pq_flush();(gdb) 269 break;(gdb) 282 }(gdb) as pq_flush() is called, the result of the query will be returned back to the client at remote destination. 10.1 Client ResultsClient will now see the output below as a result of the query carytest=&gt; select serial_number, count(serial_number) from devicedata GROUP BY serial_number ORDER BY count DESC LIMIT 2; serial_number | count ---------------+------- X00002 | 8 X00003 | 6(2 rows) 11 SummarySo far, we have traced through severl stags of query processing. Namely Parser Analyzer Rewritter Planner Executor To summarize all the above, I have created a simple call hierarchy ( or a list of breakpoints) below that outlines the important core functions that will be called while stepping through the above stages. The ‘b’ in front of each function name corresponds to the break point command of gdb. ## Main Entry ##b exec_simple_query ## Parser ## b pg_parse_query -&gt; returns (List* of Query) b raw_parser -&gt; returns (List* of Query) b base_yyparse -&gt; returns (List* of Query) ## Analzyer and Rewritter ## b pg_analyze_and_rewrite -&gt; returns (List*) b parse_analyze -&gt; returns (Query*) b pg_rewrite_query -&gt; returns (List* of Query) b QueryRewrite -&gt; returns (List* of Query) ## Planner ## b pg_plan_queries -&gt; returns (List* of plannedStmt) b pg_plan_query -&gt; returns (PlannedStmt*) b planner -&gt; returns (PlannedStmt*) ## Executor ## b PortalStart -&gt; returns void b ExecutorStart -&gt; returns void b PortalRun -&gt; returns bool b PortalRunSelect -&gt; returns uint64 b ExecutorRun -&gt; returns void b PortalDrop -&gt; returns void b PortalCleanup -&gt; returns void b ExecutorFinish -&gt; returns void b ExecutorEnd -&gt; returns void ## Present Result ##b ReadyForQuery -&gt; returns void b pq_flush -&gt; returns void","link":"/2019/09/27/Trace-Postgres-query-processing-internals-with-debugger/"},{"title":"Types of SNMP OIDs Explained","text":"1. OverviewSNMP stands for Simple Network Management Protocol and it is commonly used to access and manage network equipment such as switches and routers. It has been available for a long time and has evolved quite a lot with better functionality and security. Each SNMP object can be addressed by its Object IDs (OIDs) and I find that many new SNMP users are confused about the SNMP OIDs and how they should be used correctly. There are 3 major kinds of SNMP OIDs, scalar, table and dynamic and each has its own unique way of accessing. This blog will explain each type of OID and show how they can be accessed correctly. 2. ScalarScalar SNMP OID represents one single value that is available for GET/SET on a SNMP agent. This type of SNMP OID is represented by a ‘leaf icon’ (if read only) or a “paper and pen” icon (if read and write) on MIB browser. To access a scalar value SNMP OID, you need to append a zero (0) in the end of the OID number. For example:cxSysBplTopologyTree has OID number = .1.3.6.1.4.1.6232.8.1.1.1 To access it, you much append a zero (0) in the end to indicate that you want to access this OID as a scalar value. So it becomes = .1.3.6.1.4.1.6232.8.1.1.1.0. without this zero, you will get “OID does not exist error” 3. TableTable OID represents one or more sets of values that are available for GET/SET on a SNMP agent. This type of SNMP OID is represented by a ‘Table icon’ on MIB browser. Each Table OID must have an index value that can be used by the client to select which table entry to retrieve value from. The index value is represented by the “key” icon on MIB browser and normally it is defined as integer. To access a value from a Table OID, you need to append an index number that is larger than zero at the end of the OID number For example:plSysFWVersion has OID = .1.3.6.1.4.1.6232.8.3.1.1.5 To access it, you must append an index number in the end of the OID number to select one of many table entries. In most cases, (also our case), you need to put the index number = 1 to indicate you would like the value from table entry number 1. So the OID becomes .1.3.6.1.4.1.6232.8.3.1.1.5.1. Without this index number 1, you will get “OID does not exist error”. Why do we need a table OID?SNMP is mostly used to manage network equipment and it is common that this network equipment can have multiple Network Interface Card (NIC), for example (eth0, eth1, eth4…etc). If table is not used, you will need to define multiple separate sets of SNMP OIDS, each corresponding to one NIC. This is extremely not efficient and could get messy. Easier way is to define the table OIDs containing only one set of OID common for all NICs, and manager software can access each NIC’s OID values simply by varying the index number 4. Dynamic OIDDynamic OID is a more advanced usage of Table OID, which involves using more than one value to index table entries. In table OID, we talked about using one integer value to select a table entry by appending a number in the end of OID value. In addition to the integer number acting as primary index, we can define a second index value to further refine the result… like a 2 dimensional table, and we call this dynamic OID. For dynamic OID, the table is defined like the picture below… with 2 keys defined to look up a table value: plPhyByMACCard and plPhyByMACMAC This means in order to retrieve a value from this table, you have to supply 2 key values • plPhyByMACCardThis is an integer value index, which we will append “1” in our case just like the previous chapter • plPhyByMACMACThis is the second index, and it has a type = PHYSADDRESS instead of integer, this means you need to append the 6 byte MAC address (in decimal form) also to the end of the OID in order to retrieve this value For example:• plPhyByMACTxPhySpeed has OID = .1.3.6.1.4.1.6232.8.3.3.1.1.11 We need to append a “1” in the end of the OID due to first index (plPhyByMACCard)So it becomes .1.3.6.1.4.1.6232.8.3.3.1.1.11.1 • Now we need to tell the agent which MAC address we would like to get the PHY speed from, let’s say MAC = 00:0B:C2:12:CD:E3. In our case, this MAC address is sent to the transport in cxSysPeerMacAddr attribute field • We need to convert the MAC to 6 decimal numbersSo it becomes 0.11.194.18.205.227 • Now we append the six numbers at the end of the OID that has been appended 1 from previous step:So it becomes .1.3.6.1.4.1.6232.8.3.3.1.1.11.1.0.11.194.18.205.227 Why do we need a second key value defined as MAC address?Let’s say a network switch has 5 devices connected to it having 5 different MAC addresses and there is one dynamic table OID using Network ID and MAC as keys defined to record the PHY speed to each connected equipment. In order for a SNMP manager to read the speed to one MAC equipment, it has to supply both network id and MAC address as index to the SNMP table so the agent can return the speed for respective device. Without dynamic table OID, the client simply cannot retrieve the speed value for any of the devices. SNMP walk can be used to GET all the available table entries, which represent all the devices connected. In the screenshot above, I first did a snmpwalk on OID .1.3.6.1.4.1.6232.8.3.3.1.1.11 without any index added, then I receive 2 entries, each entry corresponding to the Tx speed of one MAC. As you can see, even the returned value has the OID + integer index + MAC index. Second, I did a snmpget on OID .1.3.6.1.4.1.6232.8.3.3.1.1.11.1.0.11.194.18.205.227 and I only get one entry in the result as I have selected.","link":"/2019/07/04/Types-of-SNMP-OIDs-Explained/"},{"title":"Replication-Failover-with-pg_rewind-in-PG12","text":"1. OverviewIn the previous blog, we have discussed how to correctly set up streaming replication clusters between one master and one slave in Postgres version 12. In this blog, we will simulate a failover scenario on the master database, which causes the replica (or slave) database cluster to be promoted as new master and continue the operation. We will also simulate a failback scenario to reuse the old master cluster after the failover scenario with the help of pg_rewind. Normally it is quite easy to do a failback to the old master after slave gets promoted to master but if there is data written to the old master after slave promotion, we will have an out-of-sync case between them both and we will have to use the pg_rewind tool to synchronize the two data directories in order to bring the old master to match the state of the new master. Please note that the pg_rewind tool will remove transactions from the old master in order to match up with the new, so certain pre-caution is needed to use this tool. Here’s a brief overview of list of actions we are going to perform: simulate failover by promoting slave cluster, so it becomes a new master simulate data insertion to master cluster, also referred as old master after promotion shutdown the old master cluster and set it up as a standby server run pg_rewind on old master to synchronize transaction states with new master bring up old master as a standby server to synchronize with the new master This blog assumes you already have streaming replication setup between one master and one slave from previous blog. If you have not checked out the previous blog titled “Streaming Replication Setup in PG12 - How to Do it Right”, it is recommended to give that a read first. The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04 2. Simulate a Failover CaseWe will simply promote the slave database cluster to simulate a failover. $ pg_ctl promote -D db-slave2019-10-30 11:10:16.951 PDT [16643] LOG: received promote request2019-10-30 11:10:16.951 PDT [16651] FATAL: terminating walreceiver process due to administrator command2019-10-30 11:10:16.966 PDT [16643] LOG: redo done at 0/3003B602019-10-30 11:10:16.991 PDT [16643] LOG: selected new timeline ID: 22019-10-30 11:10:17.030 PDT [16643] LOG: archive recovery complete2019-10-30 11:10:17.051 PDT [16642] LOG: database system is ready to accept connections As seen above, After slave gets promoted, it switches to a new timeline for future data operations. At this point the master and slave are no longer streaming WAL files from each other and we essentialyl have two independent database clusters running. We will call them old master and new master in the following sections instead so it is clear. 3. Insert Some Data to the Old MasterWe would like to create a data out-of-sync case by inserting some more data to the old master cluster. $ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y);\" -p 5432INSERT 0 100 Check that both the old master and new master are clearly out of sync: ## new master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row)## old master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 300(1 row) 4. Configure the Old Master as Standby Server to Sync with New MasterNow we would like to attempt a failover to make the old master as a standy server to syncrhonize with the new master. Let’s shutdown the old master cluster. $ pg_ctl -D db-master stopwaiting for server to shut down.... doneserver stopped Let’s update postgresql.conf in the old master: db-master/postgresql.confrecovery_target_timeline = 'latest'archive_cleanup_command = 'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r'restore_command = 'cp /home/caryh/streaming-replication/archivedir/%f %p'primary_slot_name = 'main'primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5433 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any' the primary_conninfo tells the old master to stream WAL files from the new master located at 127.0.0.1:5433. Also, do not forget to touch the standby.signal file to tell the cluster to run in standby mode: touch db-master/standby.signal We specified in the old master to connect to primary_slot_name = main. Let’s create the matching replication slot on the new master. $ psql -d clusterdb -U cary -c \"select * from pg_create_physical_replication_slot('main');\" -p 5433 slot_name | lsn -----------+----- main | (1 row)$ psql -d clusterdb -U cary -c \"select * from pg_replication_slots;\" -p 5433 -x-[ RECORD 1 ]-------+---------slot_name | mainplugin | slot_type | physicaldatoid | database | temporary | factive | factive_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | Now the new master has a matching replication slot called main and it is not active at the moment. Now we are ready to start the old master as standby server 5. Start the Old Master as Standby ServerNow, we are ready to start the old master as a standby: $ pg_ctl -D db-master start2019-10-30 11:30:04.071 PDT [1610] HINT: Future log output will go to log destination \"syslog\".2019-10-30 11:30:04.075 PDT [1611] LOG: database system was shut down at 2019-10-30 11:29:13 PDT2019-10-30 11:30:04.079 PDT [1611] LOG: restored log file \"00000002.history\" from archive2019-10-30 11:30:04.082 PDT [1611] LOG: entering standby mode2019-10-30 11:30:04.084 PDT [1611] LOG: restored log file \"00000002.history\" from archive2019-10-30 11:30:04.095 PDT [1611] FATAL: requested timeline 2 is not a child of this server's history2019-10-30 11:30:04.095 PDT [1611] DETAIL: Latest checkpoint is at 0/4000028 on timeline 1, but in the history of the requested timeline, the server forked off from that timeline at 0/3003B98.2019-10-30 11:30:04.096 PDT [1610] LOG: startup process (PID 1611) exited with exit code 12019-10-30 11:30:04.096 PDT [1610] LOG: aborting startup due to startup process failure2019-10-30 11:30:04.098 PDT [1610] LOG: database system is shut down As you can see above, the old master refuses to start because there is a timeline difference between the old master and the new master. This is caused by the additional data insertions that happens to the old master after the promotion event in step number 3. This is where pg_rewind comes handy in situation like this, to synchronize the two clusters. 6. Use pg_rewind to Synchronize the two ClustersNow, let’s synchronize the two database clusters with pg_rewind. $ pg_rewind --target-pgdata=db-master --source-server=\"port=5433 user=cary dbname=clusterdb\" --progresspg_rewind: connected to serverpg_rewind: servers diverged at WAL location 0/3003E58 on timeline 1pg_rewind: rewinding from last common checkpoint at 0/2000060 on timeline 1pg_rewind: reading source file listpg_rewind: reading target file listpg_rewind: reading WAL in targetpg_rewind: need to copy 53 MB (total source directory size is 78 MB)54363/54363 kB (100%) copiedpg_rewind: creating backup label and updating control filepg_rewind: syncing target data directorypg_rewind: Done! After pg_rewind is finished, we will have to edit once more the configuration of the old master because the tool copies most of the configuration settings from the new master to the old master as a synchronization process. Let’s examine both db-master/postgresql.auto.conf and db-master/postgresql.conf and make sure of the followings again. db-master/postgresql.conf############# db-master/postgresql.conf #############primary_slot_name = 'main'recovery_target_timeline = 'latest'port = 5432############# db-master/postgresql.auto.conf #############primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5433 sslmode=disable sslcompression=0 gssencmode=disable target_session_attrs=any' and also, don’t forget about this: touch db-master/standby.signal Now, we should be ready to start the old master again. 7. Start the Old Master Agian as Standby Server$ pg_ctl -D db-master start2019-10-30 12:27:28.140 PDT [5095] LOG: restored log file \"000000010000000000000002\" from archive2019-10-30 12:27:28.167 PDT [5095] LOG: redo starts at 0/20000282019-10-30 12:27:28.182 PDT [5095] LOG: consistent recovery state reached at 0/30272582019-10-30 12:27:28.183 PDT [5095] LOG: invalid record length at 0/3027258: wanted 24, got 02019-10-30 12:27:28.183 PDT [5095] LOG: redo done at 0/30272302019-10-30 12:27:28.183 PDT [5095] LOG: last completed transaction was at log time 2019-10-30 12:20:34.056723-07019-10-30 12:27:28.226 PDT [5094] LOG: database system is ready to accept connections The old master can now start as a streaming replication to the new master and we can observe that after using pg_rewind the additional data that was inserted to old master in step number 3 is now removed, as it has been rewound from 300 entries to 200 entries to match up with the new master. ## new master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row)## old master ##$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 200(1 row) 8. SummaryIn this blog, we have simulated a failover case and observe the effect of promoting a standby slave server while more data insertions happening to the original master server. We have demonstrated how to use pg_rewind tool to synchronize both master and slave after the slave promotion. Though it results some data deletion at the original master, in the end, we are able to resolve the timeline conflict with pg_rewind and complete the database failover scenario.","link":"/2019/09/27/Replication-Failover-with-pg-rewind-in-PG12/"},{"title":"Steaming-Replication-Setup-in-PG12-How-to-do-it-right","text":"1. OverviewPostgreSQL 12 has been considered as a major update consisting of major performance boost with partitioning enhancements, indexing improvements, optimized planner logics and several others. One of the major changes is noticeably the removal of recovery.conf in a standby cluster. For this reason, the procedure to set up a streaming replication clusters has changed, and in this blog, I will demonstrate how to properly setup a streaming replication setup in PG12. Streaming replication setup requires a master cluster and one or more slave clusters that will replicate the data inserted to the master by streaming the archived WAL files generated by master. The master and slaves can reside on different machines connected via network but in this blog, we will use one master and one slave setup and both will be run on the same machine with different port number. The procedures illustrated in this blog is based on Postgres version 12 built from source running on Ubuntu 18.04 2. Master Database Cluster SetupCreate a master database cluster using initdb tool: $ initdb /home/caryh/streaming-replication/db-master$ cd /home/caryh/streaming-replication /home/caryh/streaming-replication is the root folder to all the database clusters that we will be creating in this blog and db-master directory will be created here as a result of above commands. Let’s modify the default postgreql.conf and enable several important configuration options as shown below for streaming replication setup. db-master/postgresql.confwal_level = replicaarchive_mode = onmax_wal_senders = 10 wal_keep_segments = 10hot_standby = onarchive_command = 'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'port = 5432wal_log_hints = on The configuration above enables Postgres to archive the WAL files in the directory /home/caryh/streaming-replication/archivedir/ when it has completed writing to a full block of WAL file or when pg_basebackup command has been issued. The %f and %p used within archive_command are internal to Postgres and %f will be replaced with the filename of the target WAL file and %p replaced with path to the targeted WAL file. It is very important when setting the archive_command to ensure the WAL files are archived to a location where the slave cluster can access. Please note that wal_log_hints must be enabled for pg_rewind tool to work properly. We will discuss more about pg_rewind in the next blog post. Examine the client authentication file db-master/pg_hba.conf and make sure the master cluster allows replication connections from a slave cluster remotely. In my case, both my master and slave will be run on the same host, so I will leave the loopback IP address as it is. If your slave cluster is located in another machine, make sure to replace the loopback address with the right one. db-master/pg_hba.conf # Allow replication connections from 127.0.0.1, by a user with the replication privilege.# TYPE DATABASE USER ADDRESS METHODhost replication all 127.0.0.1/32 trust Let’s go ahead and start the master database cluster with the above configuration files, create a super user with permission to do replication, and a database called clusterdb $ pg_ctl -D db-master start$ createuser cary -s --replication$ createdb clusterdb Insert some test data to the master cluster. For simplicity, we will insert 100 integers to test_table. $ psql -d clusterdb -U cary -c \"CREATE TABLE test_table(x integer)\"CREATE TABLE$ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y)\"INSERT 0 100$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" count ------- 100(1 row) 3. Slave Database Cluster SetupThe goal of setting up the slave cluster is to make a backup of the current master and set it up as a standby server, meaning it will stream the WAL file updates from the master and perform replication of the data. Postgres provides several tools and methods to perform physical database backup. Exclusive methods such as pg_start_backup('label') and pg_stop_backup() are quite common in earlier Postgres versions. In this blog, we will use the newer, and simpler non-exclusive pg_basebackup fronend tool to execute the backup. There are advantages and disadvantaged for both methods and this discussion is not within the scope of this blog. This article here provides very good explaination on both methods: https://www.cybertec-postgresql.com/en/exclusive-backup-deprecated-what-now/ Let’s use pg_basebackup to create the slave cluster. $ pg_basebackup -h 127.0.0.1 -U cary -p 5432 -D db-slave -P -Xs -R31373/31373 kB (100%), 1/1 tablespace where:-h is the IP of the master cluster-U is the username that is permitted to do replication-p is the port number of the running master cluster-D is the directory where we want to set up the slave database cluster-P to show the progress-Xs to select WAL streaming method-R to write a recovery.conf file. This step is where it would differ from the previous PG versions. The -R command will no longer output a recovery.conf file in the db-slave directory. $ ls db-slavebackup_label pg_dynshmem pg_multixact pg_snapshots pg_tblspc pg_xactbase pg_hba.conf pg_notify pg_stat pg_twophase postgresql.auto.confglobal pg_ident.conf pg_replslot pg_stat_tmp PG_VERSION postgresql.confpg_commit_ts pg_logical pg_serial pg_subtrans pg_wal standby.signal The contents of the old recovery.conf file are moved to postgresql.conf and postgresql.auto.conf instead. Let’s examine db-slave/postgresql.auto.conf first, and we will see that pg_basebackup already created the primary_conninfo for us. This line used to be located in recovery.conf and it tells where and how a slave cluster should stream from the master cluster. Make sure this line is present in the postgresql.auto.conf. db-slave/postgresql.auto.conf# Do not edit this file manually!# It will be overwritten by the ALTER SYSTEM command.primary_conninfo = 'user=cary passfile=''/home/caryh/.pgpass'' host=127.0.0.1 port=5432 sslmode=prefer sslcompression=0 gssencmode=disable target_session_attrs=any' Let’s examine db-slave/postgresql.conf and update some of the parameters. db-slave/postgresql.confwal_level = replicaarchive_mode = onmax_wal_senders = 10 wal_keep_segments = 10hot_standby = onarchive_command = 'test ! -f /home/caryh/streaming-replication/archivedir/%f &amp;&amp; cp %p /home/caryh/streaming-replication/archivedir/%f'wal_log_hints = onport = 5433restore_command = 'cp /home/caryh/streaming-replication/archivedir/%f %p'archive_cleanup_command = 'pg_archivecleanup /home/caryh/streaming-replication/archivedir %r' Since db-slave/postgresql.conf is directly copied from master cluster via pg_basebackup, we will need to change the port to some port different (5433 in this case) from the master since both are running on the same machine. We will need to fill the restore_command and archive_cleanup_command so the slave cluster knows how to get the archived WAL files for streaming purposes. These two parameters used to be defined in recovery.conf and are moved to postgresql.conf in PG12. In the db-slave directory, please note that a new standby.signal file is created automatically by pg_basebackup to indicate that this slave cluster will be run in standby mode. The standby.signal file is a new addition in PG12 to replace standby_mode = 'on' that used to be defined in recovery.conf. If this file is not present, make sure it is created by: $ touch db-slave/standby.signal Now, let’s start the slave cluster: $ pg_ctl -D db-slave start 4. Verify the Streaming Replication SetupOnce both master and slave clusters are setup and running, we should see from the ps -ef command that some of the backend processes are started to achieve the replication, namely, walsender and walreceiver. $ ps -ef | grep postgrescaryh 12782 2921 0 16:12 ? 00:00:00 /usr/local/pgsql/bin/postgres -D db-mastercaryh 12784 12782 0 16:12 ? 00:00:00 postgres: checkpointer caryh 12785 12782 0 16:12 ? 00:00:00 postgres: background writer caryh 12786 12782 0 16:12 ? 00:00:00 postgres: walwriter caryh 12787 12782 0 16:12 ? 00:00:00 postgres: autovacuum launcher caryh 12788 12782 0 16:12 ? 00:00:00 postgres: archiver last was 000000010000000000000002.00000028.backupcaryh 12789 12782 0 16:12 ? 00:00:00 postgres: stats collector caryh 12790 12782 0 16:12 ? 00:00:00 postgres: logical replication launcher caryh 15702 2921 0 17:06 ? 00:00:00 /usr/local/pgsql/bin/postgres -D db-slavecaryh 15703 15702 0 17:06 ? 00:00:00 postgres: startup recovering 000000010000000000000003caryh 15708 15702 0 17:06 ? 00:00:00 postgres: checkpointer caryh 15709 15702 0 17:06 ? 00:00:00 postgres: background writer caryh 15711 15702 0 17:06 ? 00:00:00 postgres: stats collector caryh 15713 15702 0 17:06 ? 00:00:00 postgres: walreceiver streaming 0/3000148caryh 15714 12782 0 17:06 ? 00:00:00 postgres: walsender cary 127.0.0.1(59088) streaming 0/3000148caryh 15728 10962 0 17:06 pts/5 00:00:00 grep --color=auto post We can also check the replication status in details by issuing a query to the master cluster: $ psql -d clusterdb -U cary -c \"select * from pg_stat_replication;\" -x -p 5432-[ RECORD 1 ]----+------------------------------pid | 15714usesysid | 16384usename | caryapplication_name | walreceiverclient_addr | 127.0.0.1client_hostname | client_port | 59088backend_start | 2019-10-29 17:06:49.072082-07backend_xmin | state | streamingsent_lsn | 0/3000148write_lsn | 0/3000148flush_lsn | 0/3000148replay_lsn | 0/3000148write_lag | flush_lag | replay_lag | sync_priority | 0sync_state | asyncreply_time | 2019-10-29 17:10:09.515563-07 Lastly, we can insert additional data to the master cluster and verify that slave also has the data updated. # Query slave cluster$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 100(1 row)# Query master cluster$ psql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5432 count ------- 100(1 row)# Insert more data to master cluster$ psql -d clusterdb -U cary -c \"INSERT INTO test_table(x) SELECT y FROM generate_series(1, 100) a(y)\" -p 5432INSERT 0 100# Query slave cluster againpsql -d clusterdb -U cary -c \"SELECT count(*) from test_table\" -p 5433 count ------- 200(1 row) Both master and slave clusters are now in sync. 5. Setup Replication SlotsThe previous steps illustrate how to correctly setup streaming replication between a master and slave cluster. However, there may be a case where the slave can be disconnected for some reason for extended period of time and may fail to replicate with the master when some of the un-replicated WAL files are recycled or deleted from the master cluster controlled by wal_keep_segments parameter. Replication slots ensure master can retain enough WAL segments for all slaves to receive them and prevent the master from removing rows that could cause a recovery conflict on the slaves. Let’s create a replication slot on the master cluster called slave: $ psql -d clusterdb -U cary -c \"select * from pg_create_physical_replication_slot('slave')\" -p 5432 slot_name | lsn -----------+----- slave | (1 row)$ psql -d clusterdb -U cary -c \"select * from pg_replication_slots\" -x -p 5432-[ RECORD 1 ]-------+---------slot_name | slaveplugin | slot_type | physicaldatoid | database | temporary | factive | factive_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | We have just created replication slot on master called slave and it is currently not active (active = f). Let’s modify slave’s postgresql.conf and make it connect to the master’s replication slot db-slave/postgresql.confprimary_slot_name = 'slave' Please note that this argument primary_slot_name us also used to be defined in recovery.conf and moved to postgresql.conf in PG12. After the change, we are required to restart the slave. $ pg_ctl -D db-slave stop$ pg_ctl -D db-slave start If all is good, checking the replication slots on master should have the slot status as active. $ psql -d clusterdb -U cary -c \"select * from pg_replication_slots\" -x -p 5432-[ RECORD 1 ]-------+----------slot_name | slaveplugin | slot_type | physicaldatoid | database | temporary | factive | tactive_pid | 16652xmin | catalog_xmin | restart_lsn | 0/3003B98confirmed_flush_lsn | 6. SummaryIn this blog, we have discussed the updated procedures to setup streaming replication clusters in PG12, in which several steps have been changed from the older versions, particularly the removal of recovery.conf. Here is a short list of changes related to replication setup that have been moved from recovery.conf restore_command =&gt; moved to postgresql.conf recovery_target_timeline =&gt; moved to postgresql.conf standby_mode =&gt; replaced by standby.signal primary_conninfo =&gt; moved to postgresql.conf or postgresql.auto.conf archive_cleanup_command =&gt; moved to postgresql.conf primary_slot_name =&gt; moved to postgresql.conf","link":"/2019/10/29/Steaming-Replication-Setup-in-PG12-How-to-do-it-right/"},{"title":"Understanding Security Features in PostgreSQL - Part 2","text":"1. IntroductionThis is part 2 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing TLS in greater details. I will begin by going over some of the most important security concepts around TLS before jumping into enabling TLS on PostgreSQL server. I believe it is crucial to have sufficient background information on TLS before tweaking the TLS settings in both client and server sides. In part 1 of this blog, we mostly discussed about authentication and authorization (AA), which is important to identify which client is permitted to connect and which table or column he/she is permitted to operate. Even with the strongest authentication and authorization, the actual communication between client and server will not be encrypted unless Transport Layer Security (TLS) is specifically enabled in the database server. TLS is one of the least understood but commonly used security protocol that ensures the security of many HTTPS sites and other services. TLS is a big protocol and this blog will describe how it works and how to enable TLS in your PostgreSQL server. Here is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. Security Concepts around TLSBefore we jump into configuring TLS in PostgreSQL. It is super important to have some background information on the following security topics build around TLS. 2.1 Symmetrical EncryptionSymmetrical Encryption is a type of encryption where only one secret key is used to encrypt and decrypt a message. In other words, the connecting client will use the secret key to encrypt the message and send to server, the server uses the same key to decrypt the ciphered message and obtain the original message. This is a very fast encryption operation and may sound simple, but the challenge here is how to securely share this one and only secret key between the client and server, how long should the secret key be used before next rotation? Should the secret key be pre-configured on both client and server sides? Should third-party key management software be integrated? These are some of the common challenges with symmetrical encryption. The following is some of the most common symmetrical encryption algorithms today with the AES being the most popular: (reference: https://en.wikipedia.org/wiki/Symmetric-key_algorithm). Each algorithm supports key lengths having multiple sizes and normally is denoted after the encryption algorithm name, for example, AES-128, AES-256…etc. AES (Advanced Encryption Standard) DES (Data Encryption Standard) Triple DES Blowfish Symmetrical encryption is normally paired with a Block Cipher Mode of Operation to encrypt or decrypt a stream of data. Imagine there is a data stream of size 30GB that needs to be encrypted. Without Block Cipher Mode, we will have to load all 30GB of data into memory and encrypt it with (say AES128) and most likely we do not large enough memory to load all the data stream. This is where Block Cipher Mode of Operations come in handy, it encrypts the data stream block by block (most likely 16 byte block) until the entire block is encrypted. We basically can encrypt the 30GB data stream without having to have at least 30GB of memory. 2.2 Asymmetrical Encryption (a.k.a Public Key Cryptography)Unlike symmetrical encryption, asymmetrical encryption uses two distinct keys called public and private keys; Public key is used for encryption and private key is used for decryption. Both keys are different but related by math and it is much slower than symmetrical encryptions. As name implies, public key can be distributed publicly while private key is to be kept private as it is the only key that is able to decrypt the messages encrypted by public key. This essentially forms a secured one-way communication. Generally, asymmetrical encryption is not desirable to be used as stream data encryption algorithm though it is more secured; it requires more computational power to perform encryption and decryption and this is a major drawback. Asymmetrical encryption is commonly used as authentication protocol for a client to verify that server is indeed valid. During a TLS handshake for example, server will present its TLS certificate, which contains a public key, to the client, client uses the public key to encrypt a message and asks the server to decrypt with its private key and send back the result. If message match, then client is sure that the server possess the private key and therefore is valid. The following is some of the most common asymmetrical encryption algorithms today with the RSA and Elliptic curve being the most popular: (reference: https://en.wikipedia.org/wiki/Public-key_cryptography). RSA DSS Elliptic curve 2.3 Block Cipher Mode of Operation (a.k.a Stream Cipher)Block Cipher Mode of Operation is normally used with Symmetrical encryption to encrypt or decrypt a stream of data block by block. There are several available modes of block cipher operations that have different strengths and weaknesses. Most modes require a complete block of 16 bytes to be able to encrypt. In the case where the input stream is not in multiple of 16, padding is normally use to fill the block. The following is some of the most common block cipher mode of operations today with the CBC and CTR being the most popular and ECB being the least secured: (reference: https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation). Cipher Block Chaining (CBC) Counter (CTR) Cipher Feedback (CFB) Output Feedback (OFB) Electronic Codebook (ECB) The mode is normally denoted with the desired symmetrical encryption algorithm, for example, AES-128-CBC or AES-256-CTR are quite common. 2.4 Key Exchange AlgorithmKey exchange algorithm is a math algorithm designed to make both client and server agree on a secret key without actually sending the key to each other. This is done by pure math equations and require several steps of intermediate token exchange. In the end both client and server will end up with the same value, which is to be used as the secret key for symmetrical encryption algorithms. Key exchange algorithm is common used in many services such as SSH and TLS. Services like these normally have a handshake stage where both client and server have to agree on the subsequent algorithms to use for encryption and perform key exchange algorithm to get the secret key for symmetrical encryption. The following is some of the most common key exchange algorithms with diffie-hellman and elliptic curve diffie-hellman being the most popular (reference: https://en.wikipedia.org/wiki/Key_exchange). Diffie-Hellman (DH) Elliptic Curve Diffie-Hellman (ECDH) Ephemeral Diffie-Hellman (DHE) RSA 2.6 Data Integrity Check / AuthenticationThe data integrity authentication is not to be confused with host-based or role-based authentication mentioned in part 1. Data integrity authentication refers to the methods to ensure that the data stream has been received without being altered during transmission. Think of it as a data checksum. In addition to encryption, ensuring data integrity is also very important security measure to avoid man-in-the-middle attack. Please note that data integrity check and data encryption are 2 separate processes, meaning that you can have data authentication without encryption, or encryption without authentication. SNMPv3 is a good example that treats data authentication and encryption separately while TLS requires both at the same time. The following is some of the most common hash algorithms with SHA1 and MD5 being the most common (reference: https://en.wikipedia.org/wiki/Message_authentication). SHA1 SHA2 MD5 BLAK2 2.5 TLS Certificate and Chain of TrustTLS certificate and chain of trust are the core concepts in TLS to ensure maximum trust between a client and a server. The certificate used by PostgreSQL is X509 version 3 certificate, which has extension support to further refine the purpose of the certificate issued. The certificates are created and signed in hierarchy. The certificate created at the top hierarchy is called a root CA (root Certificate Authority) and is normally created by a trusted organization. This root CA is able to sign additional Intermediate CA that can be distributed to other organizations. The intermediate CA can then be used to create and sign individual certificates to be used by services like HTTPS, FTPS…etc. There are several types of TLS certificate and each has its own place in the certificate hierarchy and serve different purposes. A TLS certificate is a small data file that contains the public key, organization details, trustee’s digital signature, extensions and validity dates. Normally a TLS certificate is generated with a private key. The key pair bounded with the certificate is important as they are required for authentication when a TLS client wishes to connect to the server. The following image illustrates the idea of certificate trust chain: As you can see, the root CA is on top of hierarchy and is able to generate and sign additional intermediate CA and issue to several organizations. The organization then is able to take the intermediate CA and generate additional CA-signed certificates and matching private keys to use in their services such as PostgreSQL server, FTPS and HTTPS server. A CA certificate can be purchased from a trusted organization or generated by oneself using openssl and java key tool. We will go over the procedure to generate these certificates using OpenSSL as examples in part 3 of this blog. 2.7 TLS versionsTLS is a newer protocol that replaces its predecessor, Secured Sockets Layer (SSL). Below is a list of TLS versions that we should use as of today: TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3 TLSv1.3 is the newest TLS version that has significant improvement in the handshake process and introduces many more cipher suites specifically designed for TLSv1.3. Before TLSv1.3, TLSv1.2 is the most popular TLS version deployed in the world today. PostgreSQL server defaults to accept client connection that supports minimum TLS version to be TLSv1.2 and will reject any connection in the versions earlier than v1.2 2.8 TLS Cipher Suite and TLS handshakeTLS cipher suite refers to a set of algorithms that help secure a network connection. The suite of algorithms normally contains Key exchange algorithm Authentication algorithm Asymmetrical encryption algorithm Message authentication algorithm for example, a TLSv1.2 cipher suite TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 indicates the following DHE - use Ephemeral Diffie-Hellman key exchange algorithm RSA - use RSA asymmetrical keys for authentication AES_256_CBC - use AES-256 symmetrical encryption with CBC block cipher mode SHA256 - use SHA-256 as message authentication algorithm to make sure exchanged messages are not tempered with. When a TLS client initiates a TLS connection to a server, a TLS handshake process takes place that roughly performs the following: Agree on the TLS version to use. Abort if version cannot be agreed Agree on the cipher suite to use. Abort if cipher suite cannot be agreed Certificate exchange Client authenticates the server using agreed algorithm perform key exchange using agreed algorithm ensure handshake message is not tempered with the agreed message authentication algorithm secured communication then begins. 7. SummaryTLS is a big protocol involving a lot of steps including certificate exchange, chain of trust verification, key exchange, cipher suite exchange, authentication, data integrity check and finally symmetrical encryption of application data with appropriate block cipher mode. Having adequate fundamental understanding to TLS is crucial to ensure a correct and secured database environment setup. Of course there is more to what we have discussed here so far and I will be producing more articles in the near future to address some of the advanced TLS related practices.","link":"/2020/01/13/Understanding-Security-Features-in-PostgreSQL-Part2/"},{"title":"Understanding Security Features in PostgreSQL - Part 3","text":"1. IntroductionThis is part 3 of the blog “Understanding Security Features in PostgreSQL”, in which I will be discussing how to apply TLS in both PostgreSQL server and client using the principles we have learned in part 2 of the blog. In the end, I will also briefly talk about Transparent Data Encryption (TDE) and security vulnerability. Here is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. Preparing TLS CertificatesBefore we can utilize TLS to secure both the server and the client, we must prepare a set of TLS certificates to ensure mutual trust. Normally the CA (Certificate Authority) certificates can be purchased from a trusted organization and used it to create more CA-Signed certificates for services and applications. In this section, I will show you how to create your own CA Certificate and CA-Signed certificates using OpenSSL command line tool for both PostgreSQL server and client. You may also have heard the term self-signed certificate. This type of certificate is not signed by a trusted CA and is normally considered insecured in many applications. We will not go over the self-signed certificate generation in this blog. 2.1 Generate a Private Key for CA CertificateRemember in last blog we mention that each certificate contains organization information and public key, which is paired with a private key file. Let’s generate a private key file for our CA first. $ openssl genrsa -des3 -out cacert.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)...............+++++........................+++++e is 65537 (0x010001)Enter pass phrase for cacert.key:Verifying - Enter pass phrase for cacert.key: Your will be prompted with pass phrase, which is recommended to provide as it will prevent someone else from generating more root CA certificate from this key. 2.2 Generate CA Certificate Using the Private keyNow, let’s generate the CA Certificate with the private key $ openssl req -x509 -new -nodes -key cacert.key -sha256 -days 3650 -out cacert.pemEnter pass phrase for cacert.key:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CAState or Province Name (full name) [Some-State]:BCLocality Name (eg, city) []:VancouverOrganization Name (eg, company) [Internet Widgits Pty Ltd]:HighGoOrganizational Unit Name (eg, section) []:SoftwareCommon Name (e.g. server FQDN or YOUR name) []:va.highgo.comEmail Address []:cary.huang@highgo.ca Please note that OpenSSL will prompt you to enter several pieces of organizational information that identifies the CA certificate. You should enter these information suited to your organization. The most important field is Common Name, which is commonly checked against the hostname or domain name of the service. Depending on the security policy, some server will enforce the rule that common name must equal its host / domain name; some servers do not have this restriction. 2.3 Generate a private key for CA-Signed certificateLike in the CA case, CA-signed certificate is also paired with a private key file $ openssl genrsa -out server.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)...................+++++............................................................................+++++e is 65537 (0x010001) 2.4 Generate a Certificate Signing Request for CA-Signed certificateThen we create a Certificate Signing Request (CSR), which contains a list of organizational information to be presented to the CA server for verification. The CA server then decide if the CSR should be granted a new certificate according to the security policy configured. Since we are using OpenSSL for certificate generation, the CA server here refers to OpenSSL itself, and the security policy configuration is located in openssl.cnf, which is commonly located in /usr/local/ssl/openssl.cnf. In an enterprise environment where Public Key Infrastructure (PKI) is deployed, the CA Server could refer to an actual service whose sole purpose is to verify incoming CSRs and renew or issue new certificates to requesting clients. $ openssl req -new -key server.key -out server.csrYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [AU]:CAState or Province Name (full name) [Some-State]:BCLocality Name (eg, city) []:VancouverOrganization Name (eg, company) [Internet Widgits Pty Ltd]:HighGoOrganizational Unit Name (eg, section) []:SoftwareCommon Name (e.g. server FQDN or YOUR name) []:va.highgo.caEmail Address []:cary.huang@highgo.caPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:HighGo Canada 2.5 Generate a CA-Signed certificateSince we are generating CA-signed certificate with OpenSSL locally, we can configure how the certificate should be generated using openssl.cnf file. We will just be using the default policy set in openssl.cnf. Here’s a snapshot of the default settings: [ usr_cert ]# These extensions are added when 'ca' signs a request.# This goes against PKIX guidelines but some CAs do it and some software# requires this to avoid interpreting an end user certificate as a CA.basicConstraints=CA:FALSE# Here are some examples of the usage of nsCertType. If it is omitted# the certificate can be used for anything *except* object signing.# This is OK for an SSL server.# nsCertType = server# For an object signing certificate this would be used.# nsCertType = objsign# For normal client use this is typical# nsCertType = client, email# and for everything including object signing:# nsCertType = client, email, objsign# This is typical in keyUsage for a client certificate.# keyUsage = nonRepudiation, digitalSignature, keyEncipherment Let’s generate the CA-signed certificate. Note that the command will take cacert.pem, cacert.key and server.csr as inputs, in which we have already generated from previous steps. server.pem will be the output. $ openssl x509 -req -in server.csr -CA cacert.pem -CAkey cacert.key -CAcreateserial -out server.pem -days 3650 -sha256Signature oksubject=C = CA, ST = BC, L = Vancouver, O = HighGo, OU = Software, CN = va.highgo.ca, emailAddress = cary.huang@highgo.caGetting CA Private KeyEnter pass phrase for cacert.key: We can repeat from step 2.3 to 2.5 to generate a new pair for the client application. To conclude, we have the following files generated: cacert.pem - Root CA certificate that is at the top of the chain of trust. We use it to sign and create other certificates cacert.key - key for the Root CA Certificate - must keep it secured. server.pem - CA-signed certificate for server application server.key - key for the server certificate client.pem - CA-signed certificate for client application client.key - key for the client certificate 3. Enabling Transport Layer Security (TLS) to PostgreSQL ServerPostgreSQL has native support for TLS to secure connection between client and server. The TLS support has to be enabled during build time and requires OpenSSL libraries. Depending on the versions of OpenSSL that the client or server is built with, TLS versions and ciphersuites may differ as well. This does not mean that both client and server must be linked with the same version of OpenSSL. It is possible that a client with older OpenSSL can connect to a server with newer OpenSSL if the server is configured to accept it. The TLS handshake process is initiated when a client first connects to the server in which they will evaluate TLS version used and negotiate ciphersuite that both ends are able to support. In this case, the server may use less secured ciphersuite and TLS version to communicate with the client, which may not be ideal. The TLS support for a PostgreSQL server can be enabled in postgresql.conf. postgresql.confssl = onssl_ca_file = '~/cert/cacert.pem'ssl_cert_file = '~/cert/server.pem'ssl_crl_file = ''ssl_key_file = '~/cert/server.key'ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphersssl_prefer_server_ciphers = onssl_ecdh_curve = 'prime256v1'ssl_min_protocol_version = 'TLSv1.2'ssl_max_protocol_version = ''ssl_dh_params_file = ''ssl_passphrase_command = ''ssl_passphrase_command_supports_reload = off Let’s examine the configuration parameters. ssl = on This line turns on the TLS support. Please note that even if TLS is turned on, the server will still be able to accept connections that do not use TLS. Normally, the client is the entity that decides if TLS should be used or not. The server can also enforce the incoming connections to use TLS by modifying the pg_hba.conf file like this, where the connections from 172.16.30.0/24 must be TLS, otherwise the server will deny. hostssl sales_team all 172.16.30.0/24 trust ssl_ca_file = '~/cert/cacert.pem'ssl_cert_file = '~/cert/server.pem'ssl_crl_file = ''ssl_key_file = '~/cert/server.key' These 4 lines tell PostgreSQL where to load the X509 certificate, the CA certificate, server private key and the certificate revocation list. These certificates must be pre-generated by OpenSSL command or purchased from a trusted organization. For TLS to work, ssl_ca_file, ssl_cert_file and ssl_key_file must be provided. We will use the certificates we have generated for server in the previous section. The file pointed by ssl_ca_file will be used to determined if the certificate can be trusted by deriving the chain of trust.The file pointed by ssl_cert_file will be sent to the connecting client during TLS handshake for authentication purposes.The file pointed by ssl_key_file will be used for asymmetrical encryption during authentication The file pointed by ssl_crl_file is optional and it contains a list of certificates that cannot be trusted (or revoked). Distributing revoked certificates using this file is not the most ideal but still being practice today. It may have performance impact if the list is very large and it introduces a problem of when the list should be renewed and how often. Online Certificate Status Protocol (OCSP. ref:https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol) is a newer protocol designed for Public Key Infrastructure (PKI) for querying certificate revocation status that addresses some of the issues with revocation file. Feel free to give a read on OCSP in the link above. ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphersssl_prefer_server_ciphers = onssl_ecdh_curve = 'prime256v1' During TLS handshake, both client and server will present to each other a list of desired ciphersuites ordered by preference. Handshake process will go through both lists and find a common ciphersuite supported by both sides or abort if there is nothing in common. The ssl_ciphers configuration is used to configure the size of the ciphersuite lists to be presented to the client during handshake. ssl_ciphers is a string list consisting of one or more cipher strings separated by colons ( ref: https://www.openssl.org/docs/man1.1.1/man1/ciphers.html) and defaults to HIGH:MEDIUM:+3DES:!aNULL which translates to: allows high strength ciphersuites (HIGH) allows medium strength ciphersuites (MEDIUM) move any ciphersuite using 3DES algorithm to the end of the list (+3DES) remove any ciphersuite that does not have authentication algorithm (!aNULL) For example, “HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256” will use high strength ciphersuites while removing any ciphersuites containing ADH, MD5, RC4…etc. Before applying the cipher string to PostgreSQL, it is recommended to check the output cipher list after tuning the cipher string using Openssl client tool. $ openssl ciphers -v 'HIGH:!ADH:!MD5:!RC4:!SRP:!PSK:!DSS:!ECDHE:!ECDSA:!EDH:!DH:!ECDH:!CAMELLIA256'TLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEADTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEADTLS_AES_128_GCM_SHA256 TLSv1.3 Kx=any Au=any Enc=AESGCM(128) Mac=AEADAES256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(256) Mac=AEADAES256-CCM8 TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM8(256) Mac=AEADAES256-CCM TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM(256) Mac=AEADARIA256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=ARIAGCM(256) Mac=AEADAES128-GCM-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(128) Mac=AEADAES128-CCM8 TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM8(128) Mac=AEADAES128-CCM TLSv1.2 Kx=RSA Au=RSA Enc=AESCCM(128) Mac=AEADARIA128-GCM-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=ARIAGCM(128) Mac=AEADAES256-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA256AES128-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA256CAMELLIA128-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=Camellia(128) Mac=SHA256AES256-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA1AES128-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA1CAMELLIA128-SHA SSLv3 Kx=RSA Au=RSA Enc=Camellia(128) Mac=SHA1 ssl_prefer_server_ciphers specifies whether to use the server’s SSL cipher preferences, rather than the client’s. It should always be on for more control in terms of ciphersuite selection. ssl_ecdh_curve specifies the name of the curve to use in ECDH key exchange algorithms and is useful only if the ciphersuite uses ECDHE key exchange algorithm. The most common curves are : prime256v1, secp384r1 and secp521r1 and normally leaving it default should suffice. ssl_min_protocol_version = 'TLSv1.2'ssl_max_protocol_version = '' These 2 lines configure the minimum and maximum TLS versions to accept. By default the server will only serve the TLS client using TLSv1.2 and above. TLSv1.2 is a very secured TLS version and it is widely used in the world. Normally, we only change the minimum TLS version with assumption that all future versions will be more secured and for this reason, we normally don’t put restriction on the max version. TLSv1.3 is recently introduced that has new ciphersuite support and has more improvement in the handshake process. To enforce TLSv1.3 to be used, set the ssl_min_protocol_version to ‘TLSv1.3’ will suffice. ssl_dh_params_file = ''ssl_passphrase_command = ''ssl_passphrase_command_supports_reload = off ssl_dh_params_file points to a file that contains custom diffie-hellman key exchange algorithm parameter. This is an optional parameter and is only useful if the ciphersuite uses DHE key exchange algorithm. If left empty, compiled-in defaults will be used. Custom DH parameters can be generated using command openssl dhparam -out dhparams.pem 2048 and will normally reduce the attack exposure as attacker will have hard time cracking the key exchange process using custom parameter instead of the well-known default. ssl_passphrase_command is the command to obtain the password for the private key file specified by ssl_key_file. There is an option to add a password to a private key file during its generation and if password is used, ssl_passphrase_command must be set with the system command that will retrieve such password. Otherwise, TLS handshake will abort as PostgreSQL will not be able to access private key without password. ssl_passphrase_command_supports_reload configures if the ssl_passphrase_command should be re-run at every reload (ie. SIGHUP). It is default to off, so the ssl_passphrase_command will not be run at every reload. 4. Enabling Transport Layer Security (TLS) to PostgreSQL ClientNow that we have a PostgreSQL server with TLS setup, we can use psql client to connect to the server also using TLS. Depending on the client connect parameters given, we can utilize TLS in different security levels. I will show the most common usages here: # Case 1: connect to server in TLS mode$ psql -U user -h localhost -d \"sslmode=require dbname=postgres\"# Case 2: connect to server in TLS mode if server supports it$ psql -U user -h localhost -d \"sslmode=prefer dbname=postgres\"# Case 3: connect to server in TLS mode and verify server CA against client CA$ psql -U user -h localhost -d \"sslmode=verify-ca dbname=postgres sslrootcert=~/cert/cacert.pem\"# Case 4: connect to server in TLS mode and present client certificate. Verify all certificate details and trust chain. Check certificate revocation list does not contain server cert.$ psql -U user -h localhost -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\" The usage in Case 4 is the most secured because both server and client will verify each other’s certificate and decide if both can be mutually trusted. The common name field in the certificate is checked against the server hostname; certificate validity period is checked, organization details are checked; certificate trust chain is checked; revocation list is checked. Please note that PostgreSQL server with TLS enabled by default does not force the client to present a TLS certificate for verification. If client presents one like in Case 4 above, the server will verify and deny connection is certificate is bad. If client does not provide a certificate like in Case 1 ~ 3, the server will skip the client certificate verification as there is nothing to verify, which is less secure. To enforce the connecting client to present a TLS certificate for verification, we will need to add a special clientcert=1 argument in existing authentication rules defined in pg_hba.conf. # TYPE DATABASE USER ADDRESS METHODhostssl production production_user 0.0.0.0/0 trust clientcert=1hostssl marketing marketing_user 0.0.0.0/0 pam clientcert=1hostssl sales sales_user 0.0.0.0/0 md5 clientcert=1hostssl software software_user 0.0.0.0/0 password clientcert=1hostssl hardware hardware_user 0.0.0.0/0 scram-sha-256 clientcert=1hostssl hardware hardware_user 0.0.0.0/0 cert clientcert=1 The example above will enforce connecting client to present TLS certificate to access production_team database as production_user. If a TLS certificate is not provided by client, the connection will abort. 5. TLS Connect Examples$ psql -U user -h localhost -d \"sslmode=require dbname=postgres\"psql (13devel)SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)Type \"help\" for help.postgres=# Please note that psql prints the TLS version used (TLSv1.2) and the cipher suite negotiated during handshake (ECDHE-RSA-AES256-GCM-SHA384). Below is the wireshark capture of the above TLS connection: Another Example: $ psql -U cary -h localhost -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\"psql: error: could not connect to server: server certificate for \"va.highgo.ca\" does not match host name \"localhost\" Here, we have an error when we set sslmode to verify-full, where both server and client will verify each other with the most strict criteria. This error happens because the Common Name field in the certificate does not match the host name. Did I mention that Common Name is the most important field of a certificate? To resolve this error, we can either re-generate certificate with matching Common name, or change the host name. I simply add an entry to /etc/hosts to resolve the error 127.0.0.1 localhost127.0.0.1 va.highgo.ca and the error will disappear when both Common Name and Hostname match $ psql -U cary -h va.highgo.ca -d \"sslmode=verify-full dbname=postgres sslrootcert=~/cert/cacert.pem sslcert=~/cert/client.pem sslkey=~/cert/client.key\"psql (13devel)SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)Type \"help\" for help. Please note that this command also forces the client to submit a certificate to server as well as seen from the wireshark capture. We can tell by looking at the length field of the packet capture. There are 2 exchanges having lengths = 2675 and 2446. Those are the actual certificate contents being transmitted. Previous capture only has 1 exchanges having packet length = 2675; it means only server is providing certificate to client for verification. 6. Transparent Data Encryption (TDE)Transparent Data Encryption refers to the process of protecting data at rest by encrypting database files on the hard disk level and decrypting them while reading from hard disk. This is to prevent physical storage media theft. This is called transparent because the encryption and decryption happen between PostgreSQL server and the physical hard disk and it is not visible to the client applications. TDE uses symmetrical encryption for securing blocks of database files such as shared buffer and WAL files, and it is designed to accompany with a internal Key Management System (KMS) to manage the lifecycle of the encryption keys. TDE and KMS are still under development by the PostgreSQL community. The KMS feature is expected to be released in PG13 while the TDE feature to be in PG14. With its completion, it will add another layer of security feature on top of already security-rich PostgreSQL database. 7. Security VulnerabilitySecurity Vulnerability is a weakness which can be exploited by an attacker to perform unauthorized actions, sabotage a service, or inject malicious software or virus. These weaknesses are generally implementation mistakes, undiscovered bugs or a legacy problem that require an update to the server to resolve. PostgreSQL also has a list of known security vulnerability that has been discovered and fixed by the community. The list can be found here: https://www.postgresql.org/support/security/. These vulnerability ranges from different severity levels, from simple memory leak to crash the server. This is why doing regular PostgreSQL server upgrade is important because each minor release fixes some of the discovered security vulnerabilities and therefore reducing the attack surface on your server. 8. SummaryIn part 3 of the blog, we have learned and understood what each TLS related configuration means in postgresql.conf and how to initiate TLS connection with psql client. We learned that keeping PostgreSQL server up-to-date can reduce the attack surface on some of the discovered vulnerabilities. We can ensure a fairly secured database network environment with TLS having adequate understanding of its fundamentals and practices. With the TDE feature coming in near future, we can further secure the database environment in the disk level and prevent possible data loss due to disk theft.","link":"/2020/01/20/Understanding-Security-Features-in-PostgreSQL-Part3/"},{"title":"A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function","text":"1. OverviewTable partitioning is introduced after Postgres version 9.4 that provides several performance improvement under extreme loads. Partitioning refers to splitting one logically large table into smaller pieces, which in turn distribute heavy loads across smaller pieces (also known as partitions). There are several ways to define a partition table, such as declarative partitioning and partitioning by inheritance. In this article we will focus on a simple form of declarative partitioning by value range. Later in this article, we will discuss how we can define a TRIGGER to work with a FUNCTION to make table updates more dynamic. 2. Creating a Table Partition by RangeLet’s define a use case. Say we are a world famous IT consulting company and there is a database table called salesman_performance, which contains all the sales personnel world wide and their lifetime revenue of sales. Technically it is possible to have one table containing all sales personnel in the world but as entries get much larger, the query performance may be greatly reduced. Here, we would like to create 7 partitions, representing 7 different levels of sales (or ranks) like so: CREATE TABLE salesman_performance ( salesman_id int not NULL, first_name varchar(45), last_name varchar(45), revenue numeric(11,2), last_updated timestamp) PARTITION BY RANGE (revenue); Please note that, we have to specify that it is a partition table by using keyword “PARTITION BY RANGE”. It is not possible to alter a already created table and make it a partition table. Now, let’s create 7 partitions based on revenue performance: CREATE TABLE salesman_performance_chief PARTITION OF salesman_performance FOR VALUES FROM (100000000.00) TO (999999999.99);CREATE TABLE salesman_performance_elite PARTITION OF salesman_performance FOR VALUES FROM (10000000.00) TO (99999999.99);CREATE TABLE salesman_performance_above_average PARTITION OF salesman_performance FOR VALUES FROM (1000000.00) TO (9999999.99);CREATE TABLE salesman_performance_average PARTITION OF salesman_performance FOR VALUES FROM (100000.00) TO (999999.99);CREATE TABLE salesman_performance_below_average PARTITION OF salesman_performance FOR VALUES FROM (10000.00) TO (99999.99);CREATE TABLE salesman_performance_need_work PARTITION OF salesman_performance FOR VALUES FROM (1000.00) TO (9999.99);CREATE TABLE salesman_performance_poor PARTITION OF salesman_performance FOR VALUES FROM (0.00) TO (999.99); Let’s insert some values into “salesman_performace” table with different users having different revenue performance: INSERT INTO salesman_performance VALUES( 1, 'Cary', 'Huang', 4458375.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 2, 'Nick', 'Wahlberg', 340.2, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 3, 'Ed', 'Chase', 764.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 4, 'Jennifer', 'Davis', 33750.12, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 5, 'Johnny', 'Lollobrigida', 4465.23, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 6, 'Bette', 'Nicholson', 600.44, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 7, 'Joe', 'Swank', 445237.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 8, 'Fred', 'Costner', 2456789.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 9, 'Karl', 'Berry', 4483758.34, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 10, 'Zero', 'Cage', 74638930.64, '2019-09-20 16:00:00');INSERT INTO salesman_performance VALUES( 11, 'Matt', 'Johansson', 655837.34, '2019-09-20 16:00:00'); Postgres will automatically distribute queries to the respective partition based on revenue range. You may run the \\d+ command to see the table and its partitions or examine just salesman_performance, which shows partition key and range \\d+ salesman-performance we can also use EXPLAIN ANALYZE query to see the query plan PG system makes to scan each partition. In the plan, it indicates how many rows of records exist in each partition EXPLAIN ANALYZE SELECT * FROM salesman_performance; There you have it. This ia a very basic partition table that distributes data by value range. One of the advantages of using partition table is that bulk loads and deletes can be done simply by adding or removing partitions (DROP TABLE). This is much faster and can entirely avoid VACUUM overhead caused by DELETE When you make a update to an entry. Say salesman_id 1 has reached the “Chief” level of sales rank from “Above Average” rank UPDATE salesman_performance SET revenue = 445837555.34 where salesman_id=1; You will see that Postgres automatically put salesman_id 1 into the “salesman_performance_chief” partition and removes from “salesman_performance_above_average” 3. Delete and Detach PartitionA partition can be deleted completely simply by the “DROP TABLE [partition name]” command. This may not be desirable in some use cases. The more recommended approach is to use “DETACH PARTITION” queries, which removes the partition relationship but preserves the data. ALTER TABLE salesman_performance DETACH PARTITION salesman_performance_chief; If a partition range is missing, and the subsequent insertion has a range that no other partitions contain, the insertion will fail. INSERT INTO salesman_performance VALUES( 12, 'New', 'User', 755837555.34, current_timestamp);=&gt; should result in failure because no partitions contain a range for this revenue = 755837555.34 If we add back the partition for the missing range, then the above insertion will work: ALTER TABLE salesman_performance ATTACH PARTITION salesman_performance_chiefFOR VALUES FROM (100000000.00) TO (999999999.99); 4. Create Function Using Plpgsql and Define a TriggerIn this section, we will use an example of subscriber and coupon code redemption to illustrate the use of Plpgsql function and a trigger to correctly manage the distribution of available coupon codes. First we will have a table called “subscriber”, which store a list of users and a table called “coupon”, which stores a list of available coupons. CREATE TABLE subscriber ( sub_id int not NULL, first_name varchar(45), last_name varchar(45), coupon_code_redeemed varchar(200), last_updated timestamp);CREATE TABLE coupon ( coupon_code varchar(45), percent_off int CHECK (percent_off &gt;= 0 AND percent_off&lt;=100), redeemed_by varchar(100), time_redeemed timestamp); Let’s insert some records to the above tables: INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Cary','Huang',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Nick','Wahlberg',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Johnny','Lollobrigida',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Joe','Swank',current_timestamp);INSERT INTO subscriber (sub_id, first_name, last_name, last_updated) VALUES(1,'Matt','Johansson',current_timestamp);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-746353',20);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-653834',30);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-538463',40);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-493567',50);INSERT INTO coupon (coupon_code, percent_off) VALUES('CXNEHD-384756',95); The tables now look like: Say one subscriber redeems a coupon code, we would need a FUNCTION to check if the redeemed coupon code is valid (ie. Exists in coupon table). If valid, we will update the subscriber table with the coupon code redeemed and at the same time update the coupon table to indicate which subscriber redeemed the coupon and at what time. CREATE OR REPLACE FUNCTION redeem_coupon() RETURNS trigger AS $redeem_coupon$ BEGIN IF EXISTS ( SELECT 1 FROM coupon c where c.coupon_code = NEW.coupon_code_redeemed ) THEN UPDATE coupon SET redeemed_by=OLD.first_name, time_redeemed='2019-09-20 16:00:00' where coupon_code = NEW.coupon_code_redeemed; ELSE RAISE EXCEPTION 'coupon code does not exist'; END IF; RETURN NEW; END;$redeem_coupon$ LANGUAGE plpgsql; we need to define a TRIGGER, which is invoked BEFORE UPDATE, to check the validity of a given coupon code. CREATE TRIGGER redeem_coupon_trigger BEFORE UPDATE ON subscriber FOR EACH ROW EXECUTE PROCEDURE redeem_coupon(); \\d+ subscriber should look like this: Let’s have some users redeem invalid coupon codes and as expected, an exception will be raised if coupon code is not valid. UPDATE subscriber set coupon_code_redeemed='12345678' where first_name='Cary';UPDATE subscriber set coupon_code_redeemed='87654321' where first_name='Nick';UPDATE subscriber set coupon_code_redeemed='55555555' where first_name='Joe'; Let’s correct the above and redeem only the valid coupon codes and there should not be any error. UPDATE subscriber set coupon_code_redeemed='CXNEHD-493567' where first_name='Cary';UPDATE subscriber set coupon_code_redeemed='CXNEHD-653834' where first_name='Nick';UPDATE subscriber set coupon_code_redeemed='CXNEHD-384756' where first_name='Joe'; Now both table should look like this, and now both table have information cross-related. And there you have it, a basic trigger function executed before each update. 5. SummaryWith the support of partitioned table defined by value range, we are able to define a condition for postgres to automatically split the load of a very large table across many smaller partitions. This has a lot of benefits in terms of performance boost and more efficient data management. Having postgres FUNCTION and TRIGGER working together as a duo, we are able to make general queries and updates more dynamic and automatic to achieve more complex operations. As some of the complex logics can be defined and handled as FUNCTION, which is then invoked at appropriate moment defined by TRIGGER, the application integrated to Postgres will have much less logics to implement.","link":"/2019/09/25/A-Guide-to-Basic-Postgres-Partition-Table-and-Trigger-Function/"},{"title":"Can Sequence Relation be Logically Replicated?","text":"1. IntroductionI have noticed that there is a page on the offical PostgreSQL documentation (https://www.postgresql.org/docs/current/logical-replication-restrictions.html) that states several restrictions to the current logical replication design. One of the restrictions is about sequence relation type where any changes associated with a sequence relation type is not logically replicated to the subscriber or to the decoding plugin. This is an interesting restriction and I took the initiative to look into this restriction further and evaluate if it is possible to have it supported. I have consulted several senior members in the PostgreSQL communitiy and got some interesting responses from them. In this blog, I will share my current work in the area of supporting sequence replication. 2. What is a Sequence?Sequence is a special type of relation that is used as a number generator manager, which allows an user to request the next number from the sequence, reset the current value, change the size of increment (or decrement) and perform several other configurations that suit their needs. A sequence is automatically created when an user creates a regular table that contains a column of type SERIAL. Alternatively, a sequence can also be created manually by using the CREATE SEQUENCE seqname; command. A sequence is similar to a regular table except that it can only contain 1 single row, is created with a special schema by default that contains several control parameters for managing the number generation and user cannot use UPDATE clause on a sequence. SQL functions such as nextval(), currval(), setval() and ALTER commands are the proper methods of accessing or modifying sequence data. 3. Why is Sequence not Replicated in Current Design?This is the question I ask myself and the PostgreSQL community for several times and I have received several interesting responses to this question. Like a regular table, sequence also emits a WAL update upon a change to the sequence value but with a major difference. Instead of emitting a WAL update at every nextval() call, sequence actually does this at every 32 increments and it logs a future value 32 increments after instead of current value. Doing WAL logging every 32 increments adds a significant gain in performance according to a benchmark report shared by the community. For example, if current sequence value is 50 with increment of 5, the value that is written to WAL record will be 210, because ( 50 + (32x5) = 210). This also means that in an events of a crash, some sequence values will be lost. Since sequence does not guarentee free of gap and is not part of user data, such a sequence loss is generally ok. Logical replication is designed to track the WAL changes and report to subscribers about the current states and values. It would be quite contradicting to replicate sequence because the current sequence value does not equal to the value stored in the WAL. The subscriber in the sequence case will receive a value that is 32 increments in the future. Another response I have got is that the implementation of sequence intermixed a bunch of transactional and non-transactional states in a very messy way, thus making it difficult to achieve sensible behaviour for logical decoding. 4. Can Sequence Relation be Logically Replicated?In the current PostgreSQL logical replication architecture, yes it is possible to have a patch to replicate changes to a sequence relation. Before we dive in further, we have to understand what the benefit would be if we were able to replicate a sequence. In the current design, an user is able to set up a PostgreSQL publisher and subscriber to replicate a table that could be associated with a sequence if it has a column of data type SERIAL. The values of the table will be copied to the subscriber of course, but the state of sequence will not. In the case of a failover, the subscriber may not be able to insert more data to the table because SERIAL data is often declared as PRIMARY KEY and it could use an unexpected sequence value that conflicts with existing records. To remedy this, PostgreSQL documentation suggests manually copying over the sequence values or use utility such as pg_dump to do the copying. I believe it is the biggest benefit if sequence relation can be replicated such that in a fail over case, the user is no longer required to manually synchronize the sequence states. 5. Where to Add the Sequence Replication Support?Logical replication actually has 2 routes, first is via the logical decoding plugin to a third party subscriber, second is between a PostgreSQL publisher and subscriber. Both routes are achieved differently in multiple source files but both do invoke the same common modules in the replication module in the PostgreSQL source repository. This section will describe briefly these common modules 5.1 Define a New Change TypeSince sequence change has some fundamental difference between the usual changes caused by INSERT, UPDATE or DELETE, it is better to define a new change type for sequence in reorderbuffer.h first: src/include/replication/reorderbuffer.henum ReorderBufferChangeType{ REORDER_BUFFER_CHANGE_INSERT, REORDER_BUFFER_CHANGE_UPDATE, REORDER_BUFFER_CHANGE_DELETE, REORDER_BUFFER_CHANGE_MESSAGE, REORDER_BUFFER_CHANGE_INTERNAL_SNAPSHOT, REORDER_BUFFER_CHANGE_INTERNAL_COMMAND_ID, REORDER_BUFFER_CHANGE_INTERNAL_TUPLECID, REORDER_BUFFER_CHANGE_INTERNAL_SPEC_INSERT, REORDER_BUFFER_CHANGE_INTERNAL_SPEC_CONFIRM, REORDER_BUFFER_CHANGE_TRUNCATE, /* added a new CHANGE TYPE */ REORDER_BUFFER_CHANGE_SEQUENCE,}; Create a new struct that stores the context data for sequence changes within the ReorderBufferChange union src/include/replication/reorderbuffer.htypedef struct ReorderBufferChange{ ... union { ... /* * Context data for Sequence changes */ struct { RelFileNode relnode; ReorderBufferTupleBuf *newtuple; } sequence; } data; ...} ReorderBufferChange; As you can see, for sequence change, we will only have the newtuple that represents the new sequence value. Old tuple is not needed here. 5.2 The Logical Decoder Module (src/backend/replication/logical/decode.c)This module decodes WAL records for the purpose of logical decoding, utilizes snapbuild module to build a fitting catalog snapshot and passes information to the reorderbuffer module for properly decoding the changes. For every WAL log read, the handle will be passed to LogicalDecodingProcessRecord for further decoding. As you can see for the type RM_SEQ_ID, there is no dedicated decoding function invoked. We should create a dedicated decoding function called DecodeSequence and update the switch statement such that the sequence type will use this decoding method. src/backend/replication/logical/decode.cvoid LogicalDecodingProcessRecord(LogicalDecodingContext *ctx, XLogReaderState *record){ ... /* cast so we get a warning when new rmgrs are added */ switch ((RmgrId) XLogRecGetRmid(record)) { ... case RM_HEAP_ID: DecodeHeapOp(ctx, &amp;buf); break; case RM_LOGICALMSG_ID: DecodeLogicalMsgOp(ctx, &amp;buf); break; /* added a new decoder function to handle the sequence type */ case RM_SEQ_ID: DecodeSequence(ctx, &amp;buf); break; ...} Now, we shall define the DecodeSequence function to actually do the decoding. Comments are embedded in the below code block to explain what each line is doing briefly. src/backend/replication/logical/decode.cstatic voidDecodeSequence(LogicalDecodingContext *ctx, XLogRecordBuffer *buf){ ReorderBufferChange *change; RelFileNode target_node; XLogReaderState *r = buf-&gt;record; char *tupledata = NULL; Size tuplelen; Size datalen = 0; uint8 info = XLogRecGetInfo(buf-&gt;record) &amp; ~XLR_INFO_MASK; /* only decode changes flagged with XLOG_SEQ_LOG */ if (info != XLOG_SEQ_LOG) return; /* only interested in our database */ XLogRecGetBlockTag(r, 0, &amp;target_node, NULL, NULL); if (target_node.dbNode != ctx-&gt;slot-&gt;data.database) return; /* output plugin doesn't look for this origin, no need to queue */ if (FilterByOrigin(ctx, XLogRecGetOrigin(r))) return; /* Obtain the change from the decoding context */ change = ReorderBufferGetChange(ctx-&gt;reorder); /* Set the new Sequence change type */ change-&gt;action = REORDER_BUFFER_CHANGE_SEQUENCE; /* Set origin of the change. Used in logical decoding plugin to filter the sources of incoming changes */ change-&gt;origin_id = XLogRecGetOrigin(r); memcpy(&amp;change-&gt;data.sequence.relnode, &amp;target_node, sizeof(RelFileNode)); /* read the entire raw tuple data as a series of char */ tupledata = XLogRecGetData(r); /* read the length of raw tuple data as a series of char */ datalen = XLogRecGetDataLen(r); /* calculate the size of actual tuple by minusing the headers */ tuplelen = datalen - SizeOfHeapHeader - sizeof(xl_seq_rec); /* allocate a new tuple */ change-&gt;data.sequence.newtuple = ReorderBufferGetTupleBuf(ctx-&gt;reorder, tuplelen); /* decode the raw tuple data and save the results as new tuple */ DecodeSeqTuple(tupledata, datalen, change-&gt;data.sequence.newtuple); /* set the catalog change, so snapbuild module will be called to build a snapshot for this sequence change */ ReorderBufferXidSetCatalogChanges(ctx-&gt;reorder, XLogRecGetXid(buf-&gt;record), buf-&gt;origptr); /* queue this change in reorderbuffer module */ ReorderBufferQueueChange(ctx-&gt;reorder, XLogRecGetXid(r), buf-&gt;origptr, change);} The above will call a new function DecodeSeqTuple to actually turn raw tuple data into a ReorderBufferTupleBuf which is needed in reorderbuffer module. This function tries to break down each section of the WAL (written by sequence.c) into a ReorderBufferTupleBuf. src/backend/replication/logical/decode.cstatic voidDecodeSeqTuple(char *data, Size len, ReorderBufferTupleBuf *tuple){ int datalen = len - sizeof(xl_seq_rec) - SizeofHeapTupleHeader; Assert(datalen &gt;= 0); tuple-&gt;tuple.t_len = datalen + SizeofHeapTupleHeader;; ItemPointerSetInvalid(&amp;tuple-&gt;tuple.t_self); tuple-&gt;tuple.t_tableOid = InvalidOid; memcpy(((char *) tuple-&gt;tuple.t_data), data + sizeof(xl_seq_rec), SizeofHeapTupleHeader); memcpy(((char *) tuple-&gt;tuple.t_data) + SizeofHeapTupleHeader, data + sizeof(xl_seq_rec) + SizeofHeapTupleHeader, datalen);} 5.3 The Reorder Buffer Module (src/backend/replication/reorderbuffer.c)reorderbuffer module receives transaction records in the order they are written to the WAL and is primarily responsible for reassembling and passing them to the logical decoding plugin (test_decoding for example) with individual changes. The ReorderBufferCommit is the last function before the change is passed down to the logical decoding plugin by calling the begin, change and commit callback handlers. This is where we will add a new logics to pass a sequence change. src/backend/replication/reorderbuffer.cvoidReorderBufferCommit(ReorderBuffer *rb, TransactionId xid, XLogRecPtr commit_lsn, XLogRecPtr end_lsn, TimestampTz commit_time, RepOriginId origin_id, XLogRecPtr origin_lsn){ ... PG_TRY(); { ... /* call the begin callback */ rb-&gt;begin(rb, txn); ReorderBufferIterTXNInit(rb, txn, &amp;iterstate); while ((change = ReorderBufferIterTXNNext(rb, iterstate)) != NULL) { Relation relation = NULL; Oid reloid; switch (change-&gt;action) { ... case REORDER_BUFFER_CHANGE_SEQUENCE: /* check on snapshot */ Assert(snapshot_now); /* get the relation oid from sequence change context */ reloid = RelidByRelfilenode(change-&gt;data.sequence.relnode.spcNode, change-&gt;data.sequence.relnode.relNode); /* check on relation oid */ if (reloid == InvalidOid) elog(ERROR, \"could not map filenode \\\"%s\\\" to relation OID\", relpathperm(change-&gt;data.tp.relnode, MAIN_FORKNUM)); /* get the relation struct from relation oid */ relation = RelationIdGetRelation(reloid); /* check on relation struct */ if (!RelationIsValid(relation)) elog(ERROR, \"could not open relation with OID %u (for filenode \\\"%s\\\")\", reloid, relpathperm(change-&gt;data.sequence.relnode, MAIN_FORKNUM)); /* call the change callback */ if (RelationIsLogicallyLogged(relation)) rb-&gt;apply_change(rb, txn, relation, change); break; } } ... /* call commit callback */ rb-&gt;commit(rb, txn, commit_lsn); ... } PG_CATCH(); { ... } PG_END_TRY();} Once the decoding plugin receives a change of type REORDER_BUFFER_CHANGE_SEQUENCE, it will need to handle it and look up the proper change context to get the tuple information contrib/test_decoding/test_decoding.cstatic voidpg_decode_change(LogicalDecodingContext *ctx, ReorderBufferTXN *txn, Relation relation, ReorderBufferChange *change){ ... switch (change-&gt;action) { case REORDER_BUFFER_CHANGE_INSERT: ... break; case REORDER_BUFFER_CHANGE_UPDATE: ... break; case REORDER_BUFFER_CHANGE_DELETE: ... break; case REORDER_BUFFER_CHANGE_SEQUENCE: /* print the sequence tuple out */ appendStringInfoString(ctx-&gt;out, \" SEQUENCE:\"); if (change-&gt;data.sequence.newtuple == NULL) appendStringInfoString(ctx-&gt;out, \" (no-tuple-data)\"); else tuple_to_stringinfo(ctx-&gt;out, tupdesc, &amp;change-&gt;data.sequence.newtuple-&gt;tuple, false); break; default: Assert(false); } ...} 6. ConclusionWe have discussed about the current implementation of logical decoding and some potential reasons why sequence is not supported in PostgreSQL logical replication. We have also gone through some important source files that could be updated to allow sequence replication. In the above approach, whenever the sequence module emits a WAL update, (which is a future value 32 increments later as discussed previously), the logical decoding plugin will receive this same future value, which is in fact different from the actual sequence value currently. This can be justified if we think about the purpose of sequence replication for a second, which is useful in fail over cases. With this future sequence value, the subsequent data insersions will be able to continue starting from this future sequence value.","link":"/2020/04/22/Can-Sequence-Relation-be-Logically-Replicated/"},{"title":"Understanding Security Features in PostgreSQL - Part 1","text":"1. IntroductionPostgreSQL is packed with several security features for a database administrator to utilize according to his or her organizational security needs. The word Security is a very broad concept and could refer to completely different procedures and methodology to achieve in different PostgreSQL components. This blog is divided into part 1, 2 and 3 and I will explain the word Security with regards to PostgreSQL version 12.1 and how it is practiced in different areas within the system. In Part 1 of the blog, I will be discussing the basic security features that exist in PostgreSQL with emphasis on Host-based authentication methods as well as user-based access control with the concept of roles. If done right, we could have a much more robust database server and potentially reduce the attack surface on the server, protecting it from attacks like SQL injections. I will also briefly discuss a few of the advanced authentication methods such as LDAP and PAM authentication. There are many more advanced authentication methods supported and we will be producing more articles in the near future to cover more of these methods. In Part 2 of the blog, I will be discussing TLS in greater detail, which I believe is crucial for a database administrator to understand first before enabling TLS in the PostgreSQL server. TLS is a fairly large and one of the least understood protocol today, which contains a lot of security components and methodology related to cryptography that could be quite confusing. In Part 3 of the blog, I will be discussing how to apply TLS configurations to both PostgreSQL server and client following the TLS principles that have been discussed in Part 2. I will also briefly discuss Transparent Data Encryption (TDE) that the PG community is currently working on that introduces another layer of secured database environment. Below is the overview of the security topics that will be covered in all parts of the blog: Part 1: PostgreSQL Server Listen Address Host-Based Authentication Authentication with LDAP Server Authentication with PAM Role-Based Access Control Assign Table and Column Level Privileges to Users Assign User Level Privileges as Roles Assign and Column Level Privileges via Roles Role Inheritance Part 2: Security Concepts around TLS Symmetrical Encryption Asymmetrical Encryption (a.k.a Public Key Cryptography) Block Cipher Mode of Operation (a.k.a Stream Cipher) Key Exchange Algorithm TLS Certificate and Chain of Trust Data Integrity Check / Data Authentication TLS Cipher Suite and TLS handshake TLS versions Part 3: Preparing TLS Certificates Enabling Transport Layer Security (TLS) to PostgreSQL Server Enabling Transport Layer Security (TLS) to PostgreSQL Client TLS Connect Examples Transparent Data Encryption (TDE) Security Vulnerability 2. PostgreSQL Server Listen AddressPostgreSQL server is a TCP server that by default listens on localhost at port 5432. The server listen address may seem very trivial at first in terms of security but it is actually very important because understanding how the PostgreSQL is serving the incoming connections is fundamental to building a more secured network environment. Connection settings are located in postgresql.conf The listen_addresses parameter tells PostgreSQL which addresses to listen on. This value should match the IP address of the network interface cards in the host machine. ifconfig on Unix-based systems (or ipconfig for Windows) is a handy command that lists all the network interfaces and their IP addresses. listen_address supports the less secured * configuration, which will listen to all the network interfaces available #listen_addresses = 'localhost' # what IP address(es) to listen on; # comma-separated list of addresses; # defaults to 'localhost'; use '*' for all # (change requires restart)#port = 5432 # (change requires restart) Another important connection configuration is the maximum connections allowed. By default PostgreSQL allows 100 simultaneous connections to be active at a time with 3 connections reserved for super user. That is 97 connections for regular database users. These numbers should be configured accordingly depending on the usage case of the database server and we definitely don’t want too many unintentional connections to access the database max_connections = 100 # (change requires restart)superuser_reserved_connections = 3 # (change requires restart) 3. Host-Based AuthenticationHost-based authentication refers to the process of verifying the identity of a user connection based on the IP addresses of the connecting host. PostgreSQL supports host-based authentication by adding and removing desired entries in the pg_hba.conf file. This file works in a similar way as defining firewall rules. The official documentation on pg_hba.conf can be found here: https://www.postgresql.org/docs/current/auth-pg-hba-conf.html A simple example below defines the following rules: Allows connections from subnet 192.168.3.0/24 to access the database named “software_team” Allows connections from subnet 192.168.4.0/24 to access the database named “marketing_team” Allows connections from subnet 192.168.5.0/24 to access the database named “sales_team” Allows connections from subnet 192.168.6.0/24 to access the database named “management” The admin user has permission to access all the database given that the admin is connecting from localhost (both IPv4 and IPv6) or from a UNIX domain socket. Allows all user connections coming from subnet 192.168.7.0/24 to access the database named “production_team” and is able to do replication connection. Please note that the word “replication” is a special term reserved to allow replication connections rather than database name. Allows user from a unsecured network “172.16.30.0/24” to access the “sales_team” database only if the connection uses SSL (a.k.a TLS) Rejects all connections from 172.16.50.5 # TYPE DATABASE USER ADDRESS METHODlocal all admin trustlocal all admin 127.0.0.1/32 trustlocal all admin ::1/128 trusthost software_team all 192.168.3.0/24 trusthost marketing_team all 192.168.4.0/24 trusthost sales_team all 192.168.5.0/24 trusthost management all 192.168.6.0/24 trusthost production_team all 192.168.7.0/24 trusthost replication all 192.168.7.0/24 trusthost all all 172.16.50.5/32 rejecthostssl sales_team all 172.16.30.0/24 trust The simple example above uses 2 basic methods to control the access, trust and reject. This will suffice in a small database server environment. However, depending on the infrastructure, the application’s nature and data security, stronger authentication methods are encouraged, such as LDAP, GSSPI with Kerberos, SSPI, RADIUS SCRAM-SHA-256…etc. Generally speaking, most of these stronger authentication methods require PostgreSQL to communicate with foreign authentication servers to complete the authentication process in a more secured way and provide automatic “single-sign-on” authentications through means of shared secrets, token exchange, or user name mappings. I will briefly introduces LDAP and PAM authentication in this blog. 4. Authentication with LDAP ServerLDAP stands for Light-weight Directory Access Protocol, which is commonly deployed as centralized authentication system for medium to large organizations. This authentication server provides user credential authentication and stores related user details like distinguished name, domain names and business units..etc. Every entry in an LDAP directory server has a distinguished name (DN). It is the name that uniquely identifies an entry in the directory and made up of attribute=value pairs. As a LDAP client on the PostgreSQL side, attribute=value pairs are required to be supplied in pg_hba.conf file separated by commas. For example: # TYPE DATABASE USER ADDRESS METHODhost production_team production_user 0.0.0.0/0 ldap dapserver=192.168.7.100 ldapport=389 ldapprefix=\"cn=\" ldapsuffix=\", dc=organization, dc=com\" Please note that LDAP by default is not encrypted and communicating user credential unencrypted is never a good idea. LDAP over TLS is supported by appending ldaptls=1 to the ldap attributes in pg_hba.conf file. Please also note that ldaptls=1 only provides secured connection between PostgreSQL server and LDAP server. The connection between PostgreSQL server and client is not using TLS by default, so it needs to be enabled as well. TLS is discussed in details in part 2 of this blog. 5. Authentication with PAMPAM stands for Pluggable Authentication Module and it operates similarly to password. The default service name is postgresql. First we need to create a linux user (Example based on Ubuntu 18.04). $ useradd production_user$ passwd production_userChanging password for user production_user.New UNIX password:Retype new UNIX password:passwd: all authentication tokens updated successfully. Create /etc/pam.d/postgresql with the content: #%PAM-1.0auth include system-authccount include system-authpassword include system-authsession include system-auth Create production_user in PostgreSQL server $ CREATE USER production_user; Then finally update the pg_hba.conf with pam authentication method. # TYPE DATABASE USER ADDRESS METHODhost production_team production_user 0.0.0.0/0 pam 6. Authentication with CertificateAuthentication with certificate can be applied to all the authentication methods by appending clientcert=1 in method parameters. This is only useful with hostssl type records in pg_hba.conf file and requires that the PostgreSQL server has TLS enabled in postgresql.conf with path to CA certificate specified. We will discuss TLS and certificates in part 2 of the blog in more details. With clientcert=1 in place, the server will require that the client to send its TLS certificate for verification. The connection will abort if client fails to provide a certificate. The server will verify the common name (CN) in the certificate against the server’s hostname. Both should match. In addition, certificate validity dates will be verified and most importantly, the server will try to determine the chain of trust from the client certificate against the CA certificate configured in the server to determine if the client can be trusted. # TYPE DATABASE USER ADDRESS METHODhostssl production_team production_user 0.0.0.0/0 pam clientcert=1 7. Role-Based Access ControlRole-based access control refers to the process of verifying database access permissions based on the pre-defined roles and user privileges. PostgreSQL supports role-based access in several levels, such as table, function, procedural language and user levels. I will explain the concept in table and user level access control that follow the general guidelines below: A user with super user privilege can do any activities in the database A user who creates a table owns the table and can set its permission A user needs to belong to a proper role to perform administrative operations such as create another user or role Other users need proper permissions to view or operate on a table created by another user. When a PostgreSQL database cluster has been initialized, a super user will be created by default that equals to the system user that initializes the cluster. This super user is the starting point to define other role and other users and privileges to ensure proper database access. 8. Assign Table and Column Level Privileges to UsersThe GRANT clause supported in PostgreSQL is used to configure the access privileges (official documentation here: https://www.postgresql.org/docs/current/sql-grant.html). GRANT is a very universal clause that can be used to add access privileges to tables, databases, roles, table spaces…etc. The opposite of GRANT is REVOKE, which removes privileges (official documentation here: https://www.postgresql.org/docs/current/sql-revoke.html). In this blog, I will use GRANT on table and role level. When a table is created, it is assigned an owner. The owner is normally the user that executed the creation statement. The initial state is that only the owner (or a superuser) can do anything with the table. To allow other users to use it, privileges must be granted. There are many types of privileges that can be granted to a table or a table column. The image below is taken directly from the official PostgreSQL documentation that lists all the available privileges and their applicable objects. Consider a simple SQL command example below that assigns table and column access privileges to other users $ GRANT SELECT ON table1 TO userA;$ GRANT SELECT ON table2 TO userA;$ GRANT SELECT ON table3 TO userA;$ GRANT UPDATE ON table1 TO userB;$ GRANT INSERT ON table2 TO userB;$ GRANT INSERT ON table3 TO userB;$ GRANT UPDATE ON table2 TO userC;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO userC; The above SQL commands can be illustrated as: 9. Assign User Level Privileges as RolesA ROLE is an entity that can own database objects and have database privileges; a role can be considered a “user”, a “group”, or both depending on how it is used.(official documentation here: https://www.postgresql.org/docs/current/sql-createrole.html). Similar to a table, a created role can be altered with the ALTER clause or deleted with the DROP clause. Please note that when a role is created initially, the permission to LOGIN is not allowed by default and has to be manually set such that the users belonging to this role can log in to the database server. The same can be done with CREATE USER clause, which allows LOGIN by default. So the following 2 commands are essentially the same $ CREATE ROLE username LOGIN;$ CREATE USER username; The following image is taken directly from the official PostgreSQL documentation that lists all the privilege keywords that can be associated to a role. Consider the following simple example that creates 3 users and 4 different roles having different user level access privileges. /* Create 3 users */$ CREATE USER userA;$ CREATE USER userB;$ CREATE USER userC;/* Create 4 roles */$ CREATE ROLE role1 LOGIN CREATEDB CREATEROLE;$ CREATE ROLE role2 WITH PASSWORD '12345678' LOGIN REPLICATION;$ CREATE ROLE role3 LOGIN CREATEDB INHERIT;$ CREATE ROLE role4 LOGIN CONNECTION LIMIT 5 ;$ GRANT role1 TO userA;$ GRANT role1 TO userB;$ GRANT role3 TO userC;$ GRANT role4 TO userC;postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+----------------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {role1} | userB | | {role1} | userC | | {role3, role4} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | $ GRANT SELECT ON table1 TO role1;$ GRANT SELECT ON table2 TO role1;$ GRANT SELECT ON table3 TO role1;$ GRANT UPDATE ON table1 TO role1;$ GRANT INSERT ON table2 TO role1;$ GRANT INSERT ON table3 TO role1;$ GRANT UPDATE ON table2 TO role3;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO role3; Use the \\du+ meta command to see all the roles that have been created with summary of the attributes associated with each role. To see the full list of attributes per role, use the SQL command SELECT * FROM pg_roles;. postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+-----------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {} | userB | | {} | userC | | {} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | 10. Assign Table Level Privileges via RolesSection 3.1 illustrates privilege assignments directly to each individual users, which is desirable in smaller database servers. Imagine a larger database server where there could potentially be hundreds of users exist in the entire database cluster. Managing the table level privileges would get quite complicated and tedious. Luckily, PostgreSQL supports assigning users to roles for better privilege management Following the examples in section 3.2, we can use the GRANT command again to assign users to roles. Note that the Member of will display the relationship between users and roles after we have related them with GRANT clause. $ GRANT role1 TO userA;$ GRANT role1 TO userB;$ GRANT role3 TO userC;$ GRANT role4 TO userC;postgres=# \\du+ List of roles Role name | Attributes | Member of | Description -----------+------------------------------------------------------------+----------------+------------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} | userA | | {role1} | userB | | {role1} | userC | | {role3, role4} | role1 | Replication, Create DB, Create role | {} | role2 | Replication | {} | role3 | Create DB | {} | role4 | 5 connections | {} | Following the examples in section 3.1, we can use the GRANT command again to assign table level privileges to roles that we have created instead of to users directly $ GRANT SELECT ON table1 TO role1;$ GRANT SELECT ON table2 TO role1;$ GRANT SELECT ON table3 TO role1;$ GRANT UPDATE ON table1 TO role1;$ GRANT INSERT ON table2 TO role1;$ GRANT INSERT ON table3 TO role1;$ GRANT UPDATE ON table2 TO role3;$ GRANT SELECT(column1), UPDATE(column3) ON table3 TO role3; The above SQL commands can be illustrated as: 11. Role InheritanceINHERIT and NOINHERIT are one of the special attributes that can be assigned to a role. When a role (say role 1) contains INHERIT attribute and is a member of another role (say role 2). All the attributes existing in both role 1 and role 2 will be applied to the user. Consider a simple example below: $ CREATE ROLE role1 LOGIN CREATEDB REPLICATION;$ CREATE ROLE role2 LOGIN CREATEROLE INHERIT;$ GRANT role1 TO role2;$ GRANT role2 TO userA; Which can be visualized as: In this case, role2 is created with INHERIT, userA will be assigned the privileges defined in both role1 and role2. 12. SummaryIn this blog, we went over several mechanisms in postgreSQL that allows a database administrator to configure the authentication of incoming user connections and the privilege configuration in table, column and user level via the concept of roles. PostgreSQL provides pg_hba.conf file that configures simple authentication and supports stronger authentication methods against remote authentication services such as GSSAPI, kerberos, RADIUS, PAM and LDAP..etc. So far we have only talked about authentication and authorization (AA) in PostgreSQL terms, in part 2 of this blog, I will explain the general concept of data encryption, how to secure data communication between server and client with TLS and how to achieve encryption on storage devices.","link":"/2020/01/10/Understanding-Security-Features-in-PostgreSQL-Part1/"},{"title":"Approaches to Achieve in-memory table storage with PostgreSQL pluggable API","text":"1. IntroductionRecently, I have had an opportunity to perform some in-depth feasibility study in-memory table using PostgreSQL’s pluggable storage API. The pluggable storage API was introduced Since PostgreSQL v12 and it allowed custom table storage Access Methods (AM for short) to be developed. Some famous examples include zheap from EDB, which aims to store a much more simplified tuple structure to achieve vacuum free storage and zedstore from Greenplum to utilize columnar storage. But they do not utilize in-memory storage and still interacts with existing buffer manager to persist data to disk. Yes, it would be nice to have in-memory tables if they would perform faster. It would definitely be a very interesting challenge if we could achieve an in-memory table storage engine and design it in a way that will give serious performance advantages using the existing pluggable storage API architecture. Developing a custom storage AM is no easy task and it requires a very deep understanding on how the current PostgreSQL storage engine works before we are able to improve it. In addition to redefining the tuple structure and custom algorithms to store and retrieve, it is also possible to define our own in-memory based storage module to handle the insertion and retrieval of tuple instead of utilizing the same buffer manager routines like what heap, zheap and zedstore access methods are using. In this blog, I will briefly talk about the capability of pluggable storage API and share some progress on our in-memory table analysis 2. The Pluggable Storage Access Method APIPostgreSQL already has a pluggable index access method API for defining different index methods such as btree, gin and gist…etc where btree is the default index method today. The desired method can be selected when issuing the CREATE INDEX command like: CREATE INDEX gin_idx ON movies USING gin (year);CREATE INDEX gist_idx ON movies USING gist (year); Before PostgreSQL v12, there was not a pluggable access method for defining table storage and heap was the only access method available. After the introduction of pluggable storage API in v12, it is now possible to create custom storage access methods other than the default heap using similar syntax when creating a table like: CREATE ACCESS METHOD myheap TYPE TABLE HANDLER myheap_handler;CREATE TABLE mytable (a int, b char(10)) using myheap; There are a total of 38 callback functions provided by the pluggable API that requires to be implemented to develop a new table access method. They are defined in TableAmRoutine structure in tableam.h. It is quite tedious to explain all 38 callback functions here but in short, they primarily have to deal with: slot type table sequential scan parallel table scan index scan tuple visibility check tuple modification, update, insert, etc DDL related function, setting relfilenode, vacuum and …etc TOAST information planner time estimation bitmap and sample scan functionality As you can see, the functionalities to be provided by these callback functions are very critical as they have direct impact on how efficient or inefficient it is to retrieve and store data 3. Using Pluggable Storage API to Achieve in-memory tableThe pluggable API does not directly deal with data storage to disk and it has to rely on interacting with buffer manager, which in turn, puts the tuple on disk via storage manager. This is a good thing, because with this architecture, we could potentially create a memory cache module and have the pluggable API’s tuple modification and scanning callbacks to interact with this new memory cache instead for faster tuple insertion and retrieval. It is possible to achieve a very simple in-memory table with this approach, but there are some interesting considerations here. can existing buffer manager still play a role to perform data persistence on disk? how large can memory cache size be? how and when should memory cache persist data to disk? can the access method work if it does not utilize buffer manager routines at all, ie. without CTID? is existing buffer manager already acting like a in-memory data storage if its buffer pool is allocated large enough and we don’t mark a page as dirty, so in theory the data always stays in the buffer pool? 3.1 Buffer ManagerLet’s talk about buffer manager. PostgreSQL buffer manager is a very well-written module that works as an intermediate data page buffer before they are flushed to the disk. Existing heap access method and others like zheap and zedstore impementations make extensive use of buffer manager to achieve data storage and persistence on disk. In our in-memory table approach, we actually would like to skip the buffer manager routines and replace it with our own memory cache or similar. So in terms of architecture, it would look something like this where the green highlighted block is what we would like to achieve with in-memory table. In this approach, the design of the memory cache component would be a challenge as it essentially replaces the functionality of existing buffer manager and utilize in-memory data storage instead. For this reason, the CTID value, which points directly to a specific data page managed by buffer manager, may not be valid anymore if buffer manager is no longer to be used. The concern would be if it is possible to implement a new access method without buffer manager and CTID? Our investigation shows yes, it is possible for the access method to not use buffer manager and CTID at all, but it would require the pluggable API to manually fill the TupleTableSlot when it has retrieved a tuple from memory cache. 3.2 Buffer Manager as In-memory storage?Another question that rises is that If the existing buffer manager has a large enough buffer pool and we never mark a page as dirty, the buffer manager theoretically will not flush a page to disk and in this case, will we have something very similar to the memory cache module? Theoretically yes, but unfortunately it is not how current buffer manager is designed to do. Buffer manager maintains a ring buffer with limited size and data is flushed to disk when new data enters or when they are deemed as “dirty”. It uses a 3-layer buffer structure to manage the location of each data page on disk and provides comprehensive APIs to the PostgreSQL backend processes to interact, insert and retrieve a tuple. If a tuple resides on the disk instead of in the buffer, it has to retrieve it from the disk. In terms of time ticks, this is how PG retrieves a tuple from disk: As you can see, it takes T1+T2+T3+T4+T5 to retrieve a tuple from disk. With the approach of in-memory table, we want to cut off the time takes to retrieve tuple from disk, like: which takes T1’ + T2’ + T3’ to retrieve a tuple from the memory cache. This is where in-memory table implementation can help with performance increase. 3.3 Ideal Size for this New Memory Cache?The official PostgreSQL documentation recommends allocation of 25% of all the available memory, but no more than 40%. The rest of the available memory should be reserved for kernel and data caching purposes. From this 25% ~ 40% of reserved memory for PG, we need to minus the shared memory allocations from other backend processes. The remainder would be the maximum size the memory cache can allocate and depending on the environment it may or may not be enough. See image below. 4. Our Approach?Since our focus is primarily on the memory cache, which is an alternative to existing buffer manager, we would prefer to use the existing Heap tuple as data strucuture to begin with. This way, we can use the existing TOAST, vacuum, WAL, scanning logics and we will primarily focus on replacing its buffer manager interactions with memory-cache equivalent function calls. All this will be done as a separate extension using pluggable API, so it is still possible to use the default Heap access methods on some tables and use in-memory access methods for some other tables.","link":"/2020/07/23/Approaches-to-Achieve-in-memory-table-storage-with-PostgreSQL-pluggable-API/"},{"title":"An Overview of PostgreSQL Backend Architecture","text":"1. IntroductionPostgreSQL backend is a collection of processes forked from the main process called Postmaster. Each forked process has different roles and responsibilities in the backend. This article describes the responsibility of core backend processes that power the PostgreSQL system as we know it today. The overall PostgreSQL backend architecture can be illustrated by the image below: Postmaster is the first process to be started who has control of all the backend processes and is responsible for accepting and closing the database connections. At start up, the postmaster forks several backend processes that are intended to process different aspects of backend tasks, which we will be covering in this blog. When a user initiates a connection to the PostgreSQL database, the client process will send an authentication message to the Postmaster main process. The Postmaster main process authenticates the user according to the authentication methods configured and will fork a new session to provide service to this user only if the user passes authentication. 2. BgWriter (Background Writer) ProcessThe BgWriter process is a process that writes dirty pages in shared memory to disk. It has two functions: one is to periodically flush out the dirty data from the memory buffer to the disk to reduce the blocking during the query; the other is that the PG needs to write out all the dirty pages to the disk during the regular checkpoint. By BgWriter Writing out some dirty pages in advance, it can reduce the IO operations to be performed when setting checkpoints (A type of database recovery technology), so that the system’s IO load tends to be stable. BgWriter is a process added after PostgreSQL v8.0 and it has a dedicated section in postgresql.conf to configure its behavior. # - Background Writer -#bgwriter_delay = 200ms # 10-10000ms between rounds#bgwriter_lru_maxpages = 100 # max buffers written/round, 0 disables#bgwriter_lru_multiplier = 2.0 # 0-10.0 multiplier on buffers scanned/round#bgwriter_flush_after = 512kB # measured in pages, 0 disables bgwriter_delay:The time interval between two consecutive flush data in the backgroud writer process. The default value is 200, and the unit is milliseconds. bgwriter_lru_maxpages:The maximum amount of data written by the backgroud writer process at a time. The default value is 100, in units of buffers. If the amount of dirty data is less than this value, the write operation is all completed by the backgroud writer process; conversely, if it is greater than this value, the greater part will be completed by the server process process. When the value is set to 0, it means that the backgroud writer writing process is disabled, and it is completely completed by the server process; when it is set to -1, it means that all dirty data is done by the backgroud writer. (Checkpoint operations are not included here) bgwriter_lru_multiplier:This parameter indicates the number of data blocks written to the disk each time, of course, the value must be less than bgwriter_lru_maxpages. If the setting is too small, the amount of dirty data that needs to be written is greater than the amount of data written each time, so the remaining work that needs to be written to the disk needs to be completed by the server process process, which will reduce performance; if the value configuration is too large, the amount of dirty data written More than the number of buffers required at the time, which is convenient for applying for buffer work again later, and IO waste may occur at the same time. The default value of this parameter is 2.0. bgwriter_flush_after:BgWriter is triggered when the data page size reaches bgwriter_flush_after, the default is 512KB. 3. WalWriter ProcessThe core idea of ​​Write Ahead Log (also called Xlog) is that the modification of data files must only occur after these modifications have been recorded in the log, that is, the log is written first before the data is written . Using this mechanism can avoid frequent data writing to the disk, and can reduce disk I/O. The database can use these WAL logs to recover the database after a database restart. WalWriter Process is a backend process responsible for ensuring the WAL files are properly written to the disk and its behavior is configurable with the following parameters set in postgresql.conf #------------------------------------------------------------------------------# WRITE-AHEAD LOG#------------------------------------------------------------------------------# - Settings -wal_level = logical # minimal, replica, or logical # (change requires restart)#fsync = on # flush data to disk for crash safety # (turning this off can cause # unrecoverable data corruption)#synchronous_commit = on # synchronization level; # off, local, remote_write, remote_apply, or on#wal_sync_method = fsync # the default is the first option # supported by the operating system: # open_datasync # fdatasync (default on Linux) # fsync # fsync_writethrough # open_sync#full_page_writes = on # recover from partial page writes#wal_compression = off # enable compression of full-page writes#wal_log_hints = off # also do full page writes of non-critical updates # (change requires restart)#wal_init_zero = on # zero-fill new WAL files#wal_recycle = on # recycle WAL files#wal_buffers = -1 # min 32kB, -1 sets based on shared_buffers # (change requires restart)#wal_writer_delay = 200ms # 1-10000 milliseconds#wal_writer_flush_after = 1MB # measured in pages, 0 disables#commit_delay = 0 # range 0-100000, in microseconds#commit_siblings = 5 # range 1-1000 wal_level:Controls the level of wal storage. wal_level determines how much information is written to the WAL. The default value is replica, which adds WAL archive information and includes information required by read-only servers (streaming replicattion). It can also be set to minimal, which only writes the information needed to recover from a crash or immediate shutdown. Setting to Logical allows WAL streaming to be done in logical decoding scenarios. fsync:This parameter directly controls whether the log is written to disk first. The default value is ON (write first), which means that the system shall ensure the change is indeed flushed to disk, by issuing the fsync command set by wal_sync_method. While turning off fsync is often a performance benefit, this can result in unrecoverable data corruption in the event of a power failure or system crash. Thus it is only advisable to turn off fsync if you can easily recreate your entire database from external data. synchronous_commit:This parameter configures whether the system will wait for WAL to complete before returning status information to the user transaction. The default value is ON, indicating that it must wait for WAL to complete before returning transaction status information; configuring OFF can feed back the transaction status faster. wal_sync_method:This parameter controls the fsync method of WAL writing to disk. The default value is fsync. The available values ​​include open_datasync, fdatasync, fsync_writethrough, fsync, and open_sync. open_datasync and open_sync respectively. full_page_writes:indicates whether to write the entire page to the WAL. wal_buffers:The amount of memory space used to store WAL data. The system default value is 64K. This parameter is also affected by the two parameters wal_writer_delay and commit_delay. wal_writer_delay:The write interval of the WalWriter process. The default value is 200 milliseconds. If the time is too long, it may cause insufficient memory in the WAL buffer; if the time is too short, it will cause the WAL to continuously write, increasing the disk I/O burden. wal_writer_flush_after:When dirty data exceeds this threshold, it will be flushed to disk. commit_delay:indicates the time that the submitted data is stored in the WAL buffer. The default value is 0 milliseconds, which means no delay; when it is set to a non-zero value, the transaction will not be written to the WAL immediately after the commit is executed, but it is still stored in the WAL In the buffer, waiting for the WalWriter process to write to the disk periodically. commit_siblings:When a transaction issues a commit request, if the number of transactions in the database is greater than the value of commit_siblings, the transaction will wait for a period of time (commit_delay value); otherwise, the transaction is directly written to WAL. The system default value is 5, and this parameter also determines the validity of commit_delay. 4. PgArch ProcessSimilar to the ARCH archiving process in the Oracle database, the difference is that ARCH performs archiving on redo log while PgArch performs archiving on WAL logs. This is needed because the WAL log will be recycled. In other words, the WAL log in the past will be overwritten by the newly generated ones. The PgArch process is responsible for backing up the WAL log before they are overwritten. Starting from version 8.x, these WAL logs can then be used for PITR (Point-In-Time-Recovery), which restores the database state to a certain state at certain period of time. PgArch also has a dedicated section in postgresql.conf to configure its behavior. # - Archiving -#archive_mode = off # enables archiving; off, on, or always # (change requires restart)#archive_command = '' # command to use to archive a logfile segment # placeholders: %p = path of file to archive # %f = file name only # e.g. 'test ! -f /mnt/server/archivedir/%f &amp;&amp; cp %p /mnt/server/archivedir/%f'#archive_timeout = 0 # force a logfile segment switch after this # number of seconds; 0 disables archive_mode: Indicates whether to perform the archive operation; it can be set to (off), (on) or (always), the default value is off. archive_command: The command set by the administrator for archiving WAL logs. In the command for archiving, the predefined variable “%p” is used to refer to the WAL full path file name that needs to be archived while “%f” indicates the file name without a path (the paths here are relative to the current working directory). When each WAL segment file is archived, the command specified by archive_command will be executed. If the archive command returns 0, PostgreSQL considers the file successfully archived, and then deletes or recycles the WAL segment file. If a non-zero value is returned, PostgreSQL will consider the file was not successfully archived, and will periodically retry until it succeeds. archive_timeout: Indicates the archiving period. When the time set by this parameter is exceeded, the WAL segment is forcibly switched. The default value is 0 (function disabled). 5. AutoVacuum ProcessIn PostgreSQL database, after performing UPDATE or DELETE operations on the data, the database will not immediately delete the old version of the data. Instead, the data will be marked as deleted by PostgreSQL’s multi-version mechanism. If these old versions of data are being accessed by other transactions, it is necessary to retain them temporarily. After the transaction is submitted, the old versions of the data are no longer required (dead tuples) and therefore the database needs to clean them up to make room. This task is performed by the AutoVacuum process and the parameters related to the AutoVacuum process are also in the postgresql.conf. #------------------------------------------------------------------------------# AUTOVACUUM#------------------------------------------------------------------------------#autovacuum = on # Enable autovacuum subprocess? 'on' # requires track_counts to also be on.#log_autovacuum_min_duration = -1 # -1 disables, 0 logs all actions and # their durations, &gt; 0 logs only # actions running at least this number # of milliseconds.#autovacuum_max_workers = 3 # max number of autovacuum subprocesses # (change requires restart)#autovacuum_naptime = 1min # time between autovacuum runs#autovacuum_vacuum_threshold = 50 # min number of row updates before # vacuum#autovacuum_analyze_threshold = 50 # min number of row updates before # analyze#autovacuum_vacuum_scale_factor = 0.2 # fraction of table size before vacuum#autovacuum_analyze_scale_factor = 0.1 # fraction of table size before analyze#autovacuum_freeze_max_age = 200000000 # maximum XID age before forced vacuum # (change requires restart)#autovacuum_multixact_freeze_max_age = 400000000 # maximum multixact age # before forced vacuum # (change requires restart)#autovacuum_vacuum_cost_delay = 2ms # default vacuum cost delay for # autovacuum, in milliseconds; # -1 means use vacuum_cost_delay#autovacuum_vacuum_cost_limit = -1 # default vacuum cost limit for # autovacuum, -1 means use # vacuum_cost_limit autovacuum:whether to start the auto vacuum process automatically, the default value is on. log_autovacuum_min_duration:This parameter records the execution time of autovacuum. When the execution time of autovaccum exceeds the setting of the log_autovacuum_min_duration parameter, this incident will be recorded in the log. The default is “-1”, which means no recording. autovacuum_max_workers:Set the maximum number of autovacuum subprocesses autovacuum_naptime:Set the interval time between two autovacuum processes. autovacuum_vacuum_threshold and autovacuum_analyze_threshold:Set the threshold values of the number of updated tuples on the table, if number of tuple updates exceed these values, vacuum and analysis need to be performed respectively. autovacuum_vacuum_scale_factor and autovacuum_analyze_scale_factor:Set the scaling factor for table size. autovacuum_freeze_max_age:Set the upper limit of transaction ID that needs to be forced to clean up the database. autovacuum_vacuum_cost_delay:When the autovacuum process is about to be executed, the vacuum execution cost is evaluated. If the value set by autovacuum_vacuum_cost_limit is exceeded, there will be a delay set by the autovacuum_vacuum_cost_delay parameter. If the value is -1, it means to use vacuum_cost_delay value instead. the default value is 20 ms. autovacuum_vacuum_cost_limi: This value is the evaluation threshold of the autovacuum process. The default is -1, which means to use the “vacuum_cost_limit” value. If the cost evaluated during the execution of the autovacuum process exceeds autovacuum_vacuum_cost_limit, the autovacuum process will sleep. 6. Stat CollectorStat collector is a statistical information collector of the PostgreSQL database. It collects statistical information during the operation of the database, such as the number of table additions, deletions, or updates, the number of data blocks, changes in indexes…etc. Collecting statistical information is mainly for the query optimizer to make correct judgment and choose the best execution plan. The parameters related to the Stat collector in the postgresql.conf file are as follows: #------------------------------------------------------------------------------# STATISTICS#------------------------------------------------------------------------------# - Query and Index Statistics Collector -#track_activities = on#track_counts = on#track_io_timing = off#track_functions = none # none, pl, all#track_activity_query_size = 1024 # (change requires restart)#stats_temp_directory = 'pg_stat_tmp' track_activities:Indicates whether to enable the statistical information collection function for the command currently executed in the session. This parameter is only visible to the super user and session owner. The default value is on. track_counts:indicates whether to enable the statistical information collection function for database activities. Since the database to be cleaned is selected in the AutoVacuum automatic cleaning process, the database statistical information is required, so the default value of this parameter is on. track_io_timing:Timely call data block I/O, the default is off, because set to the on state will repeatedly call the database time, which adds a lot of overhead to the database. Only super user can set track_functions:indicates whether to enable the number of function calls and time-consuming statistics. track_activity_query_size:Set the number of bytes used to track the currently executed command of each active session. The default value is 1024, which can only be set after the database is started. stats_temp_directory:Temporary storage path for statistical information. The path can be a relative path or an absolute path. The default parameter is pg_stat_tmp. This parameter can only be modified in the postgresql.conf file or on the server command line. 7. Checkpointer ProcessThe checkpointer is a sequence of transaction points set by the system. Setting the checkpoint ensures that the WAL log information before the checkpoint is flushed to the disk. In the event of a crash, the crash recovery procedure looks at the latest checkpoint record to determine the point in the log (known as the redo record) from which it should start the REDO operation. The relevant parameters in the postgresql.conf file are: # - Checkpoints -#checkpoint_timeout = 5min # range 30s-1dmax_wal_size = 1GBmin_wal_size = 80MB#checkpoint_completion_target = 0.5 # checkpoint target duration, 0.0 - 1.0#checkpoint_flush_after = 256kB # measured in pages, 0 disables#checkpoint_warning = 30s # 0 disables checkpoint_timeout:this parameter configures the period of performing a checkpoint. The default is 5 minutes. This means a checkpoint will occur every 5 minutes or when max_wal_size is about to be exceeded. Default is 1GB. max_wal_size:this parameter sets the max WAL size before a checkpoint will happen min_wal_size:this parameter sets a minimum on the amout of WAL files recycled for future usage checkpoint_completion_target:To avoid flooding the I/O system with a burst of page writes, writing dirty buffers during a checkpoint is spread over a period of time. That period is controlled by checkpoint_completion_target, which is given as a fraction of the checkpoint interval. checkpoint_flush_after:This parameter allows to force the OS that pages written by the checkpoint should be flushed to disk after a configurable number of bytes. Otherwise, these pages may be kept in the OS’s page cache. Default value is 256kB checkpoint_warning:Checkpoints are faily expensive operation. This parameter configures a threshold between each checkpoint and if checkpoints happen too close together than checkpoint_warningperiod, the system will output a warning in server log to recommend user to increase max_wal_size 8. Shared Memory and Local MemoryWhen PostgreSQL server starts, a shared memory will be allocated to be used as a buffer of data blocks to improve the reading and writing capabilities.The WAL log buffer and CLOG buffer also exist in shared memory. Some global information such as process information, lock information, global statistics, etc are all stored in shared memory In addition to the shared memory, the background services will also allocate some local memory to temporarily store data that does not require global storage. These memory buffers mainly include the following categories: Temporary buffer: local buffer used to access temporary tables work_mem: Memory buffering used by memory sort operations and hash tables before using temporary disk files. maintenance_work_mem: Memory buffer used in maintenance operations (such as vacuum, create index, and alter table add foreign key, etc.). 9. SummaryThis blog provides an overview of the core backend processes that drive the PostgreSQL as we see it today and they serve as foundations to database performance tuning. There are many parameters that can be changed to influence the behavior of these backend processes to make the database perform better, safer and faster. This will be a topic for the future.","link":"/2020/06/15/An-Overview-of-PostgreSQL-Backend-Architecture/"},{"title":"Logical Replication Between PostgreSQL and MongoDB","text":"1. IntroductionPostgreSQL and MongoDB are two popular open source relational (SQL) and non-relational (NoSQL) databases available today. Both are maintained by groups of very experienced development teams globally and are widely used in many popular industries for adminitration and analytical purposes. MongoDB is a NoSQL Document-oriented Database which stores the data in form of key-value pairs expressed in JSON or BSON; it provides high performance and scalability along with data modelling and data management of huge sets of data in an enterprise application. PostgreSQL is a SQL database designed to handle a range of workloads in many applications supporting many concurrent users; it is a feature-rich database with high extensibility, which allows users to create custom plugins, extensions, data types, common table expressions to expand existing features I have recently been involved in the development of a MongoDB Decoder Plugin for PostgreSQL, which can be paired with a logical replication slot to publish WAL changes to a subscriber in a format that MongoDB can understand. Basically, we would like to enable logical replication between MongoDB (as subscriber) and PostgreSQL (as publisher) in an automatic fashion. Since both databases are very different in nature, physical replication of WAL files is not applicable in this case. The logical replication supported by PostgreSQL is a method of replicating data objects changes based on replication identity (usually a primary key) and it would be the ideal choice for this purpose as it is designed to allow sharing the object changes between PostgreSQL and multiple other databases. The MongoDB Decoder Plugin will play a very important role as it is directly responsible for producing a series of WAL changes in a format that MongoDB can understand (ie. Javascript and JSON). In this blog, I would like to share some of my initial research and design approach towards the development of MongoDB Decoder Plugin. 2. ArchitectureSince it is not possible yet to establish a direct logical replication connection between PostgreSQL and MongoDB due to two very different implementations, some kind of software application is ideally required to act as a bridge between PostgreSQL and MongoDB to manage the subscription and publication. As you can see in the image below, the MongoDB Decoder Plugin associated with a logical replication slot and the bridge software application are required to achieve a fully automated replication setup. Unfortunately, the bridge application does not exist yet, but we do have a plan to develop such application in near future. So, for now, we will not be able to have a fully automated logical replication setup. Fortunately, we can utilize the existing pg_recvlogical front end tool to act as a subscriber of database changes and publish these changes to MongoDb in the form of output file, as illustrated below. With this setup, we are able to verify the correctness of the MongoDB Decoder Plugin output against a running MongoDB in a semi-automatic fashion. 3. Plugin UsageBased on the second architecture drawing above without the special bridge application, we expect the plugin to be used in similar way as normal logical decoding setup. The Mongodb Decoder Plugin is named wal2mongo as of now and the following examples show the envisioned procedures to make use of such plugin and replicate data changes to a MongoDB instance. First, we will have to build and install wal2mongo in the contrib source folder and start a PostgreSQL cluster with the following parameters in postgresql.conf. The wal_level = logical tells PostgreSQL that the replication should be done logically rather than physically (wal_level = replica). Since we are setting up replication between 2 very different database systems in nature (PostgreSQL vs MongoDB), physical replication is not possible. All the table changes will be replicated to MongoDB in the form of logical commands. max_wal_senders = 10 limits the maximum number of wal_sender proccesses that can be forked to publish changes to subscriber. The default value is 10, and is sufficient for our setup. wal_level = logicalmax_wal_senders = 10 On a psql client session, we create a new logical replication slot and associate it to the MongoDB logical decoding plugin. Replication slot is an important utility mechanism in logical replication and this blog from 2ndQuadrant has really good explaination of its purpose: (https://www.2ndquadrant.com/en/blog/postgresql-9-4-slots/) $ SELECT * FROM pg_create_logical_replication_slot('mongo_slot', 'wal2mongo'); where mongo_slot is the name of the new logical replication slot and wal2mongo is the name of the logical decoding plugin that you have previously installed in the contrib folder. We can check the created replication slot with this command: $ SELECT * FROM pg_replication_slots; At this point, the PostgreSQL instance will be tracking the changes done to the database. We can verify this by creating a table, inserting or deleting some values and checking the change with the command: $ SELECT * FROM pg_logical_slot_get_changes('mongo_slot', NULL, NULL); Alternatively, one can use pg_recvlogical front end tool to subscribe to the created replication slot, automatically receives streams of changes in MongoDB format and outputs the changes to a file. $ pg_recvlogical --slot mongo_slot --start -f mongodb.js Once initiated, pg_recvlogical will continuously stream database changes from the publisher and output the changes in MongoDB format and in mongodb.js as output file. It will continue to stream the changes until user manually terminates or the publisher has shutdown. This file can then be loaded to MongoDB using the Mongo client tool like this: $ mongo &lt; mongodb.jsMongoDB shell version v4.2.3connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { \"id\" : UUID(\"39d478df-b8ca-4030-8a05-0e1ebbf6bc44\") }MongoDB server version: 4.2.3switched to db mydbWriteResult({ \"nInserted\" : 1 })WriteResult({ \"nInserted\" : 1 })WriteResult({ \"nInserted\" : 1 })bye where the mongodb.js file contains: use mydb;db.table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});db.table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});db.table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”}); 4. TerminologyBoth databases use different terminologies to describe the data storage. Before we can replicate the changes of PostgreSQL objects and translate them to MongoDB equivalent, it is important to gain clear understanding of the terminologies used on both databases. The table below is our initial terminology mappings: PostgreSQL Terms MongoDB Terms MongoDB Description Database Database A physical container for collections Table Collection A grouping of MongoDB documents, do not enforce a schema Row Document A record in a MongoDB collection, can have difference fields within a collection Column Field A name-value pair in a document Index Index A data structure that optimizes queries Primary Key Primary Key A record’s unique immutable identified. The _id field holds a document’s primary key which is usually a BSON ObjectID Transaction Transaction Multi-document transactions are atomic and available in v4.2 5. Supported Change OperationsOur initial design of the MongoDB Decoder Plugin is to support database changes caused by clauses “INSERT”, “UPDATE” and “DELETE”, with future support of “TRUNCATE”, and “DROP”. These are few of the most common SQL commands used to alter the contents of the database and they serve as a good starting point. To be able to replicate changes caused by these commands, it is important that the table is created with one or more primary keys. In fact, defining a primary key is required for logical replication to work properly because it serves as replication identity so the PostgreSQL can accurately track a table change properly. For example, if a row is deleted from a table that does not have a primary key defined, the logical replication process will only detect that there has been a delete event, but it will not be able to figure out which row is deleted. This is not what we want. The following is some basic examples of the SQL change commands and their previsioned outputs: $ BEGIN;$ INSERT INTO table1(a, b, c) VALUES(1, 'Cary', '2020-02-01');$ INSERT INTO table1(a, b, c) VALUES(2, 'David', '2020-02-02');$ INSERT INTO table1(a, b, c) VALUES(3, 'Grant', '2020-02-03');$ UPDATE table1 SET b='Cary'; $ UPDATE table1 SET b='David' WHERE a = 3;$ DELETE FROM table1;$ COMMIT; The simple SQL commands above can be translated into the following MongoDB commands. This is a simple example to showcase the potential input and output from the plugin and we will introduce more blogs in the near future as the development progresses further to show case some more advanced cases. db.table1.insert({“a”: 1, “b”: “Cary”, “c”: “2020-02-01”})db.table1.insert({“a”: 2, “b”: “David”, “c”: “2020-02-02”})db.table1.insert({“a”: 3, “b”: “Grant”, “c”: “2020-02-03”})db.table1.updateMany({“a”: 1, “c”: ”2020-02-01”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 2, “c”: ”2020-02-02”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 3, “c”: ”2020-02-03”}, {$set:{“b”: “Cary”}}) db.table1.updateMany({“a”: 3, “c”: “2020-02-03”, {$set:{“b”: “David”}})db.table1.remove({“a”: 1, “c”: ”2020-02-01”}, true)db.table1.remove ({“a”: 2, “c”: ”2020-02-02”}, true)db.table1.remove ({“a”: 3, “c”: ”2020-02-03”}, true) 6. Atomicity and TransactionsA write operation in MongoDB is atomic on the level of a single document, and since MongoDB v4.0, multi-document transaction control is supported to ensure the atomicity of multi-document write operations. For this reason, the MongoDB Deocoder Plugin shall support 2 output modes, normal and transaction mode. In normal mode, all the PostgreSQL changes will be translated to MongoDB equivalent without considering transactions. In other words, users cannot tell from the output if these changes are issued by the same or different transactions. The output can be fed directly to MongoDB, which can gurantee certain level of atomicity involving the same document Since MongoDB v4.0, there is a support for multi-document transaction mechanism, which acts similarly to the transaction control in PostgreSQL. Consider a normal insert operation like this with transaction ID = 500 within database named “mydb” and having cluster_name = “mycluster” configured in postgresql.conf: $ BEGIN;$ INSERT INTO table1(a, b, c) VALUES(1, 'Cary', '2020-02-01');$ INSERT INTO table1(a, b, c) VALUES(2, 'Michael', '2020-02-02');$ INSERT INTO table1(a, b, c) VALUES(3, 'Grant', '2020-02-03');$ COMMIT; In normal output mode, the plugin will generate: use mydb;db.table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});db.table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});db.table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”}); In transaction output mode, the plugin will generate: session500_mycluster = db.getMongo().startSession();session500_mycluster.startTransaction();use mydb;session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 1, \"b\": \"Cary\", \"c\": “2020-02-01”});session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 2, \"b\": \"David\", \"c\": “2020-02-02”});session500_mycluster.getDatabase(\"mydb\").table1.insert({\"a\": 3, \"b\": \"Grant\", \"c\": “2020-02-03”});session500_mycluster.commitTransaction();session500_mycluster.endSession(); Please note that the session variable used in the MongoDB output is composed of the word session concatenated with the transaction ID and the cluster name. This is to gurantee that the variable name will stay unique when multiple PostgrSQL databases are publishing using the same plugin towards a single MongoDB instance. The cluster_name is a configurable parameter in postgresql.conf that is used to uniquely identify the PG cluster. The user has to choose the desired output modes between normal and transaction depending on the version of the MongoDB instance. MongoDB versions before v4.0 do not support multi-document transaction mechanism so user will have to stick with the normal output mode. MongoDB versions after v4.0 have transaction mechanism supported and thus user can use either normal or transaction output mode. Generally, transaction output mode is recommended to be used when there are multiple PostgreSQL publishers in the network publishing changes to a single MongoDB instance. 7. Data TranslationPostgreSQL supports far more data types than those supported by MongoDB, so some of the similar data types will be treated as one type before publishing to MongoDB. Using the same database name, transaction ID and cluster name in previous section, the table below shows some of the popular data types and their MongoDB transaltions. PostgreSQL Datatype MongoDB Datatype Normal Output Transaction Output smallint integer bigint numeric integer db.table1.insert({“a”:1}) session500_mycluster.getDatabase(“mydb”).table1.insert(“db”).table1.insert({“a”: 1}) character character varying text json xml composite default other types string db.table1.insert({“a”: “string_value”}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: “string_value”}) boolean boolean db.table1.insert({“a”:true}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: true}) double precision real serial arbitrary precision double db.table1.insert({“a”:34.56}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”: 34.56}) interval timestamp data time with timezone time without timezone timestamp db.table1.insert({“a”: new Date(“2020-02-25T19:33:10Z”)}) db.table1.insert({“a”: new Date(“2020-02-25T19:33:10+06:00”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10Z”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:new Date(“2020-02-25T19:33:10+06:00”)}) hex bytea bytea UUID binary data db.table1.insert({“a”: UUID(“123e4567-e89b-12d3-a456-426655440000”)}) db.table1.insert({“a”:HexData(0,”feffc2”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:UUID(“123e4567-e89b-12d3-a456-426655440000”)}) session500_mycluster.getDatabase(“mydb”).table1.insert({“a”:HexData(0,”feffc2”)}) array array db.table1.insert({ a: [ 1, 2, 3, 4, 5 ] } ) db.table1.insert({ a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } ) session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ 1, 2, 3, 4, 5 ] } ) session500_mycluster.getDatabase(“mydb”).table1.insert( { a: [ “abc”, “def”, “ged”, “aaa”, “xxx” ] } ) 8. ConclusionMongoDB has gained a lot of popularity in recent years for its ease of development and scaling and is ideal database for data analytic purposes. Having the support to replicate data from multiple PostgreSQL clusters to a single MongoDB instance can bring a lot of value to industries focusing on data analytics and business intelligence. Building a compatible MongoDB Decoder Plugin for PostgreSQL is the first step for us and we will be sharing more information as development progresses further. The wal2mongo project is at WIP/POC stage and current work can be found here: https://github.com/HighgoSoftware/wal2mongo.","link":"/2020/03/12/Logical-Replication-Between-PostgreSQL-and-MongoDB/"},{"title":"2020 PG Asia Conference Ended Successfully at an Unprecedented Scale!","text":"IntroductionOn November 17-20, 2020, PostgresConf.CN &amp; PGconf.Asia2020 (referred to as 2020 PG Asia Conference) was held online for the very first time! This conference was jointly organized by the PG China Open Source Software Promotion Alliance, PostgresConf International Conference Organization, and PGConf.Asia Asian Community. This conference was broadcast exclusively via the Modb Technology Community platform in China with a record-high number of viewers streaming the conference events. With the great support from these PostgreSQL communities, the conference was held with great success, which brought together the Chinese PG power, major Asian PG contributors and many PostgreSQL experts worldwide to build the largest PG ecosystem in Asia. About the ConferenceAlso known as the Asia’s largest open source relational database ecology conference PostgresConf.CN and PGConf.Asia, for the very first time, were hosted together as one conference online accompanied by additional offline sessions hosted at several reputable university campuses in China. PostgresConf.CN is an annual conference held by the China PostgreSQL Association for PostgreSQL users and developers. It is also one of the conference series held by PostgresConf Organization. PostgreConf.CN 2019 took place in Beijing, it was very well attended by PostgreSQL users and community members across the globe. PGCONF.Asia is also an annual PostgreSQL event that took place in Bali Indonesia in 2019, it was a continuation of the PGCONF.Asia event that took place in Tokyo, Japan in 2018. The first PGCONG.Asia conference took place in 2016 in Tokyo, this conference acts as a hub of PostgreSQL related development and technical discussion among PostgreSQL users and developers in the region as well as experts from around the globe. Learn more about these conferences and the organizers from these resources: 2020.postgresconf.cn 2020.pgconf.asia PostgresConf SponsorsThis conference was sponsored by: Platinum: Alibaba Cloud Tencent Cloud EDB HighGo Software Golden: Inspur Power Systems Silver: Equnix Business Solutions 14 Conference Channels over 4 Days!The conference lasted 4 days at an unprecedented scale. A total of 14 channels, including 5 technical training + 9 main/sub-forum training channels. Alibaba Cloud Database Training Session Tencent Cloud Database Training Session HighGo Software Training Session PG Training Institution Charity Session PostgresConf Orgnization English Training Session (1 Day) the main forum (2 days) Chinese sub-forum (2 days) English sub-forum A (2 days) English sub-forum B (2 days) CCF advanced academic forum (1 day) Over 100 Participating Experts and Scholars Around the World The number of invited speakers for this conference has set a new record. With the theme of “Born into the World”, the conference gathered 112 technical presentations and more than 100 well-known experts and scholars around the world to provide a grand technical feast for all the participating PGers. Guangnam Ni, Fellow of the Chinese Academy of Engineering Peng Liu, Vice chairman of China Open Source Software Promotion Alliance and researcher of the Chinese Academy of Sciences Zhiyong Peng, deputy director of the database committee of the Chinese Computer Society Bruce Momjian, co-founder of PostgreSQL international community and vice president of EDB Peter Zaitsev, Founder and CEO of Percona Tatsuo Ishii, the original author of Pgpool-ll and founders of PGconf.asian and Japanese PG user association Experts from from Alibaba, Tencent, Amazon, JD.com, Inspur, Ping An, Suning, ZTE, HornetLab, Equnix, VMware Greenplum, yMatrix, HighGo Software, Yunhe Enmo, Percona, EDB, NTT, Postgres Professional, Fujitsu, AsiaInfo, Giant Sequoia, Mechuang, Wenwu, Guoxin Sinan, Hytera, Airwallex, Ottertune and many others. Professors from Wuhan University, East China Normal University, Harbin Institute of Technology, Shandong University, CCF (China Computer Society) database committee members of Tianjin University Professional lecturers from 10 well-known authorized PG training service providers And many, many more! Record High LIVE Streaming ViewersThe number of LIVE streaming viewers at this conference has also hit a new record. Each channel accumulated over 30,000 active LIVE streams, the official conference blog views accumulated over 50,000+ views, and the news reports and articles from media exceeded over 3,000 entries. Conference HighlightsPostgresConf is an annual event for PostgreSQL developers and users worldwide. This conference attracted core members from the global PostgreSQL community, as well as corporate and individual users who use PostgreSQL. The PGConf.Asia conference went smoothly for 4 days and it attracted many domestic and foreign audiences worldwide to join the LIVE streaming channels. The conference has 100+ subject sharing sessions, and each session on average accumulated 30,000+ active streams. The first two days of the conference, several leading internet service vendors such as Alibaba Cloud, Tencent Cloud and database vendor HighGo Software brought a series of rich technical contents related to PostgreSQL, providing tremendous amount of values to the audiences who are willing to learn PostgreSQL database technology The third and fourth days of the conference consist of numerous technical presentations covering wide range of area of technical interests. Let’s take a look at some of the highlights of the conference! Opening Speech by Fellow Guangnam NiThe conference was kick-started by an opening speech by Guangnan Ni, fellow of the Chinese Academy of Engineering and the leader of China’s basic software industry. Fellow Ni first expressed his warm congratulations on the holding of the conference and sincere greetings to the representatives of the participating countries and open source experts. He pointed out that open source is an important trend in today’s world development, and it is profoundly changing the global digital economy and information industry. This conference brings great value to the development of the open source industry, promotes the open source movement and popularization of open source culture in Chin and also strengthens international exchanges and cooperation. Fellow Ni hopes to use the PGConf Asia Conference platform to realize intellectual exchanges between Asia and the world, achieve higher and higher levels of global cooperation, and contribute to the development of global open source technology. Finally, Fellow Ni wished the conference a complete success, and wished the PG open source ecosystem more prosperity! *”I wish the Chinese PG branch and the Asian PG community prosper and get better and better!”* Peng LiuPeng Liu, executive vice chairman of China Open Source Software Promotion Alliance and researcher at the Institute of Software and Chinese Academy of Sciences, delivered a speech on behalf of COPU. He pointed out that based on the open BSD license + decentralized ecological architecture, PostgreSQL has a permanent security guarantee in the past, present and future. Following the spirit of free software &amp; complying with the BSD open source agreement, PostgreSQL meets the most stringent security compliance audits, and is not subject to US technology export control jurisdiction restrictions (Long Arm Jurisdiction Limitations/US Export Controls). PostgreSQL is a recognized global technology with public property rights and countless global PG contributors are committed to promoting the free and democratic software movement to protect the rights of anyone who obtain and use such software. Bruce MomjianBruce Momjian, co-founder of the PostgreSQL international community and vice president of EDB, shared “Will Postgres Live Forever” speech. He said that any business has its life cycle, and open source PG is no exception, but compared to closed source commercial software, the life cycle of open source software will have more vitality. As long as the source code is valuable, it can always get a new life. In 1996, Postgres got a new life due to the introduction of the SQL standard and the improvement of its functions. The development trend continues to rise today. Tatuso IshiiPGconf.Asia and Tatsuo Ishii, the founder of the Japanese PG User Association and the original author of Pgpool-ll, shared “Wonderful PostgreSQL!”. Tatsuo Ishii wrote the WAL system by himself based on Gray’s business thesis, and his creativity is admirable. Peter ZaitsevPercona CEO Peter Zaitsev made a sharing of “The Changing Landscape of Open Source Databases”. He summarized several key points such as distributed, cloud native, storage and computing separation, and hardware acceleration, which basically covered the main focuses of the current database technology development. Zhiyong PengProfessor Peng Zhiyong, deputy director of the database committee of CCF China Computer Society, deputy dean of the School of Computer Science of Wuhan University, made a sharing of “My Way from PG to TOTEM”. Professor Peng talked about his 30-years of database research, from leading students to in-depth PG source code research database models, to compiling PG kernel analysis textbooks, to the development of the totem (TOTEM) database. As we all know, PG originated from the University of California, Berkeley. This conference was also fortunate enough to have invited Professor Peng from Wuhan University. Professor Peng has been deeply involved in the PG database for more than 30 years and has led students to in-depth PG source code research database model, to write PG kernel and analysis book. Eventually created the totem (TOTEM) database based on PG. Peng has made outstanding contributions to PostgreSQL talent training and research! Guoqiang GaiGuoqiang Gai, the founder of Enmotech, gave a speech on “Observing the elephant: the skills migration from Oracle to PostgreSQL DBA”. Gai gave a principle analysis from a source code perspective through a typical PG rollback problem, encouraging everyone to play with open source databases to learn the source code deeply, emphasizing that DBA is essential to enterprise data management, and every DBA must pass self-employment training to reflect personal value. Whether it is PG or PG-related database products, its value must always be reflected by helping companies manage core data assets. Here, Gai said that the role of DBA is crucial for the last mile from database products to users. They are the closest partners of databases, whether in client companies, database cloud vendors or software suppliers or integrators. Gai encouraged DBAs to take advantage of open source and analyze in-depth source code to solve problems. Julyanto SUTANDANGJulyanto SUTANDANG, CEO of Equnix Business Solutions, gave a very detailed presentation about the professional PostgreSQL certifications and how these certifications can help an individual advance his or her career in PostgreSQL related fields. Certification is one of the best ways to tell your client, or your boss that you are the right person for the job. Whether you are a regular user, a government regulator, a professor or a subcontractor, there will always be a suitable level of PostgreSQL certification for your needs! Lei FengLei Feng, founder and general manager of Greenplum China founder shared “Greenplum’s open source journey: ecology and community leadership” speech. the AI-enabled database is also leading the future development direction of the database. Lei shared the trilogy of their core team’s digital advancement in the cloud era and emphasized that the core competitiveness of database companies in the future will be the exploration and practice of AI models. Shuxin FangShuxin Fang, general manager of the technical support department of Inspur Business Machines, shared his presentation on “K1 power and PostgreSQL help enterprises to build new normal and new core”. Zhengsheng YeZhengsheng Ye, general manager of Alibaba Cloud Database Products and Head of Database Products and Operations, shared a speech of “Database Development Trend”. Yicheng WangYicheng Wang, Deputy General Manager of Tencent Cloud Database, shared the “Database Behind Every WeChat Payment”. Tencent Cloud uses Tbase’s distributed solution to carry the WeChat payment business, and at the same time, it continues to enhance the core value of the product through cluster scalability, enhanced security, and efficient data compression, providing DBaaS for more enterprise users. Xiaojun ZhengXiaojun Zheng, Chief Scientist of HighGo Software, shared the “Review of Commercial Database Technology Innovation”. With 30 years of senior management experience in several well-known database companies, he elaborated on major innovations in commercial databases in the past 30 years, which has important guiding significance for future database development. HighGo database (HGDB) values data security with great importance when fulfilling the demands of enterprise users, and it has enhanced the security functions through a variety of technical means, including separation of powers, FDE full disk encryption, security auditing, security marking, etc. More features of the security version can be referred to The image below figure. Not only that, the HighGo enterprise cluster database also has flexible scalability, high availability, and effective load balancing. Chaoqun ZhanChaoqun Zhan, a researcher of Alibaba Group and head of the OLAP product department of the database product division, shared “Opportunities and Challenges of Cloud Native Data Warehouse”. The development of database technology is mainly affected by business scenario requirements and development factors of hardware technology. From the perspective of business scenario requirements, Alibaba Cloud, as a domestic leading cloud vendor, mainly integrates PolarDB+ADB full-line database products and integrates PG to respond to users’ various business scenarios. Zhan shared a cloud-native integrated solution within Alibaba Group, which provides extreme performance and extremely fast and flexible expansion of cloud-native DBaaS. Bohan ZhangBohan Zhang, the co-founder of Ottertune, shared effective solutions from the Carnegie Mellon University laboratory for the automatic tuning of PG parameters, and provided DBA recruitment information. If you are interested, please contact Zhang directly Closing the ConferenceOn the last day of the conference, Peter Zaitsev, CEO of Percona, shared the topic “17 Things Developers Need to Know About Databases” to help the database developers out there (including PG and non-PG developers) to increase their productivity, their quality of work, maintain a good relationship with DevOps and most importantly, avoid deadly and expensive mistakes! About China PostgreSQL AssosicationThe China PostgreSQL Association is a legitimate and non-profit organization under the China Open Source Software Promotion Alliance. The association’s main focus is to conduct activities around PostgreSQL, organize operations, promote PostgreSQL, host trainings, contribute to technological innovations and implementations. In addition, the association aims to promote the localization of the database development by bridging the PostgreSQL Chinese community with the International community. Official Website http://www.postgresqlchina.com","link":"/2020/11/27/2020%20PG%20Asia%20Conference%20Ended%20Successfully%20at%20an%20Unprecedented%20Scale!/"},{"title":"In-Memory Table with Pluggable Storage API","text":"1. IntroductionThis blog is to follow up on the post I published back in July, 2020 about achieving an in-memory table storage using PostgreSQL’s pluggable storage API. In the past few months, my team and I have made some progress and did a few POC patches to prove some of the unknowns and hypothesis and today I would like to share our progress. 2. The PostgreSQL Buffer ManagerIn the previous post, I mentioned that we would like to build a new in-memory based storage that is based on the existing buffer manager and its related component and hooked it up with the pluggable storage API. To achieve this, my team and I underwent an in-depth study to understand how the current buffer manager works in PostgreSQL and this chapter at interdb.jp is a good starting point for us to gain a general understanding of the buffer manager design in good details. The current PostgreSQL buffer manager follows a 3-layer buffer design to manage the data pages as illustrated by this image below: where it consists of Buffer Table (hash table) Buffer Descriptors (Array) Buffer Pool (Array) 2.1 Page TableBuffer Table is used like a routing table between PostgreSQL Core and the buffer manager. It is managed using the existing hash table utilities and uses buffer_tag to look up the page descriptor and buffer id. Buffer_tag is a structure that contains the table space, database, table name. 2.2 Buffer DescriptorBuffer Descriptor is used to store the status of a buffer block and also the content lock. Refcount is a part of the buffer state, will be used to indicate the insert and delete operation. it will be increased by one when there is an insertion, and decreased by one when there is a deletion. The Vacuum process will reclaim this page once refcount reaches to 0. 2.3 Buffer PoolBuffer Pool has a one to one relationship with buffer descriptor. it can be treated a simple pointer pointing to the beginning of the buffer pool, each buffer pool slot is defined as 8KB for now. This is the lowest layer in the buffer manager structure before a page is flushed to disk. The BM_DIRTY status flag is used to indicate if a page in the buffer pool is to be flushed to disk In addition to buffer pool, buffer manager also utilizes a ring buffer for reading and writing a huge table whose size exceeds 1/4 of the buffer pool size. Clock Sweep algorithm is used to find a victim page in the ring buffer to eject and flush to disk so new page can enter, thus the name, ring buffer. 3. The In-Memory Only Buffer ManagerHaving a general understanding of the existing buffer manager’s strucutre, we hypothesize that we could potentially improve its IO performance by eliminating the need to flush any buffer data to disk. This means that the in-memory only version of buffer manager itself is the storage media. For this reason, its strucutre can be simplified as: where the buffer descriptor points to a dedicated memory storage that contains the actual page and tuple. This memory space can be allocated to a certain size at initlaization and there will not be a need to flush a page to disk. All data page and tuple will reside in this memory space. In the case where there is a huge reading and writing load, the ring buffer will not be allocated as the logic to find a victim page to evict and flush to disk will be removed since everything will reside in a dedicated memory space. For this reason, if the memory space is not sufficiently allocated, the user will get “no unpin buffer” is available, which basically means “your disk is full” and you need to do delete and vacuum. Using this approach, when the server shuts down or restarts, the data in this memory space is of course lost. So, data persistence to disk would be a topic of interest next, but right now, we already see some useful business case with this in-memory based table where data processing speed is more important than data persistence 4. Initial ResultsUsing the same tuple structure and logic as the current heap plus the memory based buffer manager with 1GB of memory allocated, we observe some interesting increase in performance comparing to PostgreSQL with default settings. For 20 million row, we observe about 50% increase for insert, 70% increase for update, 60% increase for delete and 30% increase in vacuum. This result is not too bad considering we are still at the early stages and I am sure there are many other ways to make it even faster 5. Next StepHaving some solid results from the initial test, it would make sense for us to also be looking into having the index tuples as in-memory storage only. In addition, free space map and visibility map files that are stored in the PG cluster directory could potentially also be made as in-memory to possibly further increase the DML performance.","link":"/2020/09/23/In-memory-Table-with-Pluggable-Storage-API/"},{"title":"TLS Related Updates in PostgreSQL 13","text":"1. IntroductionThe upcoming major release of PostgreSQL 13 has several important behavioral updates related to the TLS implementation. These updates may have some to your current PostgreSQL security deployment if you were considering to upgrade to 13 when it officially releases. Today I would like to do a quick summary of these updates. 2. Minimum TLS Version Changed to TLSv1.2There are 2 server parameters in postgresql.conf that influence the desired TLS versions to use during communication, ssl_min_protocol_version and ssl_max_protocol_version. In previous PG versions, the default value for ssl_min_protocol_version was TLSv1, in which many older versions of OpenSSL could support. In PG13, this default has been raised to TLSv1.2, which satisfies current industry’s best practice. This means that if your psql client is built using older version of OpenSSL, such as 1.0.0., the PG13 server by default will deny this TLS connection, until either you rebuild the psql client with newer version of OpenSSL, or you lower the ssl_min_protocol_version back to TLSv1. Lowering this default is strongly discouraged as older versions of TLS are not as secured and many industrial applications are communicating using TLSv1.2 as a standard requirement now. 3. New libpq connection parametersThere are also several updates to the libpq client side connection parameters related to TLS. See sub sections below: 3.1 ssl_min_protocol_version and ssl_max_protocol_versionThe ssl_min_protocol_version and ssl_max_protocol_version parameters defined in the postgresql.conf on the server side were not available in the libpq client side in the previous PG versions. This means that in previous versions, it was not possible for client to influence the desired version of TLS to used as it would always want to used the newest TLS versions to communicate with the server. This may not be ideal in some cases. In PG13, the same sets of parameters are added to the libpq client side as well such that the client can also play a part in determining the ideal TLS version for communication. These parameters can be specified to psql like so: psql -U user -d &quot;sslmode=require dbname=postgres ssl_max_protocol_version=TLSv1.2&quot;psql -U user -d &quot;sslmode=require dbname=postgres ssl_min_protocol_version=TLSv1.3&quot; The second command in the above example sets minimum TLS protocol to TLSv1.3, which means that this client would only want to communicate with a server that could support TLSv1.3 as minimum version requirement. TLSv1.3 is fairly new and not every PostgreSQL servers are built with this support. In this case, the client simply refuses to communicate. These parameters are great additions to the current libpq as they give the client the ability to enforce the desired TLS version to use rather than simply letting the server to decide, resulting in a much more secured environment. 3.2 channel_bindingChannel binding is a new feature introduced in PG13, in which the libpq client can optionally enable to further increase the security of the TLS connection. The channel_binding parameter can be set to require to enforce the channel binding feature, prefer to only use channel binding if it is available or disable to not use channel binding at all. prefer is the default value for the new channel_binding parameter. The channel binding feature enforces trust between client and server so that Client informs the server whom it thinks it speaks to, and Server validates whether it is correct or not. This prevents an attacker who is able to capture users’ authentication credentials (e.g. OAuth tokens, session identifiers, etc) from reusing those credentials in another TLS sessions. In PG13, the channel binding is done by tying the user’s scram-sha-256 credentials to a unique fingerprint of the TLS session in which they are used (channel binding), so they cannot be reused in another TLS sessions initiated by the attacker. To use this feature, we need to define a rule in pg_hba.conf that uses scram-sha-256 as authentication method. Ex: hostssl all all 127.0.0.1/32 scram-sha-256 also set the default password authentication method to scram-sha-256 in postgresql.conf. password_encryption = scram-sha-256 and then sets a scram-sha-256 password for the current user or another user in a existing psql connection \\passwordor \\password [username] then finally, we can use psql with channel binding to connect to the server. Ex: psql -U user -d &quot;sslmode=require dbname=postgres channel_binding=require ssl_min_protocol_version=TLSv1.2&quot; 3.3 sslpasswordIn previous version of PG, in a sslmode=verify-full case, the client will need to specify its X509 certificate, private key and CA cert to complete the entire TLS authentication. In the case where the private key supplied by the client is encrypted with a password, the psql will prompt the user to enter it before proceeding with the TLS authentication. The new sslpassword parameter allows the user to specify the password to the connection parameters without prompting the user to enter it. This is a useful addition, as the psql command could basically completes without having a human or a bot to enter the passphrase to unlock the private key. This parameter can be used like this: psql -U user -d &quot;sslmode=verify-full dbname=postgres sslrootcert=cacert.pem sslcert=client.pem sslkey=client.key sslpassword=mypass&quot; 4. Remove support for OpenSSL 0.9.8 and 1.0.0Another major change is that PG13 will refuse to be built with OpenSSL versions 1.0.0 and 0.9.8 as these are very old and are no longer considered secured in today’s standard. If you are still using these OpenSSL versions, you will need to upgrade to newer or recent versions to be compatible with PG13. 5. ConclusionPG13 brings many exciting new features and enhancements to PostgreSQL and many of these new changes need to be carefully assessed for potential incompatibility with the previous PG versions. Today, our focus is mainly on new updates related to TLS, which may have behavioral impacts to the existing deployment and the way existing client and server communicates using TLS. For the full changes and potential incompatibilities, visit the official change log here","link":"/2020/08/21/TLS-Related-Updates-in-PostgreSQL-13/"},{"title":"How PostgreSQL Executes Sequential Scans with the Help of Table Access Methods APIs","text":"1. IntroductionThere are many approaches for PostgreSQL to retrieve the data back to the user. Depending on the user’s input query, the planner module is responsible for selecting the most optimum approach to retrieve the requested data. Sequential scan is one of these approaches that is mostly selected when the user requests a large volume of data (for example, “SELECT * from tablename;”) or when a table has no index declared. Sequential scan is mostly handled by the table access method API within PostgreSQL and heap access method is the default one PostgreSQL uses today. In this short post, I will show you how sequential scan is done in the table access method API. 2. Table Access Method APIs in PostgreSQLPluggable table access method API has been made available since PostgreSQL 12, which allows a developer to redefine how PostgreSQL stores / retrieves table data. This API contains a total of 42 routines that need to be implemented in order to complete the implementation and honestly it is no easy task to understand all of them and to implement them. This API structure is defined in tableam.h under the name typedef struct TableAmRoutine Today I will describe the routines related to sequential scan and I hope it could help you if you are someone looking to create your own table access method. 3. Sequential Scan Overall Call FlowFew of the 42 routines will be called by executor just to complete a sequential scan request. This section will describe these routines in the order they are called. 3.1 relation_sizeuint64 (*relation_size) (Relation rel, ForkNumber forkNumber); relation_size is the first routine to be called and it is relatively simple. The caller will expect the routine to return the total size of the relation described by rel and forkNumber. The default heap access method will simply invoke the storage manager smgr to find the number of data blocks this particular relation physically occupies on disk and multiplies that number with the size of each block BLCKSZ (default is 8k). If you are not sure about the relationship between relation and its fork number, you could refer to this blog to get more information. The size returned by this routine basically sets the boundary of our sequential scan. 3.2 slot_callbacksconst TupleTableSlotOps *(*slot_callbacks) (Relation rel); Next, the executor needs to find out which set of tuple table slot (TTS) callback operation this table access method is compatible with. TTS is a set of routines that ensures the tuple storage is compatible between the executor and your access method. The executor will execute the TTS callback to translate your tuple strucuture to TupleTableSlot format in which the executor will understand. The default heap access method uses TTSOpsBufferHeapTuple defined in execTuples.c to handle this operation 3.3 scan_beginTableScanDesc (*scan_begin) (Relation rel, Snapshot snapshot, int nkeys, struct ScanKeyData *key, ParallelTableScanDesc pscan, uint32 flags); Now the scan can officially begin. This is sequential scan’s initialization routine in which it will allocate a new scan descriptor using the parameters passed in by the executor. The purpose of scan descriptor structure is to keep track of the sequential scan while it is being executed. For example, to track where the scan should begin,; when was the block number of the last scan; which block should we resume scanning and how many blocks have been scanned…etc. scan descriptor will be destroyed once the sequential scan has completed. The executor expects the routine to return a fully allocated and initialize pointer to TableScanDesc struct 3.4 scan_getnextslotbool (*scan_getnextslot) (TableScanDesc scan, ScanDirection direction, TupleTableSlot *slot); This is the main routine for sequential scan where the caller expects the routine to fetch one tuple from the buffer manager, converts it to the TTS format in which executor understands and save it in the input pointer called slot. Each call to this routine will results in one tuple to be returned. If a table contains 1000 tuples, this function will be called 1000 times. The boolean return code is the indication to the caller if the routine has more tuples to return, as soon as false is returned, it signals the executor that we have exhausted all the tuples and it should stop calling this function. In normal sequential scan case, this routine works in per-page mode. This means it will read one full block from buffer manager and scan it to get all the tuple addresses and their offsets in the scan descriptor, so in the subsequent calls to the same function, it will not load the full page again from buffer manage all the time; it will only start to load the next block when all the tuples on the current block have been scanned and returned. As you can see, the scan descriptor plays an important role here as most of the control information is saved there and is regularly updated whenever a call the scan_getnextslot is made. 3.5 scan_endvoid (*scan_end) (TableScanDesc scan); This is the last routine to be called to basically clean up the table scan descriptor, which was used heavily during the sequential scan. At this point, the executor should already have all the tuple information from the sequential scan methods. 4. Prepare the Data to ReturnNow, the executor is finished with the table access method and has already had access to all the tuples for a particular relation. It then needs to go through another round of filtering to determine which of these tuples satisfy the condition set by the user, (for example, when user gives the WHERE clause to limit the scan results). This is done in another infinite for loop in execScan.c to perform ExecQual on each TTS. Finally, the end results will be sent to the end user. 5. SummaryWhat we have discussed here is the basic call flow of a simple sequential scan. If we were to visualize the process, it should look something like this:","link":"/2021/01/14/How-PostgreSQL-Executes-Sequential-Scans-with-the-Help-of-Table-Access-Methods-APIs/"},{"title":"Benefits of External Key Management System Over the Internal and how these could help securing PostgreSQL","text":"1. IntroductionData and user security have always been important considerations for small to large enterprises during the deployment of their database or application servers. PostgreSQL today has rich support for many network level and user level security features. These include TLS to secure database connections, internal user authentication, integration with external user authentication services such as RADIUS, LDAP and GSSAPI, and TLS certificate based user authentication …etc. However, it does not yet support Transparent Data Encryption (TDE) feature where all the database files and logs have an option to be encrypted before written to disk or decrypted when retrieving from the disk. This adds extra security measure to protect against disk theft. All these features have something in common; they all use cryptographic keys (either symmetrical or asymmetrical, statically generated or exchanged on the fly using Diffie Hellman) in some ways to achieve the security goals. It is quite common for an organization to focus entirely on the actual data encryption part but pay minimal attention to the cryptographic keys that make the encryption possible. In fact, data encryption is the easy part, the protection of the cryptographic keys is often the hardest as it has several levels of complexities. A group of members (including myself) from the PostgreSQL community are actively working on TDE and internal KMS features towards PG14 and there has been some good work done on the internal KMS module with future support of integrating with an external KMS already in discussion. You may find the current status and the work in progress from these links below: PostgreSQL TDE Wiki PageInternal KMS for PostgreSQL Today I would like to discuss the benefits of external key management system over the internal. 2. CompliancePerhaps one of the biggest benefits of having an external KMS is compliance. For organizations that are mandated by the government to have the FIPS 140-2 compliance certification, the external KMS could potentially help them achieve FIPS 140-2 level 3 (tamper resistant key storage) certification. Internal KMS, on the other hand, may provide FIPS 140-2 level 1 (stop the use of unsafe algorithms) and at most up to FIPS 140-2 level 2 (store keys in tamper proof evident hardware). Depending on the industry governance, there may be a strict requirement that any key materials and encrypted data must be stored separately. For example, in the Payment Card Industry Data Security Standard (PCI DSS) requires that the cardholder data and encryption keys must be protected and stored separately. For this reason alone, certain data sensitive organizations cannot consider PostgreSQL as their choice of database due to lack of support to external KMS. 3. Deployment FlexibilityAnother benefit of external KMS is the flexibility in deployment. When deploying their IT infrastructure, many organizations face the decision whether to maintain all applications including key management system on site or host them in a dedicated data center or even to the cloud. With external KMS, it is possible for these organizations to have a future proof deployment where all the cryptographic keys are centrally managed and will ensure their solution will work in any of the deployment scenarios. With internal KMS, these organizations will not have the flexibility in the deployment because the application will be tied to the storage space within itself. Taking PostgreSQL as an example, each database cluster has a different storage space. With the internal KMS, each cluster manages its own set of cryptographic keys. In a larger deployment scenario where multiple PG instances will be deployed, it will become very difficult to manage the life cycle of the cryptographic keys used in each cluster. 4. Complete Life Cycle Management for Encryption KeysNormally, a complete life cycle of an encryption key is very likely involved in the following phases. Depending on the business cases, some phases can be omitted. Key generation Key registration Key storage Key distribution and installation Key usage Key rotation Key backup Key recovery Key revocation Key suspension Key destruction It is possible for an Internal KMS to support all of the phases within its storage space and allows an user to manage the key life cycle but this process will not scale as scope requirement increases. External KMS, on the other hand, provides a centralized life cycle management of all the keys that it is managing. This is a much simpler approach to manage all the key materials in a larger deployment scenario. While it is not a security best practice, it is quite common to start a project with the internal KMS and later migrate to external one. Many of the key manager software vendors have support for key migration. 5. Security AuditThis is again a requirement for certain corporate and industry compliance where there must be a detailed audit log capturing all the key usages, rotations, who accessed the key and at what time. Depending on the requirement, certain alert mechanism may be required to be implemented to alert the key administrator about any potential issues that could rise from the cryptographic key operations. With an external KMS system, it tends to be much easier to streamline the key audit reports for all the keys it is managing and easier to prove to customers or potential auditors that the keys are indeed very secured and closely monitored. Taking the PostgreSQL as an example and it’s current work on internal KMS, it is quite difficult to prove that the keys are stored securely without the auditing mechanism to closely monitor the key usages and their lifecycles. Even if there is a complete suite of auditing mechanism in place, the auditing is still a difficult and costly operation to perform especially when there are multiple servers deployed. This would result in all storage systems to be audited individually. 6. Key Management Duty SeparationNormally, the key administrators of an external KMS has the ability to configure permissions for all the cryptographic keys that it manages. Permissions such as intended purpose, owner, validity period and other user attributes. PostgreSQL and the current work on internal KMS, on the other hand, does not have this level of granularity in administrative roles. The database administrator is also the encryption key administrator. This may be an issue in certain compliance requirement like HIPAA where it is required that both roles must be separated for proper data access. 7. ConclusionExternal KMS indeed has several more benefits over the internal KMS as it provides additional management, compliance and control. It is an increasing trend that more security-conscious organizations are driving their integrations and deployments with an external KMS in the design. However, it does not mean the internal KMS should not be used at all. For smaller organziation and deployment, it is quite normal to start with the internal KMS and later migrate the key to an external KMS as the deployment gets larger in size. There is an active internal KMS development in PostgreSQL community and it can achieve basic key life cycle management and the next big focus would be on Transparent Data Encryption (TDE) and eventually an extension that supports communication to an external KMS. Key Management Interoperability Protocol (KMIP) is a communication standard set out by Organization for the Advancement of Structured Information Standards (OASIS) to enable a secured communication between key management systems and cryptographically-enabled applications such as PostgreSQL. There is not many C-based open source KMIP client implementations today that can be utilized to develop a PostgreSQL extension that acts as a KMIP client to talk to external KMS, but I am sure as externa key management becomes a mainstream, there will be many different versions of implementation emerging in the market.","link":"/2020/05/14/Benefits-of-External-Key-Management-System-Over-the-Internal-and-how-these-could-help-securing-PostgreSQL/"},{"title":"How to Analyze a PostgreSQL Crash Dump File","text":"1. IntroductionIn this blog post, I will talk about how to enable the generation of crash dump file (also known as core dump) and some common GDB commands to help a developer troubleshoot a crash-related issues within PostgreSQL and also other applications. Proper analysis of the issue normally will take time and certain degree of knowledge about the application source code. From experience, sometimes it may be better to look at the bigger environment instead of looking at the point of crash. 2. What is a Crash Dump File?A crash dump file is a file that consists of the recorded state of the working memory of an application when it crashes. This state is represented by stacks of memory addresses and CPU registers and normally it is extremely difficult to debug with only memory addresses and CPU registers because they tell you no information about the application logic. Considering the core dump contents below, which shows the back trace of memory addresses to the point of crash. #1 0x00687a3d in ?? ()#2 0x00d37f06 in ?? ()#3 0x00bf0ba4 in ?? ()#4 0x00d3333b in ?? ()#5 0x00d3f682 in ?? ()#6 0x00d3407b in ?? ()#7 0x00d3f2f7 in ?? () Not very useful is it? So, when we see a crash dump file that looks like this, it means the application is not built with debugging symbols, making this crash dump file useless. If this is the case, you will need to install the debug version of the application or re-build the application with debugging enabled. 3. How to Generate a Useful Crash Dump FileBefore the generation of crash dump file, we need to ensure the application is built with debugging symbols. This can be done by executing the ./configure script like this: ./configure enable-debug This adds the -g argument to CFLAGS in src/Makefile.global with optimization level set to 2 (-O2). My preference is to also change the optimization to 0 (-O0) so when we are navigating the stack using GDB, the navigation will make much more sense rather than jumping around and we will be able to print out most variables values in memory instead of getting optimized out error in GDB. CFLAGS = -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Werror=vla -Wendif-labels -Wmissing-format-attribute -Wimplicit-fallthrough=3 -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -Wno-format-truncation -g -O0 Now, we can enable the crash dump generation. This can be done by the user limit command. ulimit -c unlimited to disable: ulimit -c 0 Make sure there is enough disk space because crash dump file is normally very large as it records all the memory execution states from start to crash, and make sure the ulimit is set up in the shell before starting PostgreSQL. When PostgreSQL crashes, a core dump file named core will be generated in $PGDATA 4. Analyzing the Dump File using GDBGDB (GNU Debugger) is a portable debugger that runs on many Unix-like systems and can work with many programming languages and is my favorite tool to analyze a crash dump file. To demonstrate this, I will intentionally add a line in PostgreSQL source code that will result in segmentation fault crash type when a CREATE TABLE command is run. Assuming the PostgreSQL has already crashed and generated a core dump file core in this location ~/highgo/git/postgres/postgresdb/core. I would first use the file utility to understand more about the core file. Information such as the kernel info, and the program that generated it. caryh@HGPC01:~$ file /home/caryh/highgo/git/postgres/postgresdb/corepostgresdb/core: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from 'postgres: cary cary [local] CREATE TABLE', real uid: 1000, effective uid: 1000, real gid: 1000, effective gid: 1000, execfn: '/home/caryh/highgo/git/postgres/highgo/bin/postgres', platform: 'x86_64'caryh@HGPC01:~$ The file utility tells me that the core file is generated by this application /home/caryh/highgo/git/postgres/highgo/bin/postgres, so I would execute gdb like this: gdb /home/caryh/highgo/git/postgres/highgo/bin/postgres -c /home/caryh/highgo/git/postgres/postgresdb/coreGNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-gitCopyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-linux-gnu&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...Reading symbols from /home/caryh/highgo/git/postgres/highgo/bin/postgres...done.[New LWP 27417][Thread debugging using libthread_db enabled]Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.Core was generated by `postgres: cary cary [local] CREATE TABLE '.Program terminated with signal SIGSEGV, Segmentation fault.#0 heap_insert (relation=relation@entry=0x7f872f532228, tup=tup@entry=0x55ba8290f778, cid=0, options=options@entry=0, bistate=bistate@entry=0x0) at heapam.c:18401840 ereport(LOG,(errmsg(&quot;heap tuple len = %d&quot;, heaptup-&gt;t_len)));(gdb) Immediately after running gdb on the core file, it shows the location of the crash at heapam.c:1840 and that is exactly the line I have intentionally added to cause a crash. 5. Useful GDB CommandsWith gdb, it is very easy to identify the location of a crash, because it tells you immediately after running gdb on the core file. Unfortunately, 95% of the time, the location of the crash is not the real cause of the problem. This is why I mentioned earlier that sometimes it may be better to look at the bigger environment instead of looking at the point of crash. The crash is likely caused by a mistake in the application logic some where else in the application before it hits the point of crash. Even if you fix the crash, the mistake in application logic still exists and most likely, the application will crash somewhere else later or yield unsatisfactory results. Therefore, it is worth awhile to understand some of the powerful GDB commands that could help us understand the call stacks better to identify the real root cause. 5.1 The bt (Back Trace) commandThe bt command shows a series of call stacks since the beginning of the application all the way to the point of crash. With full debugging enabled, you will be able to see the function arguments and values being passed in to each function calls as well as the source file and line numbers where they were called. This allows developer to travel backwards to check for any potential application logic mistake in the earlier processing. (gdb) bt#0 heap_insert (relation=relation@entry=0x7f872f532228, tup=tup@entry=0x55ba8290f778, cid=0, options=options@entry=0, bistate=bistate@entry=0x0) at heapam.c:1840#1 0x000055ba81ccde3e in simple_heap_insert (relation=relation@entry=0x7f872f532228, tup=tup@entry=0x55ba8290f778) at heapam.c:2356#2 0x000055ba81d7826d in CatalogTupleInsert (heapRel=0x7f872f532228, tup=0x55ba8290f778) at indexing.c:228#3 0x000055ba81d946ea in TypeCreate (newTypeOid=newTypeOid@entry=0, typeName=typeName@entry=0x7ffcf56ef820 &quot;test&quot;, typeNamespace=typeNamespace@entry=2200, relationOid=relationOid@entry=16392, relationKind=relationKind@entry=114 'r', ownerId=ownerId@entry=16385, internalSize=-1, typeType=99 'c', typeCategory=67 'C', typePreferred=false, typDelim=44 ',', inputProcedure=2290, outputProcedure=2291, receiveProcedure=2402, sendProcedure=2403, typmodinProcedure=0, typmodoutProcedure=0, analyzeProcedure=0, elementType=0, isImplicitArray=false, arrayType=16393, baseType=0, defaultTypeValue=0x0, defaultTypeBin=0x0, passedByValue=false, alignment=100 'd', storage=120 'x', typeMod=-1, typNDims=0, typeNotNull=false, typeCollation=0) at pg_type.c:484#4 0x000055ba81d710bc in AddNewRelationType (new_array_type=16393, new_row_type=&lt;optimized out&gt;, ownerid=&lt;optimized out&gt;, new_rel_kind=&lt;optimized out&gt;, new_rel_oid=&lt;optimized out&gt;, typeNamespace=2200, typeName=0x7ffcf56ef820 &quot;test&quot;) at heap.c:1033#5 heap_create_with_catalog (relname=relname@entry=0x7ffcf56ef820 &quot;test&quot;, relnamespace=relnamespace@entry=2200, reltablespace=reltablespace@entry=0, relid=16392, relid@entry=0, reltypeid=reltypeid@entry=0, reloftypeid=reloftypeid@entry=0, ownerid=16385, accessmtd=2, tupdesc=0x55ba8287c620, cooked_constraints=0x0, relkind=114 'r', relpersistence=112 'p', shared_relation=false, mapped_relation=false, oncommit=ONCOMMIT_NOOP, reloptions=0, use_user_acl=true, allow_system_table_mods=false, is_internal=false, relrewrite=0, typaddress=0x0) at heap.c:1294#6 0x000055ba81e3782a in DefineRelation (stmt=stmt@entry=0x55ba82876658, relkind=relkind@entry=114 'r', ownerId=16385, ownerId@entry=0, typaddress=typaddress@entry=0x0, queryString=queryString@entry=0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;) at tablecmds.c:885#7 0x000055ba81fd5b2f in ProcessUtilitySlow (pstate=pstate@entry=0x55ba82876548, pstmt=pstmt@entry=0x55ba828565a0, queryString=queryString@entry=0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;, context=context@entry=PROCESS_UTILITY_TOPLEVEL, params=params@entry=0x0, queryEnv=queryEnv@entry=0x0, qc=0x7ffcf56efe50, dest=0x55ba82856860) at utility.c:1161#8 0x000055ba81fd4120 in standard_ProcessUtility (pstmt=0x55ba828565a0, queryString=0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;, context=PROCESS_UTILITY_TOPLEVEL, params=0x0, queryEnv=0x0, dest=0x55ba82856860, qc=0x7ffcf56efe50) at utility.c:1069#9 0x000055ba81fd1962 in PortalRunUtility (portal=0x55ba828b7dd8, pstmt=0x55ba828565a0, isTopLevel=&lt;optimized out&gt;, setHoldSnapshot=&lt;optimized out&gt;, dest=&lt;optimized out&gt;, qc=0x7ffcf56efe50) at pquery.c:1157#10 0x000055ba81fd23e3 in PortalRunMulti (portal=portal@entry=0x55ba828b7dd8, isTopLevel=isTopLevel@entry=true, setHoldSnapshot=setHoldSnapshot@entry=false, dest=dest@entry=0x55ba82856860, altdest=altdest@entry=0x55ba82856860, qc=qc@entry=0x7ffcf56efe50) at pquery.c:1310#11 0x000055ba81fd2f51 in PortalRun (portal=portal@entry=0x55ba828b7dd8, count=count@entry=9223372036854775807, isTopLevel=isTopLevel@entry=true, run_once=run_once@entry=true, dest=dest@entry=0x55ba82856860, altdest=altdest@entry=0x55ba82856860, qc=0x7ffcf56efe50) at pquery.c:779#12 0x000055ba81fce967 in exec_simple_query (query_string=0x55ba82855648 &quot;create table test (a int, b char(10)) using heap;&quot;) at postgres.c:1239#13 0x000055ba81fd0d7e in PostgresMain (argc=&lt;optimized out&gt;, argv=argv@entry=0x55ba8287fdb0, dbname=&lt;optimized out&gt;, username=&lt;optimized out&gt;) at postgres.c:4315#14 0x000055ba81f4f52a in BackendRun (port=0x55ba82877110, port=0x55ba82877110) at postmaster.c:4536#15 BackendStartup (port=0x55ba82877110) at postmaster.c:4220#16 ServerLoop () at postmaster.c:1739#17 0x000055ba81f5063f in PostmasterMain (argc=3, argv=0x55ba8284fee0) at postmaster.c:1412#18 0x000055ba81c91c04 in main (argc=3, argv=0x55ba8284fee0) at main.c:210(gdb) 5.1 The f (Fly) commandThe f command followed by a stack number allows gdb to jump to a particular call stack listed by the bt command and allows you to print other variable in that particular stack. For example: (gdb) f 3#3 0x000055ba81d946ea in TypeCreate (newTypeOid=newTypeOid@entry=0, typeName=typeName@entry=0x7ffcf56ef820 &quot;test&quot;, typeNamespace=typeNamespace@entry=2200, relationOid=relationOid@entry=16392, relationKind=relationKind@entry=114 'r', ownerId=ownerId@entry=16385, internalSize=-1, typeType=99 'c', typeCategory=67 'C', typePreferred=false, typDelim=44 ',', inputProcedure=2290, outputProcedure=2291, receiveProcedure=2402, sendProcedure=2403, typmodinProcedure=0, typmodoutProcedure=0, analyzeProcedure=0, elementType=0, isImplicitArray=false, arrayType=16393, baseType=0, defaultTypeValue=0x0, defaultTypeBin=0x0, passedByValue=false, alignment=100 'd', storage=120 'x', typeMod=-1, typNDims=0, typeNotNull=false, typeCollation=0) at pg_type.c:484484 CatalogTupleInsert(pg_type_desc, tup);(gdb) This forces gdb to jump to stack number 3, which is at pg_type.c:484. In here, you can examine all other variables in this frame (in function TypeCreate). 5.2 The p (Print) commandThe most popular command in gdb, which can be used to print variable addresses and values (gdb) p tup$1 = (HeapTuple) 0x55ba8290f778(gdb) p pg_type_desc$2 = (Relation) 0x7f872f532228(gdb) p * tup$3 = {t_len = 176, t_self = {ip_blkid = {bi_hi = 65535, bi_lo = 65535}, ip_posid = 0}, t_tableOid = 0, t_data = 0x55ba8290f790}(gdb) p * pg_type_desc$4 = {rd_node = {spcNode = 1663, dbNode = 16384, relNode = 1247}, rd_smgr = 0x55ba828e2a38, rd_refcnt = 2, rd_backend = -1, rd_islocaltemp = false, rd_isnailed = true, rd_isvalid = true, rd_indexvalid = true, rd_statvalid = false, rd_createSubid = 0, rd_newRelfilenodeSubid = 0, rd_firstRelfilenodeSubid = 0, rd_droppedSubid = 0, rd_rel = 0x7f872f532438, rd_att = 0x7f872f532548, rd_id = 1247, rd_lockInfo = {lockRelId = {relId = 1247, dbId = 16384}}, rd_rules = 0x0, rd_rulescxt = 0x0, trigdesc = 0x0, rd_rsdesc = 0x0, rd_fkeylist = 0x0, rd_fkeyvalid = false, rd_partkey = 0x0, rd_partkeycxt = 0x0, rd_partdesc = 0x0, rd_pdcxt = 0x0, rd_partcheck = 0x0, rd_partcheckvalid = false, rd_partcheckcxt = 0x0, rd_indexlist = 0x7f872f477d00, rd_pkindex = 0, rd_replidindex = 0, rd_statlist = 0x0, rd_indexattr = 0x0, rd_keyattr = 0x0, rd_pkattr = 0x0, rd_idattr = 0x0, rd_pubactions = 0x0, rd_options = 0x0, rd_amhandler = 0, rd_tableam = 0x55ba82562c20 &lt;heapam_methods&gt;, rd_index = 0x0, rd_indextuple = 0x0, rd_indexcxt = 0x0, rd_indam = 0x0, rd_opfamily = 0x0, rd_opcintype = 0x0, rd_support = 0x0, rd_supportinfo = 0x0, rd_indoption = 0x0, rd_indexprs = 0x0, rd_indpred = 0x0, rd_exclops = 0x0, rd_exclprocs = 0x0, rd_exclstrats = 0x0, rd_indcollation = 0x0, rd_opcoptions = 0x0, rd_amcache = 0x0, rd_fdwroutine = 0x0, rd_toastoid = 0, pgstat_info = 0x55ba828d5cb0}(gdb) With the asteroid, you can tell the p command to either print the address of a pointer or the values pointed by the pointer. 5.3 The x (examine) commandThe x command is used to examine a memory block contents with specified size and format. The following example tries to examine the t_data values inside a HeapTuple structure. Note that we first print the *tup pointer to learn the size of t_data is 176, then we use the x command to examine the first 176 bytes pointed by t_data (gdb) p *tup$6 = {t_len = 176, t_self = {ip_blkid = {bi_hi = 65535, bi_lo = 65535}, ip_posid = 0}, t_tableOid = 0, t_data = 0x55ba8290f790}(gdb) p tup-&gt;t_data$7 = (HeapTupleHeader) 0x55ba8290f790(gdb) x/176bx tup-&gt;t_data0x55ba8290f790: 0xc0 0x02 0x00 0x00 0xff 0xff 0xff 0xff0x55ba8290f798: 0x47 0x00 0x00 0x00 0xff 0xff 0xff 0xff0x55ba8290f7a0: 0x00 0x00 0x1f 0x00 0x01 0x00 0x20 0xff0x55ba8290f7a8: 0xff 0xff 0x0f 0x00 0x00 0x00 0x00 0x000x55ba8290f7b0: 0x0a 0x40 0x00 0x00 0x74 0x65 0x73 0x740x55ba8290f7b8: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7c0: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7c8: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7d0: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7d8: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7e0: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7e8: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f7f0: 0x00 0x00 0x00 0x00 0x98 0x08 0x00 0x000x55ba8290f7f8: 0x01 0x40 0x00 0x00 0xff 0xff 0x00 0x630x55ba8290f800: 0x43 0x00 0x01 0x2c 0x08 0x40 0x00 0x000x55ba8290f808: 0x00 0x00 0x00 0x00 0x09 0x40 0x00 0x000x55ba8290f810: 0xf2 0x08 0x00 0x00 0xf3 0x08 0x00 0x000x55ba8290f818: 0x62 0x09 0x00 0x00 0x63 0x09 0x00 0x000x55ba8290f820: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x000x55ba8290f828: 0x00 0x00 0x00 0x00 0x64 0x78 0x00 0x000x55ba8290f830: 0x00 0x00 0x00 0x00 0xff 0xff 0xff 0xff0x55ba8290f838: 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00(gdb) 7. ConclusionIn this blog, we have discussed about how to generate a useful crash dump file with sufficient debug symbols to help developers troubleshoot a crash issue in PostgreSQL and also in other applications. We have also discussed about a very powerful and useful debugger gdb and shared some of the most common commands that can be utilized to troubleshoot a crash issue from a core file. I hope the information here can help some developers out there to troubleshoot issues better.","link":"/2020/11/06/How-to-Analyze-a-PostgreSQL-Crash-Dump-File/"},{"title":"Understanding the Security Around PostgreSQL","text":"1. What Is Security?The word “Security” is a very broad concept and could refer to completely different procedures and methodology to achieve. Knowing what security means to your application is very important, so you could execute proper security practices and procedures to ensure the safety of your company’s assets. Data compromises could often lead to financial loss, reputation damage, consumer confidence disintegration, brand erosion, and non-compliance of government and industry regulation. For this reason, the security on infrastructure software such as PostgreSQL is even more important because any data compromises could have nation or city-wide impacts, which are often difficult to completely recover. 2. Common Database CompromisesUser Compromises: Excessive privileges Privilege abuse Weak user authentication Weak password Default privilege too open Data Compromises: Unmanaged and unprotected sensitive data Backup data exposure Stolen hard disks Unmanaged encryption keys Network Compromises: Firewall rules Deep Packet Inspection (DPS) Vulnerability prevention Denial of Service (DOS) attack Vulnerability: Software bug Buffer overflow SQL injection Privileged escalation 3. The Big Picture This picture shows different types of “security” around a PostgreSQL server and there are roughly 5 types of security concepts involved here: 3.1 Data Security (Over Network)This is the security in the communication between PostgreSQL client and server that we almost always want to use TLS to encrypt the data communication in a production environment. TLS guarantees the mutual trust between the client and the server so each side is sure that it is communicating with the right entity instead of a rogue server. SSH tunneling is also a common option to secure a psql connection when TLS is not fully set up. SSH tunneling is also very secure as each connection forces client and server to generate and agree on an encryption key that is valid only for that session. Furthermore, SSH tunneling can be made more secured by setting up the public and private key pair between client and server to ensure the authenticity of the two entities. 3.2 Data SecurityThis is the security between PostgreSQL and the disk in which it writes data to. This security type is often refereed as a “Cluster Data Encryption” or “Transparent Data Encryption”. Current version of PostgreSQL does not support this feature but there is a handful of talented people working on this feature right now. This security is designed to prevent data compromises directly done on the hard disk. By encrypting the data on the disk, hard disk theft will not be able to extract useful information from the hard disk. 3.3 Network SecurityThis is the security that most likely will involve a firewall in between a connecting client and a server. The purpose of a firewall is to block most of the malicious connections coming from the public network and prevent unauthorized access to the server. Most advanced firewalls such as an IPS can block DOS attacks and perform deep packet examination according to a database of known malicious packet and attacks. There are also firewalls such as an IDS that perform network monitoring only and will raise alert to the operator should it detects an attack attempt. 3.4 VulnerabilityThis is the security that is mostly caused by a software bug that allows an attacker to take advantage of the server, steal data, or simply out a stop to the server and cause damage. The best way to prevent this is upgrade your PostgreSQL server to the latest version that has addressed most of the known vulnerabilities. 3.5 User SecurityThis is the security that relates mostly to the user management, sometimes called a Role-Based Access Control (RBAC). This is where a database administrator is managing each database user and setting the right privileges for the right users. Excessive privileges, weak passwords and privilege abuses are very common if not done correctly. Make sure the right users get the right privileges and use a third party authentication servers such as LDAP or Kerberos instead of simple passwords can significantly increase the security ratings of your database infrastructure.","link":"/2021/08/06/Understanding-the-Security-Around-PostgreSQL/"},{"title":"How PostgreSQL Inserts A New Record With The help of Table Access Method API and Buffer Manager","text":"1. IntroductionThis blog talks about a high level description of the mechanism behind PostgreSQL to execute an INSERT query. This process involves many steps of processing before the data is put in the right place. These process normally involves several catalog cache lookup to determine if the destination table exists or several checking on the constraint violations..etc. This blog will mainly focus on the part where the processing handle is passed to the PostgreSQL’s table access method API and its interaction with buffer manager and WAL routines. This is also the core area where an INSERT query is actually executed. If you are a developer looking to understand how PostgreSQL works internally, this blog may be helpful to you… 2. Table Access Method APIs in PostgreSQLPluggable table access method API has been made available since PostgreSQL 12, which allows a developer to redefine how PostgreSQL stores / retrieves table data. This API contains a total of 42 routines that need to be implemented in order to complete the implementation and honestly it is no easy task to understand all of them and to implement them. This API structure is defined in tableam.h under the name typedef struct TableAmRoutine Today I will describe the routines related to INSERT. 3. INSERT Query Overall Call FlowA few of the 42 routines will be called by executor just to complete an INSERT query. This section will describe these routines in the order they are called. 3.1 slot_callbacksconst TupleTableSlotOps *(*slot_callbacks) (Relation rel); The executor needs to find out which set of tuple table slot (TTS) callback operation this table access method is compatible with. TTS is a set of routines that ensures the tuple storage is compatible between the executor and your access method. The executor will execute the TTS callback to translate your tuple strucuture to TupleTableSlot format in which the executor will understand. The default heap access method uses TTSOpsBufferHeapTuple defined in execTuples.c to handle this operation 3.2 heap_insertvoidheap_insert(Relation relation, HeapTuple tup, CommandId cid, int options, BulkInsertState bistate) heap_insert is the entry point to perform the actual data insertion and it will undergo several other routines provided by buffer manager and WAL module in order to complete the insertion. heap_prepare_insertstatic HeapTuple heap_prepare_insert(Relation relation, HeapTuple tup, TransactionId xid, CommandId cid, int options); This is a subroutine for heap_insert where it will initialize the tuple header contents such as relation OID, infomasks, xmin, xmax values. It will also determine if the tuple is too big that TOAST is required to complete the insertion. These terms and parameters are very technical in PostgreSQL. If you are not sure what exactly they are, you could refer to resources here and here. RelationGetBufferForTupleextern Buffer RelationGetBufferForTuple(Relation relation, Size len, Buffer otherBuffer, int options, BulkInsertStateData *bistate, Buffer *vmbuffer, Buffer *vmbuffer_other); This is an entry function to access buffer manager resources and all it is doing is ask the buffer manager to return a buffer ID that can be used to store the target tuple. This may sound very straightforward, but there is quite a lot of processing on the buffer manager side to properly determine a desired buffer location. First, it will do a quick size check. If the input tuple is larger than the size of each buffer block, it will return immediately with error as TOAST has to be used in this case. Then it will try to put the tuple on the same page the system last inserted the tuple on to see if it will fit there. If not, it will utilize the free space map to find another page that could fit tuple. If that does not work out, then buffer manage will allocate a new data page (also referred to as extend) to be used to hold this new tuple. As soon as we have a desired buffer page determined, buffer manager will cache this page in the relation structure such that next time the same relation visits the buffer manager, it knows immediately about the reference to the last inserted block. RelationPutHeapTupleextern void RelationPutHeapTuple(Relation relation, Buffer buffer, HeapTuple tuple, bool token); Once we have identified the location of the buffer to store the tuple, the insert routine will then call RelationPutHeapTuple to actually put the tuple in the specified buffer location. This routine will again ask the buffer manager to get a pointer reference to the data page using the buffer ID we obtained from RelationGetBufferForTuple, then add the tuple data using PageAddItem() routine. Internally in buffer manager, it manages the relationship between buffer ID, buffer descriptor and the actual pointer to the data page to help us correctly identify and write to a data page. After a successful write, the routine will save a CTID of the inserted tuple. This ID is the location of this tuple and it consists of the data page number and the offset. For more information about how buffer manager works, you can refer to the resource here Mark buffer dirtyextern void MarkBufferDirty(Buffer buffer); At this point, the tuple data is already stored in the buffer manager referenced by a particular data page plus an offset, but it is not yet flushed to disk yet. In this case, we almost always will have to call MarkBufferDirty function to signal buffer manager that there are some tuples on the page that have not been flushed to disk and therefore in the next checkpoint, it will ensure the new tuples are flushed to disk. [Insert WAL Record]Last but not least, after doing all the hassle of finding a buffer location to put our tuple in and mark it as dirty, it is time for the heap_insert routine to populate a WAL record. This part is not the focus of this blog so I will skip the high level details of WAL writing. 3.3 End of the insertionAt this point the insertion of a new tuple data has finished and proper WAL record has been written. The routine will once again save the CTID value that we derived during the data insertion and save this value to the TTS structure so the executor also gets a copy of the location of the tuple. Then it will clean up the local resources before returning. 4. SummaryWhat we have discussed here is the basic call flow of a simple sequential scan. If we were to visualize the process, it should look something like this:","link":"/2021/02/02/How-PostgreSQL-Inserts-A-New-Record-With-The-help-of-Table-Access-Method-API-and-Buffer-Manager/"},{"title":"PostgresConf.CN and PGConf.Asia 2021 (China Branch) Join Forces Again to Bring You the Best!","text":"About the EventIn November, 2020, PostgresConf.CN and PGconf.Asia2020 cooperatively organized 2020 PG Asia Conference online for the very first time! This conference attracted over 100 participating experts, scholars and speakers around the world to deliver a grand technical feast for all the participants. The event was broadcast exclusively via the Modb Technology Community platform in China with a record-high number of viewers streaming the conference channels. Each channel on average accumulated over 30,000+ active LIVE streams, the official conference blog views accumulated over 50,000+ views, and the news reports and articles from media exceeded over 3,000+ entries. This year, both PostgresConf.CN and PGConf.Asia 2021 (China Branch) will join forces again to bring you the best PostgreSQL conference in Asia! Official Event Website - 2021 Time and Rough AgendaThe 2021 PG Asia Conference will be a 4-day event from December 14 to 17, 2021. Rough schedule is as below. Please note that the final schedule is subject to the date of the meeting. The agenda time is Beijing Time(GMT+8) December 14 - Day 1 09:00-18:00 Main Conference December 15 - Day 2 09:00-17:00 Mandarin sub-Comprehensive forum 09:00-17:00 Indonesia sub 09:00-17:00 Korean Sub 09:00-17:00 English sub 09:00-12:00 Special Training A 14:00-16:30 Special Training B December 16 - Day 3 09:00-17:00 Mandarin sub-Application Forum 09:00-17:00 Japanese Sub 09:00-17:00 Thai Sub 09:00-12:00 Special Training C 10:00-11:30 Developer Unconference 14:00-16:30 Special Training D December 17 - Day 4 09:00-12:00 Special Training E 14:00-16:30 Special Training F About PostgresConf.CNPostgresConf.CN is an annual event held by the China PostgreSQL Association for PostgreSQL users and developers, PostgreSQL is a leading open-source relational database with an active and vibrant community. PostgreConf.CN 2020 took place as an online conference for the very first time; it was very well attended by PostgreSQL users and community members across the globe. It is also one of the conference series held by PostgresConf Organization. Official Event Website - 2020 About PGConf.AsiaPGCONF.Asia is a shorthand of the Asian PostgreSQL Conference which has been hosted in Tokyo from 2016 until 2018. The conference was held in Bali for the first time in Sep 2019 and it is intended as the Asian level of PostgreSQL International Conference. It co-hosted the 2020 PG Asia Conference together with PostgresConf.CN as an online event and accumulated unprecedented number of participants worldwide. The organization is often regarded as the HUB and the Summit of the PostgreSQL Community throughout Asian Countries. Call for PaperEach speaker is given a 30-minute session (25 minutes of speech + 5 minutes of Q&amp;A).The direction of the speech can be: kernel development, application practice, operation and maintenance management, data migration, distributed, high availability, security control, optimization, cloud database, open source upstream and downstream ecology, etc. Please submit your speech topic first. The deadline for topic submission is October 31, 2021. The organizing committee will complete topic review before November 15, 2021. You will be notified the result of your topic selection at that time, please start preparing the speech content. The deadline for PPT submission is November 30, 2021. Please download the PPT template here. You will have to create a free Postgres Conference account in order to submit your topic. Access the account registration here ContactsGlobal Sponsor Interface: grantzhou@postgresconf.orgAbout Speech submission and other inquiries: meeting@postgresqlchina.com","link":"/2021/10/25/PostgresConf-CN-and-PGConf-Asia-2021-China-Branch-Join-Forces-Again-to-Bring-You-the-Best/"},{"title":"PostgreSQL 14 Continuous archiving and Point In Time Recovery Tutorial","text":"1. IntroductionRecently I have been practicing the internals of PostgreSQL continuous archiving and the point in time recovery features that are required for my current development work. Today I would like to demonstrate these important features with the recently released PostgreSQL 14 on Ubuntu 18.04. 2. Write Ahead Log?Before going into the details of continuous archiving, it is important to understand the concept of Write Ahead Log (WAL). WAL files are generated by PG that contains all of the operations done on the database since the beginning. Operations such as INSERT, UPDATE, DELETE, VACUUM …etc are captured in WAL files. Having these WAL files, it is possible to recreate the database simply by replaying them, allowing an user to recover the database to a certain state in case of fault. This is the basis of continuous archiving and point in time recovery. 3. What is Continuous Archiving?The generated WAL files are normally stored in the pg_wal directory within the PG database cluster, but they will not grow forever. The configuration parameters, max_wal_size and min_wal_size control how many WAL files can be kept in pg_wal directory. The checkpointer process will periodically purge the old WAL files, leaving only recent ones. So, it is important to set up continuous archiving so all these WAL files can be archived to somewhere else outside of PG cluster. So, when you need all the old WAL files for recovery, PG can restore them from the archive. To enable WAL archiving and restoring, set these parameters in postgresql.conf: archive_mode = onarchive_command = 'cp %p /path/to/archive/%f'restore_command = 'cp /path/to/archive/%f %p' where you should replace /path/to/archive with your own archive path on your system. %p and %f will be swapped with path to WAL segment and WAL segment name to complete the command. When a WAL segment is ready to be archived, PG will create a signal file in pg_wal/archive_status to indicate a particular WAL segment is ready for archive. In the example below, the segment 00000001000000000000000E is ready for archive, indicated by the postfix .ready while all of the previous segments have been successfully archived and therefore indicated by the .done postfix. $ ls pgtest/pg_wal/archive_status/000000010000000000000002.done 000000010000000000000005.done 00000001000000000000000A.done 00000001000000000000000E.ready000000010000000000000003.done 000000010000000000000007.done 00000001000000000000000B.done000000010000000000000004.done 000000010000000000000008.done 00000001000000000000000D.done The PG’s archiver process will then be waken up to perform the archive by running the archive_command configured. $ps -ef | grep postgrescaryh 1487 1 0 11:10 ? 00:00:00 postgres -D /home/caryh/pgtestcaryh 1510 1487 0 11:10 ? 00:00:00 postgres: checkpointercaryh 1511 1487 0 11:10 ? 00:00:00 postgres: background writercaryh 1512 1487 0 11:10 ? 00:00:00 postgres: walwritercaryh 1516 1487 0 11:10 ? 00:00:00 postgres: autovacuum launchercaryh 1520 1487 0 11:10 ? 00:00:00 postgres: archiver archiving 00000001000000000000000Ecaryh 1521 1487 0 11:10 ? 00:00:00 postgres: stats collectorcaryh 1522 1487 0 11:10 ? 00:00:00 postgres: logical replication launcher Note that the archiver (PID=1520) also shows its progress on the ps display. When done successfully, the signal file in pg_wal/archive_status will be updated to .done postfix. $ ls pgtest/pg_wal/archive_status/000000010000000000000002.done 000000010000000000000005.done 00000001000000000000000A.done 00000001000000000000000E.done000000010000000000000003.done 000000010000000000000007.done 00000001000000000000000B.done000000010000000000000004.done 000000010000000000000008.done 00000001000000000000000D.done In the next checkpoint, these .done files will be removed so these status files will not be continuously growing as well. 4. What is Point In Time Recovery (PITR)?Having all of the WAL segments backed up in a separate archive, we gained the ability to recovery the database up yo a certain point in time in the past or completely recover the whole database. This depends on your use case, if you made a major mistake and need to start again from a point of time in the past, you can have PG to recover to that particular time during recovery mode and continue the database operation from that point. This is also referred to as switching to a new time line ID and we will discuss this more in the next blog. Let’s continue from the above example (which already has 1 million rows of data) and do a point in time recovery. Make a basebackup of the current database, we can use pg_basebackup to achieve this $ pg_basebackup -U caryh -h 127.0.0.1 --progress -D pgtest-back back to the database and continue inserting some more data, use pg_switch_wal to immediate write out the WAL segment and obtain a LSN. LSN stands for Log Sequence Number, and it logically represent a WAL entry within a WAL segment. Refer to documentation here for more information. After obtaining the LSN, we again insert some more rows of data. insert into test values(generate_series(1,1000000), 'asdas');insert into test values(generate_series(1,1000000), 'asdas');pg_switch_wal(); pg_switch_wal--------------- 0/13DAC308insert into test values(generate_series(1,1000000), 'asdas');insert into test values(generate_series(1,1000000), 'asdas'); So, in total, this table test should have 5 million rows of data, because it started with 1 million and we just inserted 4 million more in the example above. The WAL location indicated by LSN 0/13DAC308 indicated a time when the database only contains 3 million rows, and this is the point of time that we would like to recover up to in our example. Stop the database server pg_ctl -D pgtest stop Wipe out everything in this database pgtest $ rm -rf pgtest/ I know it sounds crazy, but remember, we made a basebackup back in (1.) + all the WAL segments in the archive, so technically we still have everything. Copy everything from our basebackup back to pgtest cp -r pgtest-back/* pgtest/ Edit pgtest/postgresql.conf and set your recover target Since we are using LSN as our target, we can simply put the LSN we captured to recovery_target_lsn configuration recovery_target_lsn = '0/13DAC308' PG also supports other ways to define recovery target, based on timestamp, name or xid. Refer to this documentation for other options. Signal the database to run in recovery mode by creating a recovery.signal file under the pgtest cluster $touch pgtest/recovery.signal Start the server$pg_ctl -D pgtest start the server will now start in recovery mode and it will restore WAL files from the archive and perform the recover. You may log in with psql and check that the database should contain also 3 million rows instead of 5. You may notice that even though the database has been recovered to a point of time in the past, you will encounter a database in recovery or read only database error if you intend to insert additional data. This is because we are still in the recovery mode but is currently paused. This is configured by the recovery_target_action option, which defaults to pause. This is actually intended, to allow you to have a moment to check your database and confirm that it is indeed the database state that you would like to recover to. If this is wrong, you can simply shutdown the database and reconfigure the recovery_target_lsn until you reach a desired state of database. Exit the recovery modeOnce you are confirm the database is recovered correctly, you can exit the recovery mode by this psql command:select pg_wal_replay_resume(); This command will end the recovery mode and you should be able to insert additional data to the database. The recovery.signal file will be removed, and the future WAL segments will have a new timeline ID. Timeline ID is also an important aspect of the recovery and we will discuss more on timeline ID in my next post.","link":"/2021/10/01/Set-up-PostgreSQL-Continuous-archiving-and-Perform-Point-In-Time-Recovery/"},{"title":"Using GDB To Trace Into a Parallel Worker Spawned By Postmaster During a Large Query","text":"1. IntroductionI am working on a new PostgreSQL feature that redefines the way a tuple’s visibility status is determined. The feature is working very nicely until I start doing a large SELECT query, which triggers PostgreSQL to spawn multiple parallel workers to process the request. When this happens, the feature I am working on start to yield incorrect results. A good portion of the data tuples returned are missing because they are considered as invisible, while some portion of it remains visible. It immediately came to my attention that the new feature I am working on does not work in parallel worker mode and somehow I need to find a way to debug into a spawned parallel worker to examine how it is computing the visibility and what is missing inside. In this blog, I would like to share with you how I use GDB to debug and trace into a new parallel worker spawned by Postmaster in order to fix the visibility issue. 2. GDB BasicsI wrote another blog previously that shows how to use GDB to trace and debug a PostgreSQL issues and share some of the most common commands that I use every day to resolve software issues. If you are new to GDB, I suggest giving this blog a read here 3. How and When does PG Spawn A New Parallel WorkerWhen you use psql to connect to a PostgreSQL database, it will spawn a new backend worker process to serve this connecting client. Most of the queries you provide will be processed by this backend process, includes SELECT, UPDATE, INSERT…etc. By default, if your SELECT query will require doing a sequential scan over 8MB of data, it will try to use a parallel worker to help speed up the processing. This 8MB threshold can be configured by the min_parallel_table_scan_size parameter in postgresql.conf . There is another configuration parameter max_parallel_workers that controls the maximum number of parallel workers is allowed to be spawned. The default is 8. Technically, I can avoid my visibility issues simply by either setting min_parallel_table_scan_size to a huge number, or setting max_parallel_workers to 0. But this is really not my style, I would like to keep all these goodies that PG provides while being able to solve the problem. To spawn a parallel worker, the psql backend will initialize a parallel worker context in the global process table and a message queue based on shared memory for communication with the backend. Then it sends a signal to postmaster to notify that the global process table has been updated. When postmaster receives the signal, it will load the global process table and found that it needs to spawn a new parallel worker. It will proceed to fork a new parallel worker according to the context information supplied. This information determines the entry point for the parallel worker and what to do once spawned. During processing, the parallel worker and the psql backend use the message queue to communicate tuples back and forth and finally the psql backend will gather together all the data tuples and produce a final result back to the user. 4. Can We Use GDB to attach This Parallel Worker’s PID When Spawned?Technically yes, but the life time of this parallel worker may be very short, by the time you see its PID from the ps -ef command, the worker may have already done its job and exited. This means, it is too late for me to start GDB and attach to its PID. Instead, the technique I am going to show you today will trace the parallel worker from the moment it starts. 5. Tracing the Parallel WorkerI will be using this instance of PostgreSQL server (version 12.5) as an example where PID 11976 is the psql backend process serving the psql client. Pre-Condition:Connect psql to the PostgreSQL server, create an example table and inserted about 2.5M rows of data. This will for sure trigger parallel workers. $ psql -d postgres -U postgres -p 6660psql (12.5)Type &quot;help&quot; for help.postgres=# create table test(a int, b int);CREATE TABLEpostgres=# insert into test values(generate_series(1,500000),1);INSERT 0 500000postgres=# insert into test values(generate_series(1,500000),1);INSERT 0 500000postgres=# insert into test values(generate_series(1,500000),1);INSERT 0 500000postgres=# insert into test values(generate_series(1,500000),1);INSERT 0 500000postgres=# insert into test values(generate_series(1,500000),1);INSERT 0 500000postgres=# Step 1: Attach GDB to the psql backend having PID = 11976 and Set a Break PointI am setting a break point at the function RegisterDynamicBackgroundWorker. This is called when parallel worker is required to complete the query. Setting a breakpoint allows us more control as to when to proceed with a parallel worker spawn. gdb postgres(gdb) attach 11976(gdb) b RegisterDynamicBackgroundWorker Step 2: Attach GDB to the Postmaster having PID = 11959 and Set 2 Break PointsWe are using a second GDB to attach to the postmaster and set 2 break points there. fork_process is the function before postmaster actually spawns a new parallel worker using the system fork() call. ParallelWorkerMain is the main function for the parallel worker after it has been spawned. gdb postgres(gdb) attach 11959(gdb) b fork_process(gdb) b ParallelWorkerMain Step 3: Execute a Large SELECT Query On psql To Trigger the Break Pointspostgres=# select count(*) from test; The RegisterDynamicBackgroundWorker break point will be hit on the first GDB session having attached PID = 11959 Use the continue or c GDB command to continue to spawn the worker Breakpoint 1, RegisterDynamicBackgroundWorker (worker=0x7ffd867f3c80, handle=0x55a009b77388) at bgworker.c:10021002 bool success = false;(gdb) cContinuing. As you continue the first GDB session, the second GDB session will pause due to receipt of a SIGUSR1 signal. This signal tells postmaster to reload the global process table and then spawn a parallel worker. Using the continue command will hit the first break point at fork_process Program received signal SIGUSR1, User defined signal 1.0x00007f301b97d0f7 in __GI___select (nfds=5, readfds=0x7ffd867f47d0, writefds=0x0, exceptfds=0x0, timeout=0x7ffd867f4740) at ../sysdeps/unix/sysv/linux/select.c:4141 in ../sysdeps/unix/sysv/linux/select.c(gdb) cContinuing.Breakpoint 1, fork_process () at fork_process.c:4747 fflush(stdout);(gdb) Step 4: Tell Postmaster GDB To Follow Child Process Instead Of ParentAt this point, the postmaster GDB is now waiting at the fork_process call, which is right before spawning a parallel worker. This is a good time now to tell GDB to follow the child process instead of staying at parent when the process calls fork(). The reason we want to set this late at this moment is because postmaster is occasionally spawning other backend processes such as walsender and walreceiver. Setting to follow child process early may cause our GDB to follow to another backend process that we are not interested in. (gdb) set follow-fork-mode child You may use the continue command after setting it to follow child. Then immediately the GDB will switch to the new child process having PID = 12198 below and hit our second break point ParallelWorkerMain. So, Now the GDB is debugging the parallel worker process instead of the original postmaster. (gdb) cContinuing.[New process 12198][Thread debugging using libthread_db enabled]Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.[Switching to Thread 0x7f301ca79740 (LWP 12198)]Thread 2.1 &quot;postgres&quot; hit Breakpoint 2, ParallelWorkerMain (main_arg=1544458234) at parallel.c:12071207 { Step 5: Continue To Debug The Parallel ProcessUsing the ps -ef | grep postgres command, we can see a new parallel worker being spawned having PID = 12198 At this point, you are free to explore the process flow of the parallel worker. For me, I am debugging the visibility issues, so I will set additional break points at HeapTupleSatisfiesMVCC and TransactionIdIsCurrentTransactionId. In your case, you may be debugging some other functionalities. Being able to debugging into a parallel worker with GDB allows me to see the problems I was having and being able to fix quickly. If you are having trouble tracing into a parallel workers spawned by PostgreSQL during run time, I hope this blog will be helpful to you.","link":"/2021/07/09/Using-GDB-To-Trace-Into-a-Parallel-Worker-Spawned-By-Postmaster-During-A-Large-Query/"},{"title":"The PostgreSQL Timeline Concept","text":"1. IntroductionIn my previous blog here, I discussed about PostgreSQL’s point in time recovery where PostgreSQL supports an ability to recover your database to a specific time, recovery point or transaction ID in the past but I did not discuss in detail the concept of timeline id, which is also important in database recovery. 2. What is timeline ID and Why it is important?A timeline ID is basically a point of divergence in WAL. It represents a point, of to be exact, the LSN of the WAL in which the database starts to diverge. Divergence happens when an user performs a point in time recovery or when the standby server is promoted. The timeline ID is included in the first 8 bytes of WAL segment files under pg_wal/ directory. For example:pg_wal/000000010000000000000001, indicates that this WAL segment belongs to timeline ID = 1 and pg_wal/000000020000000000000001, indicates that this WAL segment belongs to timeline ID = 2 Timeline ID behaves somewhat like git branch function without the ability to move forward in parallel and to merge back to the master branch. Your development starts from a master branch, and you are able to create a new branch (A) from the master branch to continue a specific feature development. Let’s say the feature also involves several implementation approaches and you are able to create additional branches (B, C and D) to implement each approach. This is a simple illustration of git branch: With timeline ID, your database starts from timeline ID 1 and it will stay at 1 for all subsequent database operations. Timeline ID 2 will be created when the user performs a point in time recovery on timeline 1 and all of the subsequnt database operations at this point belong to timeline ID 2. While at 2, the user could perform more PITR to create timeline 3, 4 and 5 respectively. In the previous PITR blog, I mentioned that you could do PITR based on time, a recovery point, a LSN or a transaction ID but all these can only apply to one particular timeline. In postgresql.conf, you can select a desired recovery timeline by the recovery_target_timeline parameter. This parameter can be 'latest', 'current', or 'a particular timeline ID value'. With this configuration, an user is able to recovery the database to a particular point of a particular timeline in the past This is a simple illustration of timeline ID: 3. The History File Associated with a Timeline IDThe history files are created under pg_wal/ directory with a .history postfix when a new timeline Id is created. This file describes all the past divergence points that have to be replayed in order to reach the current timeline. Without this file, it is impossible to tell where a timeline comes from, thus not being able to do PITR. For example, a history file 00000003.history may contain the following contents cat pg_wal/00000003.history1 0/30000D8 no recovery target specified2 0/3002B08 no recovery target specified which means that timeline 3 comes from LSN(0/3002B08) of timeline 2, which comes from LSN(0/30000D8) of timeline 1. 4. Importance of Continuous ArchivingWith the concept of timeline ID, it is possible that the same LSN or the same WAL segments exist in multiple timelines. For example: the WAL segments, 3, 4, 5, 6 exist in both timeline 1 and timeline 2 but with different contents. Since the current timeline is 2, so the ones in timeline 2 will continue to grow forward. 000000010000000000000001000000010000000000000002000000010000000000000003000000010000000000000004000000010000000000000005000000010000000000000006000000020000000000000003000000020000000000000004000000020000000000000005000000020000000000000006000000020000000000000007000000020000000000000008 With more timelines created, the number of WAL segments files may also increase. Sine PG keeps a certain amount of WAL segment files before deleting them, it is super important to archive all the WAL segments to a separate location either by enabling continuous archiving function or using pg_receivewal tool. With all WAL segment files archived in a separate location, the user is able to perform successful point in time recovery to any timeline and any LSN.","link":"/2021/10/29/The-PostgreSQL-Timeline-Concept/"},{"title":"A Look Inside PostgreSQL's Extended Query Protocol","text":"1. IntroductionFew weeks ago, I was tasked to have a detailed look inside PostgreSQL’s extended query protocol and study its internal mechanisms for a project that depends on this particular feature. In this blog, I will explain how extended protocol works in my own words and how it differs from simple query. 2. Simple QueryA client is able to initiate 2 type of queries against a PostgreSQL server, simple query or extended query. Simple query, as the name suggests, is very simple and straightforward to understand. When we launch the psql client tool to connect to a PostgreSQLs server, almost all of the SQL commands sent are considered as simple queries. This includes the use of begin and commit to enclose a transaction, having multiple SQL queries included in one big query separated by a semicolon, executing or defining a function and much more. Simple query automatically follows the standard query processing routines, which consist of these stages: Parser Analyzer Rewriter Planner Executor Refer to this blog here for detailed information about query processing. The communication between the client and the server is also very straightforward. In case of DML operations such as INSERT, the client sends the query to the server for processing and the server responds a CommandComplete message such as INSERT 0 1 followed by a ReadyForQuery(IDLE) message to indicate that the server has finished the query and is now idle. The client can send another query and it follows the same patter. In case of a SELECT query, the server will send the row description followed by the actual row data satisfying the query until there is no more rows to return. In the end, the server sends a ReadyForQuery(IDLE) message to indicate that the server has finished the query and is now idle. 3. Extended QueryExtended query is the other way for the client to complete a query excepts that it breaks down the standard query processing into different steps and the client is responsible to ensure these steps are followed and executed correctly. The client is able to control thses steps by sending the following wire protocol message types: ‘P’ message (Parse) P message takes a generalize query string with data values repalced with placeholders like $1, $2, which can be later substitued with real values in the Bind step. This generalized query string will be parsed via these query processing routines: Parser -&gt; Analyzer -&gt; Rewriter At the end of a successful Parse, a prepared statement is produced, similar to SQL’s PREPARE clause This prepared statement can be named or unnammed (more on this next). This prepared statement is just a representation of the input query and it cannot be executed yet. ‘B’ message (Bind) B takes the named or unnammed prepared statement produced from the P message and replaces the placeholders ($1, $2) with the user-supplied values. After the values are bound to the prepared statement, we basically have a completed query and it will then be fed into the planner stage to produce the most optimized query plan for the query. At the end of a successful planning, a portal is produced. This portal can also be named or unnammed (more on this next) A portal is basically an object that represents how to execute a particular query ‘E’ message (Execute) E takes the named or unnamed portal produced from the B message and actuallly launch the executor to execute the query (or portal). resultant rows are produced (if any) and returns to the client ‘S’ message (Sync) The client has to send a S message to the server to indicate the end of the extended query. This message causes the server to end the current transaction and sends ReadyForQuery(IDLE) message back to client. What is the purpose of separating a simple query into multiple steps? One major benefit of using the extended query is that it can save a lot of unnecessary parsing of the same query structure. Instead, we can have the common strucuture parsed only once and then bind and execute with different values multiple times. 4. Named and Unnamed Prepared Statement and PortalIn previous section we mentioned that a prepared statement and protal can be either named or unnamed during the extended query protocol. What does it mean and what is the significance? The general rule is that PostgreSQL server can only keep one unnamed prepared statement or portal. Requests containing unnamed prepared statemnt or portal will replace the existing ones. With named prepared statement or portal, the server will respectively remember them based on the names given and client is able to invoke them any time or specifically destroyed by sending a Close message. So, it is possible for a client to create multiple prepared statements within a transaction, each having different prepared statement names, and then bind values for them all at the same time by giving them different portal names. Eventually, the client chooses which portal to be executed. More importantly, named prepared statement’s life time lasts for the entire TCP session unless explicitly destroyed; named portal lasts only until the end of transaction or when it is executed or explicitly destroyed. But, does it make client implementation more complicated if extended query is used? Yes, but client has a choice to complicate things or not Taking libpq for example, when extended query is used, it requires the client application to provide a prepared statement name to construct the P (Parse) message, but does not require the client application to provide portal name to construct the B (Bind) message. This means that with libpq, we can send P message multiple times with different prepared statement names, but with B message, it forces to use unnamed portal name, so we always have one portal to execute. This avoids the case with multiple portal names in the client side to manage.","link":"/2021/12/10/A-Look-Inside-PostgreSQL-s-Extended-Query-Protocol/"},{"title":"2021 PG Asia Conference Delivered Another Successful Online Conference Again!","text":"IntroductionOn December 14-17, 2021, PostgresConf.CN &amp; PGconf.Asia2021 (referred to as 2021 PG Asia Conference) was held online successfully and once again listed as a global gold standard conference on Postgresql.org! This conference was jointly organized by the PG China Open Source Software Promotion Alliance, PostgresConf International Conference Organization, PGConf.Asia Asian Community and PG user community groups from the world including: Japan PG User Group Korea PG User Group Russia PG User Group Europe PG User Group Philippine PG User Group Pakistan PG User Group Thailand PG User Group Vietnam PG User Group About the ConferenceAlso known as the Asia’s largest open source relational database ecology conference PostgresConf.CN and PGConf.Asia were hosted together as one conference online for the first time back in 2020. In 2021, both conferences joined forces again to deliver another successful conference that attracted record high numbers of PG enthusiasts from the world! PostgresConf.CN is an annual conference held by the China PostgreSQL Association for PostgreSQL users and developers. It is also one of the conference series held by PostgresConf Organization. PostgreConf.CN 2019 took place in Beijing, it was very well attended by PostgreSQL users and community members across the globe. PGCONF.Asia is also an annual PostgreSQL event that took place in Bali Indonesia in 2019, it was a continuation of the PGCONF.Asia event that took place in Tokyo, Japan in 2018. The first PGCONG.Asia conference took place in 2016 in Tokyo, this conference acts as a hub of PostgreSQL related development and technical discussion among PostgreSQL users and developers in the region as well as experts from around the globe. Learn more about these conferences and the organizers from these resources: 2021.postgresconf.cn 2021.pgconf.asia PostgresConf SponsorsThis conference was sponsored by: Platinum: Tencent Cloud AWS HighGo Database Golden: Inspur Commercial Systems Microsoft Azure Silver: IBM Kylin Software Equnix 14 Conference Channels over 4 Days!This conference was broadcast live with a record-high number of viewers streaming the conference events. It consists of a total of 14 live channels delivering speeches in Mandarin Chinese, English, Korean and Indonesian languages. the conference gathered 101 technical presentations and more than 100 well-known experts and scholars around the world to provide a grand technical feast for all the participating PG enthusiasts. Guangnam Ni, Fellow of the Chinese Academy of Engineering Peng Liu, Vice chairman of China Open Source Software Promotion Alliance and researcher of the Chinese Academy of Sciences Xing Chunxiao, deputy director of the Information System Committee of the China Computer Federation Bruce Momjian, co-founder of PostgreSQL international community and vice president of EDB Peter Zaitsev, Founder and CEO of Percona Ding Zhiming, Director of the Spatiotemporal Data Science Research Center of the Chinese Academy of Sciences Experts from from HighGo, Alibaba, Tencent, Amazon, Kylin, Inspur, Ping An, Meichuang, SphereEx, Inspur Yunxi database, VMware Greenplum, Huawei, Ali cloud, ZTE…etc Professors from Peking University, Tsinghua University, Institute of Information Technology, Chinese Academy of Sciences, Shenzhen University, Zhengzhou University, Shandong Normal University, Suzhou University of Science and Technology and other academic and scientific research institutions And many, many more! With the great support from these PostgreSQL communities, the conference was held with great success, which brought together the Chinese PG power, major Asian PG contributors and many PostgreSQL experts worldwide to build the largest PG ecosystem in Asia. Conference HighlightsOpening Speech by Fellow Guangnam Ni Guangnan Ni first expressed his warm congratulations to the holding of the PGConf.Asia 2021 Asia Conference, and greeted sincerely to the representatives of various countries and open source experts participating in the conference. He pointed out that open source is an important trend in the development of today’s world, and open source technology is profoundly changing the global digital economy and information industry pattern. The holding of this PG Asia Conference is of great significance to the development of the open source industry, the promotion of the open source movement and the popularization of open source culture in China, and the strengthening of international exchanges and cooperation. Bruce Momjian Bruce mojian, as a core group member of the PG international community, has always been keen on sharing themes in the community. This time Bruce made a sharing based on the status quo of PG, project development challenges, competition challenges, and technical challenges, mainly emphasizing that the development process of the PG version has been adapting. Changes in the environment, continuous breakthroughs and innovations, such as SSD optimization, virtualization, containerization, and functional expansion of cloud environments have been completed in the face of technical challenges. For the future evolution of the PG version, it will continue to optimize and iterate on several aspects such as PG write amplification, TDE encryption, and horizontal scalability. Peter ZaitsevMr. Peter Zaitsev, CEO of Percona, has been a guest at the PG Asia Conference for the second time. This time Peter brought us a speech of &lt;&gt;, mainly from cloud computing, PG peripheral ecology, DaaS and K8S. This aspect expresses the openness and flexibility of PG, which enables PG and the evolution of the technical architecture to be better integrated, promote mutual development, and meet the needs of different business scenarios. He said that Percona, which has been engaged in open source database services for many years, loves to embrace PG and make innovations based on PG to bring more value to customers. Chunxiao XingProfessor Xing Chunxiao, deputy dean of the Institute of Information Technology of Tsinghua University and deputy director of the Information System Committee of the China Computer Federation, shared with us the most popular topics of “metaverse”, blockchain, and data lake, emphasizing that data management is a popular technology development Only the continuous evolution and iteration of database management software can better provide support for cutting-edge technologies and make our lives more meaningful. Zhiming Ding The ivorySQL project is a PG open source derivative project with a broad ecological foundation and Chinese characteristics. Its purpose is to develop community collaboration among Chinese developers and allow engineers to create greater personal and social value. The session was presented by Professor Ding Zhiming, an early promoter of research practice in the domestic PG field, the new president of the China PG branch, and director of the Center for Space-Time Data Science (STDMS) of the Chinese Academy of Sciences. Their system completely expounds the necessity and realization idea of “PostgreSQL database software + Chinese characteristics”, the positioning, characteristics and operation mode of the ivorySQL project and community. Through the launch event, the project was officially known to the majority of PG practitioners in China. The launch of IvorySQL attracted many domestic PG enthusiasts, and they have provided suggestions and opinions to make the project better. We feel very thankful fro their effort into this new project. In the mean time, please check out the project’s official website at www.ivorysql.org for the latest developments. Grant Zhou - The Launch of IvorySQL Open Source ProjectThis conference specially invited the well-known domestic database solution supplier, HighGo Database, to jointly organize and launch the HighGo database sub-forum. Jichao Ma, the pre-sales director of HighGo Software, presented &lt;&gt;, and introduced some new features and functions of HighGo Database related to high availability. Grant Zhou, head of the IvorySQL open source community presented &lt;&lt;An Oracle-compatible PostgreSQL open source database - IvorySQL&gt;&gt;, and introduced an Oracle-compatible open source PostgreSQL database ivorySQL. Seoul Sub-Forum In the Seoul sub-forum, four Korean PostgreSQL user group members gave speeches, namely Ioseph Kim, Jaegeun Yu, Daniel Lee, and Lee Jiho. Ioseph Kim is a contributor to PostgreSQL11 and PostgreSQL12 and translated the PostgreSQL Korean documentation. The topic of his speech was &lt;&gt;. In this speech, Loseph Kim talked about the syntax of PostgreSQL’s Lateral Joins, distinct on… etc. He also introduced the new features of PostgreSQL14. Jaegeun Yu has more than 3 years of experience as a PostgreSQL DBA. The topic of his speech is &lt;&gt;. In this speech, Jaegeun Yu talks about the basic principles of function optimization and shows how to write PostgreSQL functions efficiently while porting, and illustrate the migration process with examples. Daniel Lee brought you a speech &lt;&gt;. In this speech, Daniel Lee introduced the high availability technology solution of Citus, and actually demonstrated the steps to build a Citus HA environment based on Patroni. Lee Jiho is the head of the department at insignal in Korea, and has more than 20 years of experience in using PostgreSQL. Lee Jiho brought a speech &lt;&gt;. In this speech, Lee Jiho introduced the changes and upgrades of Cluster, table clustering … etc. Jakarta Sub-Forum In Jakarta sub-forum, CEO-Julyanto Sutandang and CTO-Lucky Haryadi from Equnix Business Solutions brought 3 speeches in Indonesian. Equunix Business Solutions’ CEO-Julyanto Sutandang brought 2 speeches on &lt;&lt;Do we really need Active PostgreSQL? &gt;&gt; and &lt;&lt;In-memory database really faster?&gt;&gt;, Equunix Business Solutions CTO-Lucky Haryadi’s speech topic is &lt;&lt; Can HA help with load balancing? &gt;&gt;. As a technology company with strong influence in the field of PostgreSQL and Linux in Southeast Asia, the speech brought by Equnix Business Solutions is a perfect combination of practice and theory. The successful landing of this Jakarta sub-forum has greatly promoted the dissemination and development of PostgreSQL database technology in Indonesia and even Southeast Asia. Special Thanks To","link":"/2022/01/18/2021-PG-Asia-Conferenece-Delivered-Another-Successful-Online-Conference-Again/"},{"title":"How PostgreSQL Handles Sub Transaction Visibility In Streaming Replication Setup?","text":"1. IntroductionAs an experienced PostgreSQL user, you may have a lot of experience in setting up streaming replication in your database clusters to make multiple backups of your data. But have you wondered how the standby is able to correctly determine if a tuple sent from the primary should be visible to the user or not. In the case where a transaction contains multiple subtransactions, how does the standby determine the visibility in this case? You might say… well it is PostgreSQL so it will just work… This is true. If you are someone who is curious and interested in knowing how PostgreSQL does certain things internally, then this blog may be interesting for you as we will talk about normally transaction and subtransactions and how both affect the standby’s ability to determine tuple visibility. 2. How is Tuple Visibility Determined?PostgreSQL determines a tuple’s visibility based on the concept of transaction ID (a.k.a txid) and it is normally stored in a tuple’s header as either xmin or xmax where xmin holds the txid of the transaction that inserted this tuple and t_xmax holds the txid of the transaction that deleted or updated this tuple. If a tuple has not been deleted, its t_xmax is 0. There is also another mechanism called commit log (a.k.a clog) where it has information of currently active transaction IDs. When an inserted tuple has a valid t_xmin and invalid t_xmax, PostgreSQL will have to consult the clog first to determine if the txid stored in t_xmin is visible or not, then the result will be stored in the tuple’s hint_bit field about the visibility information such that it does not always have to consult the clog which could be resource intensive. If a tuple has a valid t_xmax, then there is no need to consult the clog as the tuple must be invisible. This is just the basic ideas of how visibility is determined but it is enough for me to continue with this blog. For more information about visibility, you could refer to this resource 3. What is a Subtransaction?As the name suggests, it is a smaller transaction that exists within a regular transaction and it can be created using the SAVEPOINT keyword after your BEGIN statement. Normally, when you have issued a BEGIN statement, all the tuples created from the subsequent DML statements such as INSERT or UPDATE will have a txid associated with these tuples and they will all be the same. When you issue a SAVEPOINT statement within this transaction, all the tuples created from the subsequent DML statements will have a different txid that is normally larger than the main transaction’s txid. When you issue the SAVEPOINT statement again, the txid changes again. The advantage of Subtransaction is that in a very large transaction, the entire transaction will not be aborted due to an error. It allows you to rollback to the previously created SAVEPOINT and continue to execute the main transaction. The biggest difference between a subtransaction and a regular transaction is that there is not a concept of commit associated with subtransactions and it will not be recorded in the WAL file. However, if subtransaction is aborted, it will be recorded in the WAL file and subsequently be sent to the standby. 4. How Does a Standby Determines Visibility Information In a Regular Transaction?Here’s an example of a regular transaction without any subtransactions: BEGIN;INSERT INTO test VALUES(111,111111); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,222222); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,333333); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyCOMMIT; =&gt; a XLOG_XACT_COMMIT WAL record having t_xmin = 500 is sent to standby In a general case, the above queries will send at least 4 WAL records to the standby during streaming replication. First 3 contain the actual tuple information with t_xmin set to the current transaction ID which is 500 in the example. In other words, before the COMMIT is issued, the standby should have already received 3 WAL records from primary and completed the redo, meaning that the 3 tuples already exist in standby’s memory buffer. However, they are not visible yet because transaction ID 500 does not exist in the standby. It is until the COMMIT is issued in the primary that will subsequently generate a XLOG_XACT_COMMIT WAL record to be sent to standby to notify that transaction ID 500 is now committed and valid. When standby receives the XLOG_XACT_COMMIT WAL record, it will eventually update to its clog so the subsequent SELECT queries can do visibility check against. 5. How Does a Standby Process Visibility Information In a Regular Transaction Containing Subtransactions?Now, let’s use the same example but add some SAVEPOINT to create subtransactions BEGIN;INSERT INTO test VALUES(111,111111); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,222222); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,333333); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbySAVEPOINT A;INSERT INTO test VALUES(111,444444); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbyINSERT INTO test VALUES(111,555555); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbyINSERT INTO test VALUES(111,666666); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbySAVEPOINT B;INSERT INTO test VALUES(111,777777); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyINSERT INTO test VALUES(111,888888); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyINSERT INTO test VALUES(111,999999); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyCOMMIT; =&gt; a XLOG_XACT_COMMIT WAL record having t_xmin = 500 is sent to standby As you can see, after the creation of SAVEPOINT, the tuple’s t_xmin value will be changed and is no longer equal to the current transaction ID, which is 500 in the example. Before the COMMIT is issued, the standby should have received 9 WAL records, 3 having t_xmin = 500, 3 having t_xmin = 501 and 3 having t_xmin = 502. None of them is visible yet in the standby because these transaction IDs do not exist yet. When the primary issues the COMMIT statement, it will generate a XLOG_XACT_COMMIT WAL record and send to standby containing only the current transaction ID 500 without the 501 and 502. After the commit, the standby is able to see all 9 tuples that are replicated from the primary. Now, you may be asking…. why? How come the standby knows that txid = 501 and 502 are also visible even though the primary never send a XLOG_XACT_COMMIT WAL record to tell the standby that 501 and 502 are also committed. Keep reading to find out. Using the similar example, if we rollback to one of the SAVEPOINTs, the primary will send a XLOG_XACT_ABORT WAL record to notify the standby a transaction ID has become invalid. BEGIN;INSERT INTO test VALUES(111,111111); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,222222); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbyINSERT INTO test VALUES(111,333333); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standbySAVEPOINT A;INSERT INTO test VALUES(111,444444); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbyINSERT INTO test VALUES(111,555555); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbyINSERT INTO test VALUES(111,666666); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standbySAVEPOINT B;INSERT INTO test VALUES(111,777777); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyINSERT INTO test VALUES(111,888888); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyINSERT INTO test VALUES(111,999999); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standbyROLLBACK TO SAVEPOINT B; =&gt; a XLOG_XACT_ABORT WAL record having t_xmin = 502 is sent to standbyCOMMIT; =&gt; a XLOG_XACT_COMMIT WAL record having t_xmin = 500 is sent to standby In this example, when doing a ROLLBACK to SAVEPOINT B, a XLOG_XACT_ABORT WAL record will be sent to the standby to invalidate that the transaction ID 502 is invalid. So when primary commits, records having txid=502 will not be visible as primary has notified. 6. The Secret of Standby’s KnownAssignedTransactionIdsIn addition to handling the XLOG_XACT_COMMIT and XLOG_XACT_ABORT WAL records to determine if a txid is valid or not, it actually manages a global txid list called KnownAssignedTransactionIds. Whenever a standby receives a heap_redo WAL record, it will save its xmin to KnownAssignedTransactionIds. Using the same example as above again here: BEGIN;INSERT INTO test VALUES(111,111111); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,222222); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,333333); =&gt; a heap_redo WAL record having t_xmin = 500 is sent to standby, and standby puts 500 in KnownAssignedTransactionIdsSAVEPOINT A;INSERT INTO test VALUES(111,444444); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,555555); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,666666); =&gt; a heap_redo WAL record having t_xmin = 501 is sent to standby, and standby puts 501 in KnownAssignedTransactionIdsSAVEPOINT B;INSERT INTO test VALUES(111,777777); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,888888); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIdsINSERT INTO test VALUES(111,999999); =&gt; a heap_redo WAL record having t_xmin = 502 is sent to standby, and standby puts 502 in KnownAssignedTransactionIdsCOMMIT; =&gt; a XLOG_XACT_COMMIT WAL record having t_xmin = 500 is sent to standby and standby submits 500, 501 and 502 to clog as valid. Before the COMMIT is issued on the primary, the standby already has txid 500, 501, and 502 present in the KnownAssignedTransactionIds. When the standby receives XLOG_XACT_COMMIT WAL, it will actually submit txid 500 from the WAL plus the 501 and 502 from KnownAssignedTransactionIds to the clog. So that the subsequent SELECT query consult the clog it will say txid 500, 501, and 502 are visible. If the primary sends XLOG_XACT_ABORT as a result of rolling back to a previous SAVEPOINT, that invalid txid will be removed from the KnownAssignedTransactionIds, so when the transaction commits, the invalid txid will not be submit to clog. 7. SummarySo this is how PostgreSQL determines a tuple’s visibility within subtransactions in a streaming replication setup. Of course, there is more details to what is discussed here in this blog, but I hope today’s discussion can help you get a general understanding of how PostgreSQL determines the visibility and maybe it will help you in your current work on PostgreSQL.","link":"/2021/03/03/How-PostgreSQL-Handles-Sub-Transaction-Visibility-In-Streaming-Replication-Setup/"},{"title":"Implement Foreign Scan With FDW Interface API","text":"1. IntroductionRecently I have been tasked to familiarize myself with the Foreign Data Wrapper (FDW) interface API to build a new FDW capable of doing vertical / columnar sharding, meaning that the FDW is capable of collecting column information from multiple sources and combine them together as a result query. I will document and blog about the vertical sharding in later posts. Now, in this blog, I would like to share some of the key findings and understanding of FDW interface related to foreign scan. 2. Scan-related API RoutinesFDW foreign scan API routines have some similarities to the table access method API’s sequential scan routines. You can learn more about it in the blog post here. Most noticeable difference is that the FDW routines require us to take part during the planning stage, so we have to come out with a foreign plan for the exectuor so it knows what and how to perform the foreign scan. In order to complete a foreign scan, the following FDW routines are invoked in order: IsForeignScanParallelSafe GetForeignRelSize GetForeignPaths GetForeignPlan BeginForeignScan IterateForeignScan … IterateForeignScan EndForeignScan 2.1 IsForeignScanParallelSafeIn this routine, you have to tell PG whether or not the foreign scan can support parallel Scan by returning true to indicate it is supported, or false otherwise. Parallel scan is invokved by the planner when a scan query involves retrieving a large amount of data. Depending on the foreign server and your implementation, you have to decide whether or not parallel scan is supported. 2.2 GetForeignRelSizeSimilar to the table access method’s relation_estimate_size() API, this routine requires you to provide an estimate of the amount of tuples/rows would be involved during this round of scanning. This information is very important because it directly affects the planner’s decision to execute the query. For example, if you say foreign relation has 1 million tuples to be fetched, then the planner is most likely to use parallel scan to complete the scan for performance reasons if you answer “true” in the IsForeignScanParallelSafe() function call. 2.3 GetForeignPathsIn this routine, we are required to provide a list of possible paths to complete the required foreign scan. Paths simply means ways to execute. For example, you can complete a foreign scan by doing a sequential scan by fetching 1 tuple at a time, or you can complete the same scan using index to look-up a target tuple if the table has an index defined and user gives specific WHERE clause to pinpoint a tuple. If user provides a WHERE clause to filter the result, you also have an option to estimate whether the WHERE can be executed by the foreign server or by the local server as it fetches a tuple. All these are possible pathnodes are required by the planner to decide on the optimal path to execute the scan. Each pathnode requires you to provide the startup cost, total cost, number of rows and path keys if any. The official documentation suggests that you should use add_path and create_foreignscan_path routines to prepare a list of path nodes. 2.4 GetForeignPlanHaving provided all the possible pathnodes, planner will pick one that is the most optimal and pass that pathnode to this routine and ask you to make a foreign scan plan for the executor. This is a very important function because whatever plan that we provide here directly affects how the executor is going to function, so be cautious here. In this function, the planner will give you everything about the query such as the target attribute list and the restricting clauses (WHERE clauses) and also the information about the foreign relation, such as its OID, and each column’s data types. Official PostgreSQL documentation suggests using make_foreignscan() routine with the desired arguments to create the foreign plan. In here, you also will have an option to push down the restricting clauses or not. In other words, you can choose to send all the WHERE clauses to the foreign server to process if it can support it. This results in much less data communication. You can also choose having all the WHERE clauses to be processed locally, but this requires the FDW to fetch every single row from the foreign server, resulting in more data communication. This decision is controlled by the List *qpqual argument to the make_foreignscan() function. If this list contains the list of your restricting clauses, the local server will perform the WHERE evaluation locally; if this list is empty, then the local server assumes the FDW will do the WHERE evaluation on the foreign server 2.5 BeginForeignScanThis routine will be called before starting the foreign scan, giving you a chance to allocate all the necessary control information required for your foreign scan. You can define your own control information, for example, cursor location, remote connection structure, current offset…etc. and store in node-&gt;fdw_state and it will be passed down to the IterateForeignScan as the scan is begin completed. 2.6 IterateForeignScanThis is the main routine to perform the actual scan. You need to put the proper logic here depending on your use case to fetch one or more rows from the foreign server. When you have the row data, you will need to convert that to a TupleTableSlot strucutre in which the PG internals can understand. We normally use ExecStoreHeapTuple() routine to convert a HeapTuple into a TupleTableSlot. This routine will be continuously called by the PG executor as long as you still have data to fetch and it will only stop once you return an empty TupleTableSlot. 2.7 EndForeignScanThis routine will be called at the end of foreign scan, giving you the opportunity to clean up the control information allocated in BeginForeignScan(). This marks the end of foreign scan via FDW. 3. SummaryThere is a lot of information and articals out there related to PostgreSQL’s FDW API interface. This blog is merely a quick summary of what I have learned during the FDW evaluation and there is much more to what’s discussed here. For more detail, you can refer to the official documentation about the FDW callback functons here","link":"/2021/09/03/How-to-Implement-Foreign-Scan-With-FDW-Interface-API/"}],"tags":[{"name":"extension","slug":"extension","link":"/tags/extension/"},{"name":"gdb","slug":"gdb","link":"/tags/gdb/"},{"name":"trace","slug":"trace","link":"/tags/trace/"},{"name":"query processing","slug":"query-processing","link":"/tags/query-processing/"},{"name":"snmp","slug":"snmp","link":"/tags/snmp/"},{"name":"networking","slug":"networking","link":"/tags/networking/"},{"name":"oid","slug":"oid","link":"/tags/oid/"},{"name":"replication","slug":"replication","link":"/tags/replication/"},{"name":"pg_rewind","slug":"pg-rewind","link":"/tags/pg-rewind/"},{"name":"failover","slug":"failover","link":"/tags/failover/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"partition","slug":"partition","link":"/tags/partition/"},{"name":"trigger","slug":"trigger","link":"/tags/trigger/"},{"name":"logical replication","slug":"logical-replication","link":"/tags/logical-replication/"},{"name":"logical decoding","slug":"logical-decoding","link":"/tags/logical-decoding/"},{"name":"sequence","slug":"sequence","link":"/tags/sequence/"},{"name":"pluggable api","slug":"pluggable-api","link":"/tags/pluggable-api/"},{"name":"in-memory table","slug":"in-memory-table","link":"/tags/in-memory-table/"},{"name":"postmaster","slug":"postmaster","link":"/tags/postmaster/"},{"name":"backend","slug":"backend","link":"/tags/backend/"},{"name":"architecture","slug":"architecture","link":"/tags/architecture/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"wal2mongo","slug":"wal2mongo","link":"/tags/wal2mongo/"},{"name":"postgres","slug":"postgres","link":"/tags/postgres/"},{"name":"postgresql","slug":"postgresql","link":"/tags/postgresql/"},{"name":"in-memory","slug":"in-memory","link":"/tags/in-memory/"},{"name":"pluggable storage","slug":"pluggable-storage","link":"/tags/pluggable-storage/"},{"name":"tls","slug":"tls","link":"/tags/tls/"},{"name":"pg13","slug":"pg13","link":"/tags/pg13/"},{"name":"table access method","slug":"table-access-method","link":"/tags/table-access-method/"},{"name":"key management system","slug":"key-management-system","link":"/tags/key-management-system/"},{"name":"external key management system","slug":"external-key-management-system","link":"/tags/external-key-management-system/"},{"name":"tde","slug":"tde","link":"/tags/tde/"},{"name":"buffer manager","slug":"buffer-manager","link":"/tags/buffer-manager/"},{"name":"2021 PG Asia Conference","slug":"2021-PG-Asia-Conference","link":"/tags/2021-PG-Asia-Conference/"},{"name":"pitr","slug":"pitr","link":"/tags/pitr/"},{"name":"recovery","slug":"recovery","link":"/tags/recovery/"},{"name":"parallel worker","slug":"parallel-worker","link":"/tags/parallel-worker/"},{"name":"timeline id","slug":"timeline-id","link":"/tags/timeline-id/"},{"name":"extended query","slug":"extended-query","link":"/tags/extended-query/"},{"name":"wire protocol","slug":"wire-protocol","link":"/tags/wire-protocol/"},{"name":"visibility","slug":"visibility","link":"/tags/visibility/"},{"name":"streaming replication","slug":"streaming-replication","link":"/tags/streaming-replication/"},{"name":"savepoint","slug":"savepoint","link":"/tags/savepoint/"},{"name":"subtransaction","slug":"subtransaction","link":"/tags/subtransaction/"},{"name":"fdw","slug":"fdw","link":"/tags/fdw/"}],"categories":[{"name":"PostgreSQL","slug":"PostgreSQL","link":"/categories/PostgreSQL/"},{"name":"Netsnmp","slug":"Netsnmp","link":"/categories/Netsnmp/"}]}